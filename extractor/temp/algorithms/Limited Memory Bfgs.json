{
    "about": "Limited-memory BFGS (L-BFGS or LM-BFGS) is an optimization algorithm in the family of quasi-Newton methods that approximates the Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) algorithm using a limited amount of computer memory. It is a popular algorithm for parameter estimation in machine learning.[1][2]", 
    "classification": "Optimization Algorithms And Methods", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Limited-memory_BFGS\n", 
    "full_text": "Limited-memory BFGS (L-BFGS or LM-BFGS) is an optimization algorithm in the family of quasi-Newton methods that approximates the Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) algorithm using a limited amount of computer memory. It is a popular algorithm for parameter estimation in machine learning.[1][2]\nLike the original BFGS, L-BFGS uses an estimation to the inverse Hessian matrix to steer its search through variable space, but where BFGS stores a dense n\u00d7n approximation to the inverse Hessian (n being the number of variables in the problem), L-BFGS stores only a few vectors that represent the approximation implicitly. Due to its resulting linear memory requirement, the L-BFGS method is particularly well suited for optimization problems with a large number of variables. Instead of the inverse Hessian Hk, L-BFGS maintains a history of the past m updates of the position x and gradient \u2207f(x), where generally the history size m can be small (often m<10). These updates are used to implicitly do operations requiring the Hk-vector product.\n\n\nL-BFGS shares many features with other quasi-Newton algorithms, but is very different in how the matrix-vector multiplication for finding the search direction is carried out \n\n\n\n\nd\n\nk\n\n\n=\n\u2212\n\nH\n\nk\n\n\n\ng\n\nk\n\n\n\n\n\n\n{\\displaystyle d_{k}=-H_{k}g_{k}\\,\\!}\n\n, where \n\n\n\n\ng\n\nk\n\n\n\n\n{\\displaystyle g_{k}}\n\n is the current derivative and \n\n\n\n\nH\n\nk\n\n\n\n\n{\\displaystyle H_{k}}\n\n is the inverse of the Hessian matrix. There are multiple published approaches using a history of updates to form this direction vector. Here, we give a common approach, the so-called \"two loop recursion.\"[3][4]\nWe'll take as given \n\n\n\n\nx\n\nk\n\n\n\n\n\n\n{\\displaystyle x_{k}\\,\\!}\n\n, the position at the \n\n\n\nk\n\n\n\n\n{\\displaystyle k\\,\\!}\n\n-th iteration, and \n\n\n\n\ng\n\nk\n\n\n\u2261\n\u2207\nf\n(\n\nx\n\nk\n\n\n)\n\n\n{\\displaystyle g_{k}\\equiv \\nabla f(x_{k})}\n\n where \n\n\n\nf\n\n\n\n\n{\\displaystyle f\\,\\!}\n\n is the function being minimized, and all vectors are column vectors. We also assume that we have stored the last \n\n\n\nm\n\n\n{\\displaystyle m}\n\n updates of the form \n\n\n\n\ns\n\nk\n\n\n=\n\nx\n\nk\n+\n1\n\n\n\u2212\n\nx\n\nk\n\n\n\n\n\n\n{\\displaystyle s_{k}=x_{k+1}-x_{k}\\,\\!}\n\n and \n\n\n\n\ny\n\nk\n\n\n=\n\ng\n\nk\n+\n1\n\n\n\u2212\n\ng\n\nk\n\n\n\n\n\n\n{\\displaystyle y_{k}=g_{k+1}-g_{k}\\,\\!}\n\n. We'll define \n\n\n\n\n\u03c1\n\nk\n\n\n=\n\n\n1\n\n\ny\n\nk\n\n\n\nT\n\n\n\n\ns\n\nk\n\n\n\n\n\n\n\n{\\displaystyle \\rho _{k}={\\frac {1}{y_{k}^{\\rm {T}}s_{k}}}}\n\n, and \n\n\n\n\nH\n\nk\n\n\n0\n\n\n\n\n\n\n{\\displaystyle H_{k}^{0}\\,\\!}\n\n will be the 'initial' approximate of the inverse Hessian that our estimate at iteration \n\n\n\nk\n\n\n\n\n{\\displaystyle k\\,\\!}\n\n begins with. Then we can compute the (uphill) direction as follows:\nThis formulation is valid whether we are minimizing or maximizing. Note that if we are minimizing, the search direction would be the negative of z (since z is \"uphill\"), and if we are maximizing, \n\n\n\n\nH\n\nk\n\n\n0\n\n\n\n\n{\\displaystyle H_{k}^{0}}\n\n should be negative definite rather than positive definite. We would typically do a backtracking line search in the search direction (any line search would be valid, but L-BFGS does not require exact line searches in order to converge).\nCommonly, the inverse Hessian \n\n\n\n\nH\n\nk\n\n\n0\n\n\n\n\n\n\n{\\displaystyle H_{k}^{0}\\,\\!}\n\n is represented as a diagonal matrix, so that initially setting \n\n\n\nz\n\n\n\n\n{\\displaystyle z\\,\\!}\n\n requires only an element-by-element multiplication.\nThis two loop update only works for the inverse Hessian. Approaches to implementing L-BFGS using the direct approximate Hessian \n\n\n\n\nB\n\nk\n\n\n\n\n\n\n{\\displaystyle B_{k}\\,\\!}\n\n have also been developed, as have other means of approximating the inverse Hessian.[5]\nL-BFGS has been called \"the algorithm of choice\" for fitting log-linear (MaxEnt) models and conditional random fields with \n\n\n\n\n\u2113\n\n2\n\n\n\n\n{\\displaystyle \\ell _{2}}\n\n-regularization.[2]\nSince BFGS (and hence L-BFGS) is designed to minimize smooth functions without constraints, the L-BFGS algorithm must be modified to handle functions that include non-differentiable components or constraints. A popular class of modifications are called active-set methods, based on the concept of the active set. The idea is that when restricted to a small neighborhood of the current iterate, the function and constraints can be simplified.\nThe L-BFGS-B algorithm extends L-BFGS to handle simple box constraints (aka bound constraints) on variables; that is, constraints of the form li \u2264 xi \u2264 ui where li and ui are per-variable constant lower and upper bounds, respectively (for each xi, either or both bounds may be omitted).[6][7] The method works by identifying fixed and free variables at every step (using a simple gradient method), and then using the L-BFGS method on the free variables only to get higher accuracy, and then repeating the process.\nOrthant-wise limited-memory quasi-Newton (OWL-QN) is an L-BFGS variant for fitting \n\n\n\n\n\u2113\n\n1\n\n\n\n\n{\\displaystyle \\ell _{1}}\n\n-regularized models, exploiting the inherent sparsity of such models.[2] It minimizes functions of the form\nwhere \n\n\n\ng\n\n\n{\\displaystyle g}\n\n is a differentiable convex loss function. The method is an active-set type method: at each iterate, it estimates the sign of each component of the variable, and restricts the subsequent step to have the same sign. Once the sign is fixed, the non-differentiable \n\n\n\n\u2225\n\n\n\nx\n\u2192\n\n\n\n\n\u2225\n\n1\n\n\n\n\n{\\displaystyle \\|{\\vec {x}}\\|_{1}}\n\n term becomes a smooth linear term which can be handled by L-BFGS. After an L-BFGS step, the method allows some variables to change sign, and repeats the process.\nSchraudolph et al. present an online approximation to both BFGS and L-BFGS.[8] Similar to stochastic gradient descent, this can be used to reduce the computational complexity by evaluating the error function and gradient on a randomly drawn subset of the overall dataset in each iteration. It has been shown that O-LBFGS has a global almost sure convergence [9] while the online approximation of BFGS (O-BFGS) is not necessarily convergent.[10]\nAn early, open source implementation of L-BFGS in Fortran exists in Netlib as a shar archive [1]. Multiple other open source implementations have been produced as translations of this Fortran code (e.g. java, and python via SciPy). Other implementations exist:\nThe L-BFGS-B variant also exists as ACM TOMS algorithm 778.[7] In February 2011, some of the authors of the original L-BFGS-B code posted a major update (version 3.0).\nA reference implementation[11] is available in Fortran 77 (and with a Fortran 90 interface) at the author's website. This version, as well as older versions, has been converted to many other languages, including a Java wrapper for v3.0; Matlab interfaces for v3.0, v2.4, and v2.1; a C++ interface for v2.1; a Python interface for v3.0 as part of scipy.optimize.minimize; an OCaml interface for v2.1 and v3.0; version 2.3 has been converted to C by f2c and is available at this website; and R's optim general-purpose optimizer routine includes L-BFGS-B by using method=\"L-BFGS-B\".[12]\nThere exists a complete C++11 rewrite of the L-BFGS-B solver using Eigen3.\nOWL-QN implementations are available in:", 
    "name": "Limited Memory Bfgs"
}