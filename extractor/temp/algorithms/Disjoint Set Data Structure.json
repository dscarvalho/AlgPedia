{
    "about": "In computer science, a disjoint-set data structure, also called a union\u2013find data structure or merge\u2013find set, is a data structure that keeps track of a set of elements partitioned into a number of disjoint (nonoverlapping) subsets. It supports two useful operations:", 
    "classification": "Search Algorithms", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Disjoint-set_data_structure\n", 
    "full_text": "In computer science, a disjoint-set data structure, also called a union\u2013find data structure or merge\u2013find set, is a data structure that keeps track of a set of elements partitioned into a number of disjoint (nonoverlapping) subsets. It supports two useful operations:\nThe other important operation, MakeSet, which makes a set containing only a given element (a singleton), is generally trivial. With these three operations, many practical partitioning problems can be solved (see the Applications section).\nIn order to define these operations more precisely, some way of representing the sets is needed. One common approach is to select a fixed element of each set, called its representative, to represent the set as a whole. Then, Find(x) returns the representative of the set that x belongs to, and Union takes two set representatives as its arguments.\n\n\nA simple disjoint-set data structure uses a linked list for each set. The element at the head of each list is chosen as its representative.\nMakeSet creates a list of one element. Union appends the two lists, a constant-time operation if the list carries a pointer to its tail. The drawback of this implementation is that Find requires O(n) or linear time to traverse the list backwards from a given element to the head of the list.\nThis can be avoided by including in each linked list node a pointer to the head of the list; then Find takes constant time, since this pointer refers directly to the set representative. However, Union now has to update each element of the list being appended to make it point to the head of the new combined list, requiring O(n) time.\nWhen the length of each list is tracked, the required time can be improved by always appending the smaller list to the longer. Using this weighted-union heuristic, a sequence of m MakeSet, Union, and Find operations on n elements requires O(m\u00a0+\u00a0nlog\u00a0n) time.[1] For asymptotically faster operations, a different data structure is needed.\nWe now explain the bound \n\n\n\nO\n(\nn\nlog\n\u2061\n(\nn\n)\n)\n\n\n{\\displaystyle O(n\\log(n))}\n\n above.\nSuppose you have a collection of lists and each node of each list contains an object, the name of the list to which it belongs, and the number of elements in that list. Also assume that the total number of elements in all lists is \n\n\n\nn\n\n\n{\\displaystyle n}\n\n (i.e. there are \n\n\n\nn\n\n\n{\\displaystyle n}\n\n elements overall). We wish to be able to merge any two of these lists, and update all of their nodes so that they still contain the name of the list to which they belong. The rule for merging the lists \n\n\n\nA\n\n\n{\\displaystyle A}\n\n and \n\n\n\nB\n\n\n{\\displaystyle B}\n\n is that if \n\n\n\nA\n\n\n{\\displaystyle A}\n\n is larger than \n\n\n\nB\n\n\n{\\displaystyle B}\n\n then merge the elements of \n\n\n\nB\n\n\n{\\displaystyle B}\n\n into \n\n\n\nA\n\n\n{\\displaystyle A}\n\n and update the elements that used to belong to \n\n\n\nB\n\n\n{\\displaystyle B}\n\n, and vice versa.\nChoose an arbitrary element of list \n\n\n\nL\n\n\n{\\displaystyle L}\n\n, say \n\n\n\nx\n\n\n{\\displaystyle x}\n\n. We wish to count how many times in the worst case will \n\n\n\nx\n\n\n{\\displaystyle x}\n\n need to have the name of the list to which it belongs updated. The element \n\n\n\nx\n\n\n{\\displaystyle x}\n\n will only have its name updated when the list it belongs to is merged with another list of the same size or of greater size. Each time that happens, the size of the list to which \n\n\n\nx\n\n\n{\\displaystyle x}\n\n belongs at least doubles. So finally, the question is \"how many times can a number double before it is the size of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n?\" (then the list containing \n\n\n\nx\n\n\n{\\displaystyle x}\n\n will contain all \n\n\n\nn\n\n\n{\\displaystyle n}\n\n elements). The answer is exactly \n\n\n\n\nlog\n\n2\n\n\n\u2061\n(\nn\n)\n\n\n{\\displaystyle \\log _{2}(n)}\n\n. So for any given element of any given list in the structure described, it will need to be updated \n\n\n\n\nlog\n\n2\n\n\n\u2061\n(\nn\n)\n\n\n{\\displaystyle \\log _{2}(n)}\n\n times in the worst case. Therefore, updating a list of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n elements stored in this way takes \n\n\n\nO\n(\nn\nlog\n\u2061\n(\nn\n)\n)\n\n\n{\\displaystyle O(n\\log(n))}\n\n time in the worst case. A find operation can be done in \n\n\n\nO\n(\n1\n)\n\n\n{\\displaystyle O(1)}\n\n for this structure because each node contains the name of the list to which it belongs.\nA similar argument holds for merging the trees in the data structures discussed below. Additionally, it helps explain the time analysis of some operations in the binomial heap and Fibonacci heap data structures.\nDisjoint-set forests are data structures where each set is represented by a tree data structure, in which each node holds a reference to its parent node (see parent pointer tree). They were first described by Bernard A. Galler and Michael J. Fischer in 1964,[2] although their precise analysis took years[citation needed].\nIn a disjoint-set forest, the representative of each set is the root of that set's tree. Find follows parent nodes until it reaches the root. Union combines two trees into one by attaching the root of one to the root of the other.\nOne way of implementing these might be:\nIn this naive form, this approach is no better than the linked-list approach, because the tree it creates can be highly unbalanced.\nThe previous implementation can be enhanced in two ways.\nThe first way, called union by rank, is to always attach the smaller tree to the root of the larger tree. Since it is the depth of the tree that affects the running time, the tree with smaller depth gets added under the root of the deeper tree, which only increases the depth if the depths were equal. In the context of this algorithm, the term rank is used instead of depth since it stops being equal to the depth if path compression (described below) is also used. One-element trees are defined to have a rank of zero, and whenever two trees of the same rank r are united, the rank of the result is r+1. Just applying this technique alone yields a worst-case running-time of O(log n) for the Union or Find operation. Pseudocode for the improved MakeSet and Union:\nThe second improvement, called path compression, is a way of flattening the structure of the tree whenever Find is used on it. The idea is that each node visited on the way to a root node may as well be attached directly to the root node; they all share the same representative. To effect this, as Find recursively traverses up the tree, it changes each node's parent reference to point to the root that it found. The resulting tree is much flatter, speeding up future operations not only on these elements but on those referencing them, directly or indirectly. Here is the improved Find:\nThese two techniques complement each other; applied together, the amortized time per operation is only \n\n\n\nO\n(\n\u03b1\n(\nn\n)\n)\n\n\n{\\displaystyle O(\\alpha (n))}\n\n, where \n\n\n\n\u03b1\n(\nn\n)\n\n\n{\\displaystyle \\alpha (n)}\n\n is the inverse of the function \n\n\n\nn\n=\nf\n(\nx\n)\n=\nA\n(\nx\n,\nx\n)\n\n\n{\\displaystyle n=f(x)=A(x,x)}\n\n, and \n\n\n\nA\n\n\n{\\displaystyle A}\n\n is the extremely fast-growing Ackermann function. Since \n\n\n\n\u03b1\n(\nn\n)\n\n\n{\\displaystyle \\alpha (n)}\n\n is the inverse of this function, \n\n\n\n\u03b1\n(\nn\n)\n\n\n{\\displaystyle \\alpha (n)}\n\n is less than 5 for all remotely practical values of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n. Thus, the amortized running time per operation is effectively a small constant.\nIn fact, this is asymptotically optimal: Fredman and Saks showed in 1989 that \n\n\n\n\u03a9\n(\n\u03b1\n(\nn\n)\n)\n\n\n{\\displaystyle \\Omega (\\alpha (n))}\n\n words must be accessed by any disjoint-set data structure per operation on average.[3]\nDisjoint-set data structures model the partitioning of a set, for example to keep track of the connected components of an undirected graph. This model can then be used to determine whether two vertices belong to the same component, or whether adding an edge between them would result in a cycle. The Union\u2013Find algorithm is used in high-performance implementations of unification.[4]\nThis data structure is used by the Boost Graph Library to implement its Incremental Connected Components functionality. It is also used for implementing Kruskal's algorithm to find the minimum spanning tree of a graph.\nNote that the implementation as disjoint-set forests doesn't allow deletion of edges\u2014even without path compression or the rank heuristic.\nWhile the ideas used in disjoint-set forests have long been familiar, Robert Tarjan was the first to prove the upper bound (and a restricted version of the lower bound) in terms of the inverse Ackermann function, in 1975.[5] Until this time the best bound on the time per operation, proven by Hopcroft and Ullman,[6] was O(log* n), the iterated logarithm of n, another slowly growing function (but not quite as slow as the inverse Ackermann function).\nTarjan and Van Leeuwen also developed one-pass Find algorithms that are more efficient in practice while retaining the same worst-case complexity.[7]\nIn 2007, Sylvain Conchon and Jean-Christophe Filli\u00e2tre developed a persistent version of the disjoint-set forest data structure, allowing previous versions of the structure to be efficiently retained, and formalized its correctness using the proof assistant Coq.[8] However, the implementation is only asymptotic if used ephemerally or if the same version of the structure is repeatedly used with limited backtracking.", 
    "name": "Disjoint Set Data Structure"
}