{
    "about": "False position method and regula falsi method are two early, and still current, names for a very old method for solving an equation in one unknown.", 
    "name": "Illinois Algorithm", 
    "classification": "Root-Finding Algorithms", 
    "full_text": "False position method and regula falsi method are two early, and still current, names for a very old method for solving an equation in one unknown.\n\n\nTo solve an equation means to write, or determine the numerical value of, one of its quantities in terms of the other quantities mentioned in the equation.\nMany equations, including most of the more complicated ones, can be solved only by iterative numerical approximation. That consists of trial and error, in which various values of the unknown quantity, referred to here as \u201cx\u201d, are tried. That trial-and-error may be informed by a calculated estimate for the solution. The iterative numerical approximation methods for solving equations, which use a calculated estimate for the solution, for use in calculating the next, improved, solution-estimate, differ only by how their calculated solution-estimates are made.\nBy moving all of an equation\u2019s terms to one side, we can get an equation that says: f(x) = 0, where f(x) is some function of the unknown variable \u201cx\u201d.\nThat transforms the problem into one of finding the x-value at which f(x) = 0. That x-value is the equation\u2019s solution.\nIn this section, the symbol \u201cy\u201d will be used interchangeably with f(x) when that improves brevity, clarity, and reduces clutter.\nHere, \u201cy\u201d means \u201cy(x)\u201d means \u201cf(x)\u201d. The expressions \u201cy\u201d and \u201cf(x)\u201d will both be used here, and they mean the same thing. The symbol \u201cy\u201d is familiar, as the often-used name for the vertical co-ordinate on a graph, often a function of \u201cx\u201d, the horizontal co-ordinate.\nLet's solve the equation x + 1/4x = 15 by false position. Try with x = 4. We get 4 + 1/4*4 = 5, note 4 is not the solution. Let's now multiply with 3 on both sides to get 12 + 1/4*12 = 15, obtaining the solution x = 12. The example is problem 26 on the Rhind papyrus. A History of Mathematics, 3rd edition, by Victor J. Katz categorizes the problem as false position.\nMany methods for the calculated-estimate are used. The oldest and simplest class of such methods, and the class that contains the most reliable method (Bisection), are the two-point bracketing methods.\nThose methods start with two x-values, initially found by trial-and-error, at which f(x) has opposite signs. In other words: Two x-values such that, for one of them, f(x) is positive, and for the other, f(x) is negative. In that way, those two f(x) values (i.e. y-values) can be said to \u201cbracket\u201d zero, because they\u2019re on opposite sides of zero.\nThat bracketing, along with the fact that the solution-estimate-calculation method (to be discussed later) always chooses an x-value between the two current bracketing values, guarantees that the solution-estimates will converge toward the solution. \u2026a guarantee not available with such other methods as Newton\u2019s method or the Secant method.\nWhen f(x) is evaluated at a certain x-value, call it x1, resulting in f(x1), that combination of x and y values is called a \u201cdata-point\u201d, the data point (x1, y1).\nThe two-point bracketing methods use, for each iterative step, two such data points, from which to get a calculated estimate for the solution. f(x) is then evaluated for that estimated x, to get a new data point, from which to calculate a new, and closer, estimated solution.\nCall any current pair of data points (x1, y1) and (x2, y2). A calculated-estimate method (none of which have been discussed here yet, but soon will be) is used to calculate, from those two data points, a third x-value, x3 at which to evaluate f(x). (to evaluate \u201cy3\u201d, in other words).\nThat evaluation of y3 provides a 3rd data point, (x3, y3).\nThat new data point will be used as one of the pair of data points for the next solution-estimate calculation. To preserve the bracketing, the data point used with it for that purpose will be the most recently-calculated one whose y value is opposite in sign to the newest y value.\nThat new pair of points becomes the new data-points pair (x1, y1), and (x2, y2) \u2026which is again used to calculate a new estimated solution, x3. \u2026at which f(x) is evaluated, for a new y3. \u2026and so on.\nJust to illustrate what happens, suppose that y3 has the same sign as y2. Then, by the rule stated above, the data point (x1, y1) \u2013instead of (x2, y2)--is used with the new data point, (x3, y3), because that earlier data-point is the most recently calculated data point whose y is opposite in sign to that of the new data point.\nThe simplest solution-estimate calculation method is just to choose, as x3, the mean of x1 and x2.\nThat is:\nThat\u2019s enough to ensure that x3 is between x1 and x2, thereby guaranteeing convergence toward the solution.\nSo, those two x-values that enclose (bracket) the solution are always, after each iteration, twice as close together as they were after the previous iteration. Additionally, that procedure ensures that:\nNo other method can guarantee those things. Bisection\u2019s error is, on average, halved with each iteration. The method gains roughly a decimal place of accuracy, by each 3 iterations.\nOne can try for a better convergence-rate, at the risk of a worse one, or none at all.\nMost numerical equation-solving methods usually converge faster than Bisection. The price for that is that some of them (e.g. Newton\u2019s method and Secant) can fail to converge at all, and all of them can sometimes converge much slower than Bisection\u2014sometimes prohibitively slowly. None can guarantee Bisection\u2019s reliable and steady guaranteed convergence rate. Regula Falsi, like Bisection, always converges, usually considerably faster than Bisection\u2014but sometimes much slower than Bisection.\nWhen numerically solving an equation manually, by calculator, or when a computer program run has to solve equations so many times that the speed of convergence becomes important, then it could be preferable to first try a usually-faster method, going to Bisection only if the faster method fails to converge, or fails to converge at a useful rate.\nThe fact that Regula Falsi always converges, and has versions that do well at avoiding slowdowns, makes it a good choice when speed is needed, and when Newton\u2019s method doesn\u2019t converge, or when the evaluation of the derivative is too time-consuming for Newton\u2019s to be useful.\nRegula Falsi\u2019s Calculated Solution-Estimate Method:\nRegula Falsi assumes that f(x) is linear\u2014even though these methods are needed only when f(x) is not linear, and usually work well anyway.\nThe ratio of the change in x, to the resulting change in y is:\n\n\n\n\n\n\n\n\nx\n\n2\n\n\n\u2212\n\nx\n\n1\n\n\n\n\n\ny\n\n2\n\n\n\u2212\n\ny\n\n1\n\n\n\n\n\n\n\n{\\displaystyle {\\frac {x_{2}-x_{1}}{y_{2}-y_{1}}}}\n\n\nBecause y, most recently, is y2, and we want y to be 0, then the change that we want in y is:\n\n\n\n\n0\n\u2212\n\ny\n\n2\n\n\n\n\n{\\displaystyle 0-y_{2}}\n\n\nOf course that\u2019s equal to \u2013y2.\nSo, given that desired change in y, and given the expected ratio of change in x to change in y, then the best (linearly-gotten) estimate for the right x-value\u2014the best estimate for the solution\u2014is:\nThe latest value of x plus the product of the desired change in y and the expected ratio of change in x to change in y:\n\n\n\n\n\nx\n\n3\n\n\n=\n\nx\n\n2\n\n\n+\n(\n\u2212\n\ny\n\n2\n\n\n)\n\n\n\n\nx\n\n2\n\n\n\u2212\n\nx\n\n1\n\n\n\n\n\ny\n\n2\n\n\n\u2212\n\ny\n\n1\n\n\n\n\n\n\n\n{\\displaystyle x_{3}=x_{2}+(-y_{2}){\\frac {x_{2}-x_{1}}{y_{2}-y_{1}}}}\n\n\nThat formula for x3 is adequate, but can be put in a more practical form:\nMultiply, put over a common denominator, and collect terms, and the result is:\n\n\n\n\n\nx\n\n3\n\n\n=\n\n\n\n\nx\n\n1\n\n\n\ny\n\n2\n\n\n\u2212\n\nx\n\n2\n\n\n\ny\n\n1\n\n\n\n\n\ny\n\n2\n\n\n\u2212\n\ny\n\n1\n\n\n\n\n\n\n\n{\\displaystyle x_{3}={\\frac {x_{1}y_{2}-x_{2}y_{1}}{y_{2}-y_{1}}}}\n\n\nNot only is this form more simple and symmetrical, but it has a computational advantage:\nAs a solution is approached, x1 and x2 will be very close together, and nearly always of the same sign. Such a subtraction can lose significant digits.\nBecause y2 and y1 are always of opposite sign the \u201csubtraction\u201d in the numerator of the improved formula is effectively an addition (as is the subtraction in the denominator too).\nThe false position method or regula falsi method is a term for problem-solving methods in arithmetic, algebra, and calculus. In simple terms, these methods begin by attempting to evaluate a problem using test (\"false\") values for the variables, and then adjust the values accordingly.\nTwo basic types of false position method can be distinguished, simple false position and double false position. Simple false position is aimed at solving problems involving direct proportion. Such problems can be written algebraically in the form: determine x such that\nif a and b are known. Double false position is aimed at solving more difficult problems that can be written algebraically in the form: determine x such that\nif it is known that\nDouble false position is mathematically equivalent to linear interpolation; for an affine linear function,\nit provides the exact solution, while for a nonlinear function f it provides an approximation that can be successively improved by iteration.\nIn problems involving arithmetic or algebra, the false position method or regula falsi is used to refer to basic trial and error methods of solving problems by substituting test values for the unknown quantities. This is sometimes also referred to as \"guess and check\". Versions of this method predate the advent of algebra and the use of equations.\nFor simple false position, the method of solving what we would now write as ax = b begins by using a test input value x\u2032, and finding the corresponding output value b\u2032 by multiplication: ax\u2032 = b\u2032. The correct answer is then found by proportional adjustment, x = x\u2032 \u00b7 b \u00f7 b\u2032. This technique is found in cuneiform tablets from ancient Babylonian mathematics, and possibly in papyri from ancient Egyptian mathematics.[1]\nLikewise, double false position arose in late antiquity as a purely arithmetical algorithm. It was used mostly to solve what are now called affine linear problems by using a pair of test inputs and the corresponding pair of outputs. This algorithm would be memorized and carried out by rote. In the ancient Chinese mathematical text called The Nine Chapters on the Mathematical Art (\u4e5d\u7ae0\u7b97\u8853),[2] dated from 200 BC to AD 100, most of Chapter 7 was devoted to the algorithm. There, the procedure was justified by concrete arithmetical arguments, then applied creatively to a wide variety of story problems, including one involving what we would call secant lines on a quadratic polynomial. A more typical example is this \"joint purchase\" problem:\nNow an item is purchased jointly; everyone contributes 8 [coins], the excess is 3; everyone contributes 7, the deficit is 4. Tell: The number of people, the item price, what is each? Answer: 7 people, item price 53.[3]\nBetween the 9th and 10th centuries, the Egyptian Muslim mathematician Abu Kamil wrote a now-lost treatise on the use of double false position, known as the Book of the Two Errors (Kit\u0101b al-kha\u1e6d\u0101\u02beayn). The oldest surviving writing on double false position from the Middle East is that of Qusta ibn Luqa (10th century), a Christian Arab mathematician from Baalbek, Lebanon. He justified the technique by a formal, Euclidean-style geometric proof. Within the tradition of medieval Muslim mathematics, double false position was known as his\u0101b al-kha\u1e6d\u0101\u02beayn (\"reckoning by two errors\"). It was used for centuries, especially in the Maghreb, to solve practical problems such as commercial and juridical questions (estate partitions according to rules of Quranic inheritance), as well as purely recreational problems. The algorithm was often memorized with the aid of mnemonics, such as a verse attributed to Ibn al-Yasamin and balance-scale diagrams explained by al-Hassar and Ibn al-Banna, all three being mathematicians of Moroccan origin.[4]\nLeonardo of Pisa (Fibonacci) devoted Chapter 13 of his book Liber Abaci (AD 1202) to explaining and demonstrating the uses of double false position, terming the method regulis elchatayn after the al-kha\u1e6d\u0101\u02beayn method that he had learned from Arab sources.[4]\nIn numerical analysis, double false position became a root-finding algorithm that combines features from the bisection method and the secant method.\nLike the bisection method, the false position method starts with two points a0 and b0 such that f(a0) and f(b0) are of opposite signs, which implies by the intermediate value theorem that the function f has a root in the interval [a0, b0], assuming continuity of the function f. The method proceeds by producing a sequence of shrinking intervals [ak, bk] that all contain a root of f.\nAt iteration number k, the number\nis computed. As explained below, ck is the root of the secant line through (ak, f(ak)) and (bk, f(bk)). If f(ak) and f(ck) have the same sign, then we set ak+1 = ck and bk+1 = bk, otherwise we set ak+1 = ak and bk+1 = ck. This process is repeated until the root is approximated sufficiently well.\nThe above formula is also used in the secant method, but the secant method always retains the last two computed points, while the false position method retains two points which certainly bracket a root. On the other hand, the only difference between the false position method and the bisection method is that the latter uses ck = (ak + bk) / 2.\nGiven ak and bk, we construct the line through the points (ak, f(ak)) and (bk, f(bk)), as demonstrated in the picture immediately above. Note that this line is a secant or chord of the graph of the function f. In point-slope form, it can be defined as\nWe now choose ck to be the root of this line (substituting for x), and setting \n\n\n\ny\n=\n0\n\n\n{\\displaystyle y=0}\n\n and see that\nSolving this equation gives the above equation for ck.\nIf the initial end-points a0 and b0 are chosen such that f(a0) and f(b0) are of opposite signs, then at each step, one of the end-points will get closer to a root of f. If the second derivative of f is of constant sign (so there is no inflection point) in the interval, then one endpoint (the one where f also has the same sign) will remain fixed for all subsequent iterations while the converging endpoint becomes updated. As a result, unlike the bisection method, the width of the bracket does not tend to zero (unless the zero is at an inflection point around which sign(f)=-sign(f\u2033)). As a consequence, the linear approximation to f(x), which is used to pick the false position, does not improve in its quality.\nOne example of this phenomenon is the function\non the initial bracket [\u22121,1]. The left end, \u22121, is never replaced (after the first three iterations, f\u2033 is negative on the interval) and thus the width of the bracket never falls below 1. Hence, the right endpoint approaches 0 at a linear rate (the number of accurate digits grows linearly, with a rate of convergence of 2/3).\nFor discontinuous functions, this method can only be expected to find a point where the function changes sign (for example at x=0 for 1/x or the sign function). In addition to sign changes, it is also possible for the method to converge to a point where the limit of the function is zero, even if the function is undefined (or has another value) at that point (for example at x=0 for the function given by f(x)=abs(x)-x\u00b2 when x\u22600 and by f(0)=5, starting with the interval [-0.5, 3.0]). It is mathematically possible with discontinuous functions for the method to fail to converge to a zero limit or sign change, but this is not a problem in practice since it would require an infinite sequence of coincidences for both endpoints to get stuck converging to discontinuities where the sign does not change (for example at x=\u00b11 in f(x)=1/(x-1)\u00b2+1/(x+1)\u00b2). The method of bisection avoids this hypothetical convergence problem.\nThe Illinois version:\nThough Regula Falsi always converges, usually considerably faster than Bisection, there are situations that can slow its convergence\u2014sometimes to a prohibitive degree. That problem isn't unique to Regula Falsi: Other than Bisection, all of the numerical equation-solving methods can have a slow-convergence or no-convergence problem under some conditions. Sometimes, Newton's Method and the Secant Method diverge instead of converging\u2014and often do so under the conditions that slow Regula Falsi's convergence.\nBut, though Regula Falsi is one of the best methods, and\u2014even in its original un-improved version\u2014would often be the best choice (e.g. when Newton's isn't used because the derivative is prohibitively time-consuming to evaluate, or when Newton's and Successive-Substitutions have failed to converge)...A number of improvements to Regula Falsi have been proposed, in order to avoid slowdowns under those relatively unusual unfavorable situations.\nThe failure mode is easy to detect (the same end-point is retained twice in a row) and easily remedied by next picking a modified false position, such as\nor\ndown-weighting one of the endpoint values to force the next ck to occur on that side of the function. The factor of 2 above looks like a hack, but it guarantees superlinear convergence (asymptotically, the algorithm will perform two regular steps after any modified step, and has order of convergence 1.442). There are other ways to pick the rescaling which give even better superlinear convergence rates.[5]\nThe above adjustment to regula falsi is sometimes called the Illinois algorithm.[6][7] Ford (1995) summarizes and analyzes this and other similar superlinear variants of the method of false position.[5]\nPut simply:\nWhen the new y-value has the same sign as the previous one, meaning that the data point before the previous one will be retained, the Illinois version halves the y-value of the retained data point.\nThe Anderson-Bj\u00f6rk version:\nIf points (x1, y1) and (x2, y2) have resulted in the new point (x3, y3), and if y3 has the same sign as y2, then retain the point (x1, y1), to use with the newest point, for the next Regula Falsi step.\nSo far, that's the same as ordinary Regula Falsi and Illinois.\nBut, where Illinois would multiply y1 by 1/2, Anderson-Bj\u00f6rk multiplies it by m, where m has the following value:\nm = 1 - y3/y2 ...if that's greater than 0\nelse: m = 1/2\nFor simple roots, Anderson-Bj\u00f6rk was the clear winner in Galdino's numerical tests.[8]\nFor multiple roots, no method was much faster than Bisection. In fact, the only methods that were as fast as Bisection were three new methods introduced by Galdino. But even they were only a little faster than Bisection.\nBisection:\nWhen solving one equation, or just a few, using a computer, there's no reason to not just use Bisection. Though Bisection isn't as fast as the other methods\u2014when they're at their best and don't have a problem\u2014Bisection nevertheless is guaranteed to converge at a useful rate, roughly halving the error with each iteration. ...gaining roughly a decimal place of accuracy with each 3 iterations.\nFor manual calculation, by calculator, one tends to want to use faster methods, and they usually, but not always, converge faster than Bisection. But a computer, even using Bisection, will solve an equation, to the desired accuracy, so rapidly that there's no need to try to save time by using a less reliable method\u2014and every method is less reliable than Bisection.\nAn exception would be if the computer program had to solve equations very many times during its run. Then the time saved by the faster methods could be significant.\nThen, a program could start with Newton's method, and, if Newton's isn't converging, switch to Regula Falsi, maybe in one of its improved versions, such as Illinois or Anderson-Bj\u0151rk. ...and then, if even that isn't converging as well as Bisection would, switch to Bisection, which always converges at a useful, if not spectacular, rate.\nNewton's Method when close to convergence:\nWhen the magnitude of y has become very small, and x is changing very little, then, most likely, Newton's Method won't run into any trouble, and will converge. So, under those favorable conditions, one could switch to Newton's method if one wanted the error to be very small and wanted very fast convergence.\nThis example programme, written in the C programming language, has been written for clarity instead of efficiency. It was designed to solve the same problem as solved by the Newton's method and secant method code: to find the positive number x where cos(x) = x3. This problem is transformed into a root-finding problem of the form f(x) = cos(x) - x3 = 0.\nAfter running this code, the final answer is approximately 0.865474033101614", 
    "dbpedia_url": "http://dbpedia.org/resource/Illinois_algorithm", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Illinois_algorithm\n"
}