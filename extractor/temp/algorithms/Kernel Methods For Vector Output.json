{
    "about": "Kernel methods are a well-established tool to analyze the relationship between input data and the corresponding output of a function. Kernels encapsulate the properties of functions in a computationally efficient way and allow algorithms to easily swap functions of varying complexity.", 
    "name": "Kernel Methods For Vector Output", 
    "classification": "Machine Learning Algorithms", 
    "full_text": "Kernel methods are a well-established tool to analyze the relationship between input data and the corresponding output of a function. Kernels encapsulate the properties of functions in a computationally efficient way and allow algorithms to easily swap functions of varying complexity.\nIn typical machine learning algorithms, these functions produce a scalar output. Recent development of kernel methods for functions with vector-valued output is due, at least in part, to interest in simultaneously solving related problems. Kernels which capture the relationship between the problems allow them to borrow strength from each other. Algorithms of this type include multi-task learning (also called multi-output learning or vector-valued learning), transfer learning, and co-kriging. Multi-label classification can be interpreted as mapping inputs to (binary) coding vectors with length equal to the number of classes.\nIn Gaussian processes, kernels are called covariance functions. Multiple-output functions correspond to considering multiple processes. See Bayesian interpretation of regularization for the connection between the two perspectives.\n\n\nThe history of learning vector-valued functions is closely linked to transfer learning, a broad term that refers to systems that learn by transferring knowledge between different domains. The fundamental motivation for transfer learning in the field of machine learning was discussed in a NIPS-95 workshop on \u201cLearning to Learn,\u201d which focused on the need for lifelong machine learning methods that retain and reuse previously learned knowledge. Research on transfer learning has attracted much attention since 1995 in different names: learning to learn, lifelong learning, knowledge transfer, inductive transfer, multitask learning, knowledge consolidation, context-sensitive learning, knowledge-based inductive bias, metalearning, and incremental/cumulative learning.[1] Interest in learning vector-valued functions was particularly sparked by multitask learning, a framework which tries to learn multiple, possibly different tasks simultaneously.\nMuch of the initial research in multitask learning in the machine learning community was algorithmic in nature, and applied to methods such as neural networks, decision trees and k-nearest neighbors in the 1990s.[2] The use of probabilistic models and Gaussian processes was pioneered and largely developed in the context of geostatistics, where prediction over vector-valued output data is known as cokriging.[3][4][5] Geostatistical approaches to multivariate modeling are mostly formulated around the linear model of coregionalization (LMC), a generative approach for developing valid covariance functions that has been used for multivariate regression and in statistics for computer emulation of expensive multivariate computer codes. The regularization and kernel theory literature for vector-valued functions followed in the 2000s.[6][7] While the Bayesian and regularization perspectives were developed independently, they are in fact closely related.[8]\nIn this context, the supervised learning problem is to learn the function \n\n\n\nf\n\n\n{\\displaystyle f}\n\n which best predicts vector-valued outputs \n\n\n\n\n\ny\n\ni\n\n\n\n\n\n{\\displaystyle \\mathbf {y_{i}} }\n\n given inputs (data) \n\n\n\n\n\nx\n\ni\n\n\n\n\n\n{\\displaystyle \\mathbf {x_{i}} }\n\n.\nIn general, each component of (\n\n\n\n\n\ny\n\ni\n\n\n\n\n\n{\\displaystyle \\mathbf {y_{i}} }\n\n), could have different input data (\n\n\n\n\n\nx\n\nd\n,\ni\n\n\n\n\n\n{\\displaystyle \\mathbf {x_{d,i}} }\n\n) with different cardinality (\n\n\n\np\n\n\n{\\displaystyle p}\n\n) and even different input spaces (\n\n\n\n\n\nX\n\n\n\n\n{\\displaystyle {\\mathcal {X}}}\n\n).[8] Geostatistics literature calls this case heterotopic, and uses isotopic to indicate that the each component of the output vector has the same set of inputs.[9]\nHere, for simplicity in the notation, we assume the number and sample space of the data for each output are the same.\nFrom the regularization perspective, the problem is to learn \n\n\n\n\nf\n\n\u2217\n\n\n\n\n{\\displaystyle f_{*}}\n\n belonging to a reproducing kernel Hilbert space of vector-valued functions (\n\n\n\n\n\nH\n\n\n\n\n{\\displaystyle {\\mathcal {H}}}\n\n). This is similar to the scalar case of Tikhonov regularization, with some extra care in the notation.\nwith \n\n\n\n\n\n\n\nc\n\n\u00af\n\n\n\n=\n(\n\nK\n\n(\n\nX\n\n,\n\nX\n\n)\n+\n\u03bb\nN\n\n(\n\nI\n)\n\n)\n\n\u2212\n1\n\n\n\n\n\n\ny\n\n\u00af\n\n\n\n\n\n{\\displaystyle {\\bar {\\mathbf {c} }}=(\\mathbf {K} (\\mathbf {X} ,\\mathbf {X} )+\\lambda N\\mathbf {(} I))^{-1}{\\bar {\\mathbf {y} }}}\n\n,\nwhere \n\n\n\n\n\n\n\nc\n\n\u00af\n\n\n\n\n\u00a0and\u00a0\n\n\n\n\n\ny\n\n\u00af\n\n\n\n\n\n{\\displaystyle {\\bar {\\mathbf {c} }}{\\text{ and }}{\\bar {\\mathbf {y} }}}\n\n are the coefficients and output vectors concatenated to form \n\n\n\nN\nD\n\n\n{\\displaystyle ND}\n\n vectors and \n\n\n\n\nK\n\n(\n\nX\n\n,\n\nX\n\n)\n\n\u00a0is an\u00a0\n\nN\nD\n\u00d7\nN\nD\n\n\n{\\displaystyle \\mathbf {K} (\\mathbf {X} ,\\mathbf {X} ){\\text{ is an }}ND\\times ND}\n\n matrix of \n\n\n\nN\n\u00d7\nN\n\n\n{\\displaystyle N\\times N}\n\n blocks: \n\n\n\n(\n\nK\n\n(\n\n\nx\n\ni\n\n\n\n,\n\n\nx\n\nj\n\n\n\n)\n\n)\n\nd\n,\n\nd\n\u2032\n\n\n\n\n\n{\\displaystyle (\\mathbf {K} (\\mathbf {x_{i}} ,\\mathbf {x_{j}} ))_{d,d'}}\n\n\n\n\n\n\n\nf\n\n\u2217\n\n\n(\n\nx\n\n)\n=\n\n\u2211\n\ni\n=\n1\n\n\nN\n\n\nk\n(\n\n\nx\n\ni\n\n\n\n,\n\nx\n\n)\n\nc\n\ni\n\n\n=\n\n\nk\n\n\n\nx\n\n\n\n\u22ba\n\n\n\nc\n\n\n\n{\\displaystyle f_{*}(\\mathbf {x} )=\\sum \\limits _{i=1}^{N}k(\\mathbf {x_{i}} ,\\mathbf {x} )c_{i}=\\mathbf {k} _{\\mathbf {x} }^{\\intercal }\\mathbf {c} }\n\n\nSolve for \n\n\n\n\nc\n\n\n\n{\\displaystyle \\mathbf {c} }\n\n by taking the derivative of the learning problem, setting it equal to zero, and substituting in the above expression for \n\n\n\n\nf\n\n\u2217\n\n\n\n\n{\\displaystyle f_{*}}\n\n:\nwhere \n\n\n\n\n\nK\n\n\ni\nj\n\n\n=\nk\n(\n\n\nx\n\ni\n\n\n\n,\n\n\nx\n\nj\n\n\n\n)\n=\n\ni\n\nth\n\n\n\n\u00a0element of\u00a0\n\n\n\nk\n\n\n\n\nx\n\nj\n\n\n\n\n\n\n\n{\\displaystyle \\mathbf {K} _{ij}=k(\\mathbf {x_{i}} ,\\mathbf {x_{j}} )=i^{\\text{th}}{\\text{ element of }}\\mathbf {k} _{\\mathbf {x_{j}} }}\n\n\n\n\n\n\n\n\n\n\u2020\n\n\n\n\n{\\displaystyle ^{\\dagger }}\n\nIt is possible, though non-trivial, to show that a representer theorem also holds for Tikhonov regularization in the vector-valued setting.[8]\nNote, the matrix-valued kernel \n\n\n\n\nK\n\n\n\n{\\displaystyle \\mathbf {K} }\n\n can also be defined by a scalar kernel \n\n\n\nR\n\n\n{\\displaystyle R}\n\n on the space \n\n\n\n\n\nX\n\n\n\u00d7\n{\n1\n,\n\u2026\n,\nD\n}\n\n\n{\\displaystyle {\\mathcal {X}}\\times \\{1,\\ldots ,D\\}}\n\n. An isometry exists between the Hilbert spaces associated with these two kernels:\nThe estimator of the vector-valued regularization framework can also be derived from a Bayesian viewpoint using Gaussian process methods in the case of a finite dimensional Reproducing kernel Hilbert space. The derivation is similar to the scalar-valued case Bayesian interpretation of regularization. The vector-valued function \n\n\n\n\n\nf\n\n\n\n\n{\\displaystyle {\\textbf {f}}}\n\n, consisting of \n\n\n\nD\n\n\n{\\displaystyle D}\n\n outputs \n\n\n\n\n\n{\n\nf\n\nd\n\n\n}\n\n\nd\n=\n1\n\n\nD\n\n\n\n\n{\\displaystyle \\left\\{f_{d}\\right\\}_{d=1}^{D}}\n\n, is assumed to follow a Gaussian process:\nwhere \n\n\n\n\n\nm\n\n\n:\n\n\nX\n\n\n\u2192\n\n\n\nR\n\n\n\nD\n\n\n\n\n{\\displaystyle {\\textbf {m}}:{\\mathcal {X}}\\to {\\textbf {R}}^{D}}\n\n is now a vector of the mean functions \n\n\n\n\n\n{\n\nm\n\nd\n\n\n(\n\n\nx\n\n\n)\n}\n\n\nd\n=\n1\n\n\nD\n\n\n\n\n{\\displaystyle \\left\\{m_{d}({\\textbf {x}})\\right\\}_{d=1}^{D}}\n\n for the outputs and \n\n\n\n\n\nK\n\n\n\n\n{\\displaystyle {\\textbf {K}}}\n\n is a positive definite matrix-valued function with entry \n\n\n\n(\n\n\nK\n\n\n(\n\n\nx\n\n\n,\n\n\n\nx\n\n\n\u2032\n\n)\n\n)\n\nd\n,\n\nd\n\u2032\n\n\n\n\n\n{\\displaystyle ({\\textbf {K}}({\\textbf {x}},{\\textbf {x}}'))_{d,d'}}\n\n corresponding to the covariance between the outputs \n\n\n\n\nf\n\nd\n\n\n(\n\n\nx\n\n\n)\n\n\n{\\displaystyle f_{d}({\\textbf {x}})}\n\n and \n\n\n\n\nf\n\n\nd\n\u2032\n\n\n\n(\n\n\n\nx\n\n\n\u2032\n\n)\n\n\n{\\displaystyle f_{d'}({\\textbf {x}}')}\n\n.\nFor a set of inputs \n\n\n\n\n\nX\n\n\n\n\n{\\displaystyle {\\textbf {X}}}\n\n, the prior distribution over the vector \n\n\n\n\n\nf\n\n\n(\n\n\nX\n\n\n)\n\n\n{\\displaystyle {\\textbf {f}}({\\textbf {X}})}\n\n is given by \n\n\n\n\n\nN\n\n\n(\n\n\nm\n\n\n(\n\n\nX\n\n\n)\n,\n\n\nK\n\n\n(\n\n\nX\n\n\n,\n\n\nX\n\n\n)\n)\n\n\n{\\displaystyle {\\mathcal {N}}({\\textbf {m}}({\\textbf {X}}),{\\textbf {K}}({\\textbf {X}},{\\textbf {X}}))}\n\n, where \n\n\n\n\n\nm\n\n\n(\n\n\nX\n\n\n)\n\n\n{\\displaystyle {\\textbf {m}}({\\textbf {X}})}\n\n is a vector that concatenates the mean vectors associated to the outputs and \n\n\n\n\n\nK\n\n\n(\n\n\nX\n\n\n,\n\n\nX\n\n\n)\n\n\n{\\displaystyle {\\textbf {K}}({\\textbf {X}},{\\textbf {X}})}\n\n is a block-partitioned matrix. The distribution of the outputs is taken to be Gaussian:\nwhere \n\n\n\n\u03a3\n\u2208\n\n\n\n\nR\n\n\n\n\nD\n\u00d7\nD\n\n\n\n\n{\\displaystyle \\Sigma \\in {\\mathcal {\\textbf {R}}}^{D\\times D}}\n\n is a diagonal matrix with elements \n\n\n\n\n\n{\n\n\u03c3\n\nd\n\n\n2\n\n\n}\n\n\nd\n=\n1\n\n\nD\n\n\n\n\n{\\displaystyle \\left\\{\\sigma _{d}^{2}\\right\\}_{d=1}^{D}}\n\n specifying the noise for each output. Using this form for the likelihood, the predictive distribution for a new vector \n\n\n\n\n\n\nx\n\n\n\n\u2217\n\n\n\n\n{\\displaystyle {\\textbf {x}}_{*}}\n\n is:\nwhere \n\n\n\n\n\nS\n\n\n\n\n{\\displaystyle {\\textbf {S}}}\n\n is the training data, and \n\n\n\n\u03d5\n\n\n{\\displaystyle \\phi }\n\n is a set of hyperparameters for \n\n\n\n\n\nK\n\n\n(\n\n\nx\n\n\n,\n\n\n\nx\n\n\n\u2032\n\n)\n\n\n{\\displaystyle {\\textbf {K}}({\\textbf {x}},{\\textbf {x}}')}\n\n and \n\n\n\n\u03a3\n\n\n{\\displaystyle \\Sigma }\n\n.\nEquations for \n\n\n\n\n\n\nf\n\n\n\n\u2217\n\n\n\n\n{\\displaystyle {\\textbf {f}}_{*}}\n\n and \n\n\n\n\n\n\nK\n\n\n\n\u2217\n\n\n\n\n{\\displaystyle {\\textbf {K}}_{*}}\n\n can then be obtained:\nwhere \n\n\n\n\n\u03a3\n\n=\n\u03a3\n\u2297\n\n\n\nI\n\n\n\nN\n\n\n,\n\n\n\nK\n\n\n\n\n\n\nx\n\n\n\n\u2217\n\n\n\n\n\u2208\n\n\n\n\nR\n\n\n\n\nD\n\u00d7\nN\nD\n\n\n\n\n{\\displaystyle {\\boldsymbol {\\Sigma }}=\\Sigma \\otimes {\\textbf {I}}_{N},{\\textbf {K}}_{{\\textbf {x}}_{*}}\\in {\\mathcal {\\textbf {R}}}^{D\\times ND}}\n\n has entries \n\n\n\n(\n\n\nK\n\n\n(\n\n\n\nx\n\n\n\n\u2217\n\n\n,\n\n\n\nx\n\n\n\nj\n\n\n)\n\n)\n\nd\n,\n\nd\n\u2032\n\n\n\n\n\n{\\displaystyle ({\\textbf {K}}({\\textbf {x}}_{*},{\\textbf {x}}_{j}))_{d,d'}}\n\n for \n\n\n\nj\n=\n1\n,\n\u22ef\n,\nN\n\n\n{\\displaystyle j=1,\\cdots ,N}\n\n and \n\n\n\nd\n,\n\nd\n\u2032\n\n=\n1\n,\n\u22ef\n,\nD\n\n\n{\\displaystyle d,d'=1,\\cdots ,D}\n\n. Note that the predictor \n\n\n\n\n\n\nf\n\n\n\n\u2217\n\n\n\n\n{\\displaystyle {\\textbf {f}}^{*}}\n\n is identical to the predictor derived in the regularization framework. For non-Gaussian likelihoods different methods such as Laplace approximation and variational methods are needed to approximate the estimators.\nA simple, but broadly applicable, class of multi-output kernels can be separated into the product of a kernel on the input-space and a kernel representing the correlations among the outputs:[8]\nIn matrix form: \n\n\n\n\nK\n\n(\n\nx\n\n,\n\n\nx\n\u2032\n\n\n)\n=\nk\n(\n\nx\n\n,\n\n\nx\n\u2032\n\n\n)\n\nB\n\n\n\n{\\displaystyle \\mathbf {K} (\\mathbf {x} ,\\mathbf {x'} )=k(\\mathbf {x} ,\\mathbf {x'} )\\mathbf {B} }\n\n \u00a0\u00a0 where \n\n\n\n\nB\n\n\n\n{\\displaystyle \\mathbf {B} }\n\n is a \n\n\n\nD\n\u00d7\nD\n\n\n{\\displaystyle D\\times D}\n\n symmetric and positive semi-definite matrix. Note, setting \n\n\n\n\nB\n\n\n\n{\\displaystyle \\mathbf {B} }\n\n to the identity matrix treats the outputs as unrelated and is equivalent to solving the scalar-output problems separately.\nFor a slightly more general form, adding several of these kernels yields sum of separable kernels (SoS kernels).\nOne way of obtaining \n\n\n\n\nk\n\nT\n\n\n\n\n{\\displaystyle k_{T}}\n\n is to specify a regularizer which limits the complexity of \n\n\n\nf\n\n\n{\\displaystyle f}\n\n in a desirable way, and then derive the corresponding kernel. For certain regularizers, this kernel will turn out to be separable.\nMixed-effect regularizer\nwhere:\nwhere \n\n\n\n\n1\n\n\n\u00a0is a\u00a0\n\nD\n\u00d7\nD\n\n\n{\\displaystyle \\mathbf {1} {\\text{ is a }}D\\times D}\n\n matrix with all entries equal to 1.\nThis regularizer is a combination of limiting the complexity of each component of the estimator (\n\n\n\n\nf\n\nl\n\n\n\n\n{\\displaystyle f_{l}}\n\n) and forcing each component of the estimator to be close to the mean of all the components. Setting \n\n\n\n\u03c9\n=\n0\n\n\n{\\displaystyle \\omega =0}\n\n treats all the components as independent and is the same as solving the scalar problems separately. Setting \n\n\n\n\u03c9\n=\n1\n\n\n{\\displaystyle \\omega =1}\n\n assumes all the components are explained by the same function.\nCluster-based regularizer\nwhere:\nwhere \n\n\n\n\n\nG\n\n\nl\n,\nq\n\n\n=\n\n\u03b5\n\n1\n\n\n\n\u03b4\n\nl\nq\n\n\n+\n(\n\n\u03b5\n\n2\n\n\n\u2212\n\n\u03b5\n\n1\n\n\n)\n\n\nM\n\n\nl\n,\nq\n\n\n\n\n{\\displaystyle \\mathbf {G} _{l,q}=\\varepsilon _{1}\\delta _{lq}+(\\varepsilon _{2}-\\varepsilon _{1})\\mathbf {M} _{l,q}}\n\n\nThis regularizer divides the components into \n\n\n\nr\n\n\n{\\displaystyle r}\n\n clusters and forces the components in each cluster to be similar.\nGraph regularizer\nwhere \n\n\n\n\nM\n\n\n\u00a0is a\u00a0\n\nD\n\u00d7\nD\n\n\n{\\displaystyle \\mathbf {M} {\\text{ is a }}D\\times D}\n\n matrix of weights encoding the similarities between the components\nwhere \n\n\n\n\nL\n\n=\n\nD\n\n\u2212\n\nM\n\n\n\n{\\displaystyle \\mathbf {L} =\\mathbf {D} -\\mathbf {M} }\n\n, \u00a0 \n\n\n\n\n\nD\n\n\nl\n,\nq\n\n\n=\n\n\u03b4\n\nl\n,\nq\n\n\n(\n\n\u2211\n\nh\n=\n1\n\n\nD\n\n\n\n\nM\n\n\nl\n,\nh\n\n\n+\n\n\nM\n\n\nl\n,\nq\n\n\n)\n\n\n{\\displaystyle \\mathbf {D} _{l,q}=\\delta _{l,q}(\\sum \\limits _{h=1}^{D}\\mathbf {M} _{l,h}+\\mathbf {M} _{l,q})}\n\n\nNote, \n\n\n\n\nL\n\n\n\n{\\displaystyle \\mathbf {L} }\n\n is the graph laplacian. See also: graph kernel.\nSeveral approaches to learning \n\n\n\n\nB\n\n\n\n{\\displaystyle \\mathbf {B} }\n\n from data have been proposed.[8] These include: performing a preliminary inference step to estimate \n\n\n\n\nB\n\n\n\n{\\displaystyle \\mathbf {B} }\n\n from the training data,[9] a proposal to learn \n\n\n\n\nB\n\n\n\n{\\displaystyle \\mathbf {B} }\n\n and \n\n\n\n\nf\n\n\n\n{\\displaystyle \\mathbf {f} }\n\n together based on the cluster regularizer,[15] and sparsity-based approaches which assume only a few of the features are needed.[16] [17]\nIn LMC, outputs are expressed as linear combinations of independent random functions such that the resulting covariance function (over all inputs and outputs) is a valid positive semidefinite function. Assuming \n\n\n\nD\n\n\n{\\displaystyle D}\n\n outputs \n\n\n\n\n\n{\n\nf\n\nd\n\n\n(\n\n\nx\n\n\n)\n}\n\n\nd\n=\n1\n\n\nD\n\n\n\n\n{\\displaystyle \\left\\{f_{d}({\\textbf {x}})\\right\\}_{d=1}^{D}}\n\n with \n\n\n\n\n\nx\n\n\n\u2208\n\n\n\n\nR\n\n\n\n\np\n\n\n\n\n{\\displaystyle {\\textbf {x}}\\in {\\mathcal {\\textbf {R}}}^{p}}\n\n, each \n\n\n\n\nf\n\nd\n\n\n\n\n{\\displaystyle f_{d}}\n\n is expressed as:\nwhere \n\n\n\n\na\n\nd\n,\nq\n\n\n\n\n{\\displaystyle a_{d,q}}\n\n are scalar coefficients and the independent functions \n\n\n\n\nu\n\nq\n\n\n(\n\n\nx\n\n\n)\n\n\n{\\displaystyle u_{q}({\\textbf {x}})}\n\n have zero mean and covariance cov\n\n\n\n[\n\nu\n\nq\n\n\n(\n\n\nx\n\n\n)\n,\n\nu\n\n\nq\n\u2032\n\n\n\n(\n\n\n\nx\n\n\n\u2032\n\n)\n]\n=\n\nk\n\nq\n\n\n(\n\n\nx\n\n\n,\n\n\n\nx\n\n\n\u2032\n\n)\n\n\n{\\displaystyle [u_{q}({\\textbf {x}}),u_{q'}({\\textbf {x}}')]=k_{q}({\\textbf {x}},{\\textbf {x}}')}\n\n if \n\n\n\nq\n=\n\nq\n\u2032\n\n\n\n{\\displaystyle q=q'}\n\n and 0 otherwise. The cross covariance between any two functions \n\n\n\n\nf\n\nd\n\n\n(\n\n\nx\n\n\n)\n\n\n{\\displaystyle f_{d}({\\textbf {x}})}\n\n and \n\n\n\n\nf\n\n\nd\n\u2032\n\n\n\n(\n\n\nx\n\n\n)\n\n\n{\\displaystyle f_{d'}({\\textbf {x}})}\n\n can then be written as:\nwhere the functions \n\n\n\n\nu\n\nq\n\n\ni\n\n\n(\n\n\nx\n\n\n)\n\n\n{\\displaystyle u_{q}^{i}({\\textbf {x}})}\n\n, with \n\n\n\nq\n=\n1\n,\n\u22ef\n,\nQ\n\n\n{\\displaystyle q=1,\\cdots ,Q}\n\n and \n\n\n\ni\n=\n1\n,\n\u22ef\n,\n\nR\n\nq\n\n\n\n\n{\\displaystyle i=1,\\cdots ,R_{q}}\n\n have zero mean and covariance cov\n\n\n\n[\n\nu\n\nq\n\n\ni\n\n\n(\n\n\nx\n\n\n)\n,\n\nu\n\n\nq\n\u2032\n\n\n\n\ni\n\u2032\n\n\n\n(\n\n\nx\n\n\n\n)\n\u2032\n\n]\n=\n\nk\n\nq\n\n\n(\n\n\nx\n\n\n,\n\n\n\nx\n\n\n\u2032\n\n)\n\n\n{\\displaystyle [u_{q}^{i}({\\textbf {x}}),u_{q'}^{i'}({\\textbf {x}})']=k_{q}({\\textbf {x}},{\\textbf {x}}')}\n\n if \n\n\n\ni\n=\n\ni\n\u2032\n\n\n\n{\\displaystyle i=i'}\n\n and \n\n\n\nq\n=\n\nq\n\u2032\n\n\n\n{\\displaystyle q=q'}\n\n. But \n\n\n\ncov\n\u2061\n[\n\nf\n\nd\n\n\n(\n\n\nx\n\n\n)\n,\n\nf\n\n\nd\n\u2032\n\n\n\n(\n\n\n\nx\n\n\n\u2032\n\n)\n]\n\n\n{\\displaystyle \\operatorname {cov} [f_{d}({\\textbf {x}}),f_{d'}({\\textbf {x}}')]}\n\n is given by \n\n\n\n(\n\n\nK\n\n\n(\n\n\nx\n\n\n,\n\n\n\nx\n\n\n\u2032\n\n)\n\n)\n\nd\n,\n\nd\n\u2032\n\n\n\n\n\n{\\displaystyle ({\\textbf {K}}({\\textbf {x}},{\\textbf {x}}'))_{d,d'}}\n\n. Thus the kernel \n\n\n\n\n\nK\n\n\n(\n\n\nx\n\n\n,\n\n\n\nx\n\n\n\u2032\n\n)\n\n\n{\\displaystyle {\\textbf {K}}({\\textbf {x}},{\\textbf {x}}')}\n\n can now be expressed as\nwhere each \n\n\n\n\n\n\nB\n\n\n\nq\n\n\n\u2208\n\n\n\n\nR\n\n\n\n\nD\n\u00d7\nD\n\n\n\n\n{\\displaystyle {\\textbf {B}}_{q}\\in {\\mathcal {\\textbf {R}}}^{D\\times D}}\n\n is known as a coregionalization matrix. Therefore, the kernel derived from LMC is a sum of the products of two covariance functions, one that models the dependence between the outputs, independently of the input vector \n\n\n\n\n\nx\n\n\n\n\n{\\displaystyle {\\textbf {x}}}\n\n (the coregionalization matrix \n\n\n\n\n\n\nB\n\n\n\nq\n\n\n\n\n{\\displaystyle {\\textbf {B}}_{q}}\n\n), and one that models the input dependence, independently of \n\n\n\n\n\n{\n\nf\n\nd\n\n\n(\n\n\nx\n\n\n)\n}\n\n\nd\n=\n1\n\n\nD\n\n\n\n\n{\\displaystyle \\left\\{f_{d}({\\textbf {x}})\\right\\}_{d=1}^{D}}\n\n(the covariance function \n\n\n\n\nk\n\nq\n\n\n(\n\n\nx\n\n\n,\n\n\n\nx\n\n\n\u2032\n\n)\n\n\n{\\displaystyle k_{q}({\\textbf {x}},{\\textbf {x}}')}\n\n).\nThe ICM is a simplified version of the LMC, with \n\n\n\nQ\n=\n1\n\n\n{\\displaystyle Q=1}\n\n. ICM assumes that the elements \n\n\n\n\nb\n\nd\n,\n\nd\n\u2032\n\n\n\nq\n\n\n\n\n{\\displaystyle b_{d,d'}^{q}}\n\n of the coregionalization matrix \n\n\n\n\n\nB\n\n\nq\n\n\n\n\n{\\displaystyle \\mathbf {B} _{q}}\n\n can be written as \n\n\n\n\nb\n\nd\n,\n\nd\n\u2032\n\n\n\nq\n\n\n=\n\nv\n\nd\n,\n\nd\n\u2032\n\n\n\n\nb\n\nq\n\n\n\n\n{\\displaystyle b_{d,d'}^{q}=v_{d,d'}b_{q}}\n\n, for some suitable coefficients \n\n\n\n\nv\n\nd\n,\n\nd\n\u2032\n\n\n\n\n\n{\\displaystyle v_{d,d'}}\n\n. With this form for \n\n\n\n\nb\n\nd\n,\n\nd\n\u2032\n\n\n\nq\n\n\n\n\n{\\displaystyle b_{d,d'}^{q}}\n\n:\nwhere\nIn this case, the coefficients\nand the kernel matrix for multiple outputs becomes \n\n\n\n\nK\n\n(\n\nx\n\n,\n\n\nx\n\n\u2032\n\n)\n=\nk\n(\n\nx\n\n,\n\n\nx\n\n\u2032\n\n)\n\nB\n\n\n\n{\\displaystyle \\mathbf {K} (\\mathbf {x} ,\\mathbf {x} ')=k(\\mathbf {x} ,\\mathbf {x} ')\\mathbf {B} }\n\n. ICM is much more restrictive than the LMC since it assumes that each basic covariance \n\n\n\n\nk\n\nq\n\n\n(\n\nx\n\n,\n\n\nx\n\n\u2032\n\n)\n\n\n{\\displaystyle k_{q}(\\mathbf {x} ,\\mathbf {x} ')}\n\n contributes equally to the construction of the autocovariances and cross covariances for the outputs. However, the computations required for the inference are greatly simplified.\nAnother simplified version of the LMC is the semiparametric latent factor model (SLFM), which corresponds to setting \n\n\n\n\nR\n\nq\n\n\n=\n1\n\n\n{\\displaystyle R_{q}=1}\n\n (instead of \n\n\n\nQ\n=\n1\n\n\n{\\displaystyle Q=1}\n\n as in ICM). Thus each latent function \n\n\n\n\nu\n\nq\n\n\n\n\n{\\displaystyle u_{q}}\n\n has its own covariance.\nWhile simple, the structure of separable kernels can be too limiting for some problems.\nNotable examples of non-separable kernels in the regularization literature include:\nIn the Bayesian perspective, LMC produces a separable kernel because the output functions evaluated at a point \n\n\n\n\n\nx\n\n\n\n\n{\\displaystyle {\\textbf {x}}}\n\n only depend on the values of the latent functions at \n\n\n\n\n\nx\n\n\n\n\n{\\displaystyle {\\textbf {x}}}\n\n. A non-trivial way to mix the latent functions is by convolving a base process with a smoothing kernel. If the base process is a Gaussian process, the convolved process is Gaussian as well. We can therefore exploit convolutions to construct covariance functions.[20] This method of producing non-separable kernels is known as process convolution. Process convolutions were introduced for multiple outputs in the machine learning community as \"dependent Gaussian processes\".[21]\nWhen implementing an algorithm using any of the kernels above, practical considerations of tuning the parameters and ensuring reasonable computation time must be considered.\nApproached from the regularization perspective, parameter tuning is similar to the scalar-valued case and can generally be accomplished with cross validation. Solving the required linear system is typically expensive in memory and time. If the kernel is separable, a coordinate transform can convert \n\n\n\n\nK\n\n(\n\nX\n\n,\n\nX\n\n)\n\n\n{\\displaystyle \\mathbf {K} (\\mathbf {X} ,\\mathbf {X} )}\n\n to a block-diagonal matrix, greatly reducing the computational burden by solving D independent subproblems (plus the eigendecomposition of \n\n\n\n\nB\n\n\n\n{\\displaystyle \\mathbf {B} }\n\n). In particular, for a least squares loss function (Tikhonov regularization), there exists a closed form solution for \n\n\n\n\n\n\n\nc\n\n\u00af\n\n\n\n\n\n{\\displaystyle {\\bar {\\mathbf {c} }}}\n\n:[8][14]\nThere are many works related to parameter estimation for Gaussian processes. Some methods such as maximization of the marginal likelihood (also known as evidence approximation, type II maximum likelihood, empirical Bayes), and least squares give point estimates of the parameter vector \n\n\n\n\u03d5\n\n\n{\\displaystyle \\phi }\n\n. There are also works employing a full Bayesian inference by assigning priors to \n\n\n\n\u03d5\n\n\n{\\displaystyle \\phi }\n\n and computing the posterior distribution through a sampling procedure. For non-Gaussian likelihoods, there is no closed form solution for the posterior distribution or for the marginal likelihood. However, the marginal likelihood can be approximated under a Laplace, variational Bayes or expectation propagation (EP) approximation frameworks for multiple output classification and used to find estimates for the hyperparameters.\nThe main computational problem in the Bayesian viewpoint is the same as the one appearing in regularization theory of inverting the matrix\nThis step is necessary for computing the marginal likelihood and the predictive distribution. For most proposed approximation methods to reduce computation, the computational efficiency gained is independent of the particular method employed (e.g. LMC, process convolution) used to compute the multi-output covariance matrix. A summary of different methods for reducing computational complexity in multi-output Gaussian processes is presented in.[8]", 
    "dbpedia_url": "http://dbpedia.org/resource/Kernel_methods_for_vector_output", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Kernel_methods_for_vector_output\n"
}