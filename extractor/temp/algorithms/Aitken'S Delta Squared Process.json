{
    "about": "In numerical analysis, Aitken's delta-squared process or Aitken Extrapolation is a series acceleration method, used for accelerating the rate of convergence of a sequence. It is named after Alexander Aitken, who introduced this method in 1926.[1] Its early form was known to Seki K\u014dwa (end of 17th century) and was found for rectification of the circle, i.e. the calculation of \u03c0. It is most useful for accelerating the convergence of a sequence that is converging linearly.", 
    "classification": "Numerical Analysis", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Aitken's_delta-squared_process\n", 
    "full_text": "In numerical analysis, Aitken's delta-squared process or Aitken Extrapolation is a series acceleration method, used for accelerating the rate of convergence of a sequence. It is named after Alexander Aitken, who introduced this method in 1926.[1] Its early form was known to Seki K\u014dwa (end of 17th century) and was found for rectification of the circle, i.e. the calculation of \u03c0. It is most useful for accelerating the convergence of a sequence that is converging linearly.\n\n\nGiven a sequence \n\n\n\nx\n=\n\n\n(\n\nx\n\nn\n\n\n)\n\n\nn\n\u2208\n\nN\n\n\n\n\n\n{\\displaystyle x={(x_{n})}_{n\\in \\mathbb {N} }}\n\n, one associates with this sequence the new sequence\nwhich can, with improved numerical stability, also be written as\nwhere\nand\nfor \n\n\n\nn\n=\n0\n,\n1\n,\n2\n,\n3\n,\n\u2026\n\n\n\n{\\displaystyle n=0,1,2,3,\\dots \\,}\n\n\n\nObviously, \n\n\n\nA\nx\n\n\n{\\displaystyle Ax}\n\n is ill-defined if \n\n\n\n\n\u0394\n\n2\n\n\nx\n\n\n{\\displaystyle \\Delta ^{2}x}\n\n contains a zero element, or equivalently, if the sequence of first differences has a repeating term. From a theoretical point of view, assuming that this occurs only for a finite number of indices, one could easily agree to consider the sequence \n\n\n\nA\nx\n\n\n{\\displaystyle Ax}\n\n restricted to indices \n\n\n\nn\n>\n\nn\n\n0\n\n\n\n\n{\\displaystyle n>n_{0}}\n\n with a sufficiently large \n\n\n\n\nn\n\n0\n\n\n\n\n{\\displaystyle n_{0}}\n\n. From a practical point of view, one does in general rather consider only the first few terms of the sequence, which usually provide the needed precision. Moreover, when numerically computing the sequence, one has to take care to stop the computation when rounding errors become too important in the denominator, where the \u0394\u00b2 operation may cancel too many significant digits. (It would be better for numerical calculation to use \n\n\n\n\u0394\n\nx\n\nn\n+\n1\n\n\n\u2212\n\u0394\n\nx\n\nn\n\n\n\u00a0\n=\n(\n\nx\n\nn\n+\n2\n\n\n\u2212\n\nx\n\nn\n+\n1\n\n\n)\n\u2212\n(\n\nx\n\nn\n+\n1\n\n\n\u2212\n\nx\n\nn\n\n\n)\n\u00a0\n\n\n{\\displaystyle \\Delta x_{n+1}-\\Delta x_{n}\\ =(x_{n+2}-x_{n+1})-(x_{n+1}-x_{n})\\ }\n\n rather than \n\n\n\n\nx\n\nn\n\n\n\u2212\n2\n\nx\n\nn\n+\n1\n\n\n+\n\nx\n\nn\n+\n2\n\n\n\u00a0\n\n\n{\\displaystyle x_{n}-2x_{n+1}+x_{n+2}\\ }\n\n .)\nAitken's delta-squared process is a method of acceleration of convergence, and a particular case of a nonlinear sequence transformation.\n\n\n\n\nx\n\n\n{\\displaystyle x}\n\n will converge linearly to \n\n\n\n\u2113\n\n\n{\\displaystyle \\ell }\n\n if there exists a number \u03bc \u2208 (0, 1) such that\nAitken's method will accelerate the sequence \n\n\n\n\nx\n\nn\n\n\n\n\n{\\displaystyle x_{n}}\n\n if \n\n\n\n\nlim\n\nn\n\u2192\n\u221e\n\n\n\n\n\n(\nA\nx\n\n)\n\nn\n\n\n\u2212\n\u2113\n\n\n\nx\n\nn\n\n\n\u2212\n\u2113\n\n\n\n=\n0.\n\n\n{\\displaystyle \\lim _{n\\to \\infty }{\\frac {(Ax)_{n}-\\ell }{x_{n}-\\ell }}=0.}\n\n\n\n\n\n\nA\n\n\n{\\displaystyle A}\n\n is not a linear operator, but a constant term drops out, viz: \n\n\n\nA\n[\nx\n\u2212\n\u2113\n]\n=\nA\nx\n\u2212\n\u2113\n\n\n{\\displaystyle A[x-\\ell ]=Ax-\\ell }\n\n, if \n\n\n\n\u2113\n\n\n{\\displaystyle \\ell }\n\n is a constant. This is clear from the expression of \n\n\n\nA\nx\n\n\n{\\displaystyle Ax}\n\n in terms of the finite difference operator \n\n\n\n\u0394\n\n\n{\\displaystyle \\Delta }\n\n.\nAlthough the new process does not in general converge quadratically, it can be shown that for a fixed point process, that is, for an iterated function sequence \n\n\n\n\nx\n\nn\n+\n1\n\n\n=\nf\n(\n\nx\n\nn\n\n\n)\n\n\n{\\displaystyle x_{n+1}=f(x_{n})}\n\n for some function \n\n\n\nf\n\n\n{\\displaystyle f}\n\n, converging to a fixed point, the convergence is quadratic. In this case, the technique is known as Steffensen's method.\nEmpirically, the A-operation eliminates the \"most important error term\". One can check this by considering a sequence of the form \n\n\n\n\nx\n\nn\n\n\n=\n\u2113\n+\n\na\n\nn\n\n\n+\n\nb\n\nn\n\n\n\n\n{\\displaystyle x_{n}=\\ell +a^{n}+b^{n}}\n\n, where \n\n\n\n0\n<\nb\n<\na\n<\n1\n\n\n{\\displaystyle 0<b<a<1}\n\n: The sequence \n\n\n\nA\nx\n\n\n{\\displaystyle Ax}\n\n will then go to the limit like \n\n\n\n\nb\n\nn\n\n\n\n\n{\\displaystyle b^{n}}\n\n goes to zero.\nOne can also show that if \n\n\n\nx\n\n\n{\\displaystyle x}\n\n goes to its limit \n\n\n\n\u2113\n\n\n{\\displaystyle \\ell }\n\n at a rate strictly greater than 1, \n\n\n\nA\nx\n\n\n{\\displaystyle Ax}\n\n does not have a better rate of convergence. (In practice, one rarely has e.g. quadratic convergence which would mean over 30 resp. 100 correct decimal places after 5 resp. 7 iterations (starting with 1 correct digit); usually no acceleration is needed in that case.)\nIn practice, \n\n\n\nA\nx\n\n\n{\\displaystyle Ax}\n\n converges much faster to the limit than \n\n\n\nx\n\n\n{\\displaystyle x}\n\n does, as demonstrated by the example calculations below. Usually, it is much cheaper to calculate \n\n\n\nA\nx\n\n\n{\\displaystyle Ax}\n\n (involving only calculation of differences, one multiplication and one division) than to calculate many more terms of the sequence \n\n\n\nx\n\n\n{\\displaystyle x}\n\n. Care must be taken, however, to avoid introducing errors due to insufficient precision when calculating the differences in the numerator and denominator of the expression.\nStarting with \n\n\n\n\na\n\n0\n\n\n=\n1\n:\n\n\n{\\displaystyle a_{0}=1:}\n\n\nThe following is an example of using the Aitken extrapolation to help find the limit of the sequence \n\n\n\n\nx\n\nn\n+\n1\n\n\n=\nf\n(\n\nx\n\nn\n\n\n)\n\n\n{\\displaystyle x_{n+1}=f(x_{n})}\n\n when given \n\n\n\n\nx\n\n0\n\n\n\n\n{\\displaystyle x_{0}}\n\n, which we assume to be the fixed point \n\n\n\n\u03b1\n=\nf\n(\n\u03b1\n)\n\n\n{\\displaystyle \\alpha =f(\\alpha )}\n\n. For instance, we could have \n\n\n\n\nx\n\nn\n+\n1\n\n\n=\n\n\n1\n2\n\n\n(\n\nx\n\nn\n\n\n+\n\n\n2\n\nx\n\nn\n\n\n\n\n)\n\n\n{\\displaystyle x_{n+1}={\\frac {1}{2}}(x_{n}+{\\frac {2}{x_{n}}})}\n\n with \n\n\n\n\nx\n\n0\n\n\n=\n1\n\n\n{\\displaystyle x_{0}=1}\n\n which has the fixed point \n\n\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\sqrt {2}}}\n\n so that \n\n\n\nf\n(\nx\n)\n=\n\n\n1\n2\n\n\n(\nx\n+\n\n\n2\nx\n\n\n)\n\n\n{\\displaystyle f(x)={\\frac {1}{2}}(x+{\\frac {2}{x}})}\n\n (see Methods of computing square roots).\nThis pseudo code also computes the Aitken approximation to \n\n\n\n\nf\n\u2032\n\n(\n\u03b1\n)\n\n\n{\\displaystyle f'(\\alpha )}\n\n. The Aitken extrapolates will be denoted by aitkenX. We must check if during the computation of the extrapolate the denominator becomes too small, which could happen if we already have a large amount of accuracy, since otherwise a large amount of error could be introduced. We denote this small number by epsilon.", 
    "name": "Aitken'S Delta Squared Process"
}