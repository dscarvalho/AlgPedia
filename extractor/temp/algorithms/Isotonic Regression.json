{
    "about": "In statistics, isotonic regression or monotonic regression is the technique of fitting a free-form line to a sequence of observations under the following constraints: the fitted free-form line has to be non-decreasing everywhere, and it has to lie as close to the observations as possible.", 
    "name": "Isotonic Regression", 
    "classification": "Numerical Analysis", 
    "full_text": "In statistics, isotonic regression or monotonic regression is the technique of fitting a free-form line to a sequence of observations under the following constraints: the fitted free-form line has to be non-decreasing everywhere, and it has to lie as close to the observations as possible.\n\n\nIsotonic regression has applications in statistical inference, for example, to fit of an isotonic curve to mean experimental results when an order is expected. A benefit of isotonic regression is that it does not assume any form for the target function, such as linearity assumed by linear regression.\nAnother application is nonmetric multidimensional scaling,[1] where a low-dimensional embedding for data points is sought such that order of distances between points in the embedding matches order of dissimilarity between points. Isotonic regression is used iteratively to fit ideal distances to preserve relative dissimilarity order.\nSoftware for computing isotone (monotonic) regression has been developed for the R statistical package [2] and the Python programming language.\nIn terms of numerical analysis, isotonic regression involves finding a weighted least-squares fit \n\n\n\nx\n\u2208\n\n\n\nR\n\n\n\nn\n\n\n\n\n{\\displaystyle x\\in {\\mathbb {R}}^{n}}\n\n to a vector \n\n\n\na\n\u2208\n\n\n\nR\n\n\n\nn\n\n\n\n\n{\\displaystyle a\\in {\\mathbb {R}}^{n}}\n\n with weights vector \n\n\n\nw\n\u2208\n\n\n\nR\n\n\n\nn\n\n\n\n\n{\\displaystyle w\\in {\\mathbb {R}}^{n}}\n\n subject to a set of non-contradictory constraints of the kind \n\n\n\n\nx\n\ni\n\n\n\u2264\n\nx\n\nj\n\n\n\n\n{\\displaystyle x_{i}\\leq x_{j}}\n\n. The usual choice for the constraints is \n\n\n\n\nx\n\ni\n\n\n\u2264\n\nx\n\ni\n+\n1\n\n\n\n\n{\\displaystyle x_{i}\\leq x_{i+1}}\n\n, or in other words: every point must be at least as high as the previous point.\nSuch constraints define a partial ordering or total ordering and can be represented as a directed graph \n\n\n\nG\n=\n(\nN\n,\nE\n)\n\n\n{\\displaystyle G=(N,E)}\n\n, where N is the set of variables (observed values) involved, and E is the set of pairs (i, j) for each constraint \n\n\n\n\nx\n\ni\n\n\n\u2264\n\nx\n\nj\n\n\n\n\n{\\displaystyle x_{i}\\leq x_{j}}\n\n. Thus, the IR problem corresponds to the following quadratic program (QP):\nIn the case when \n\n\n\nG\n=\n(\nN\n,\nE\n)\n\n\n{\\displaystyle G=(N,E)}\n\n is a total ordering, a simple iterative algorithm for solving this quadratic program is called the pool adjacent violators algorithm. Conversely, Best and Chakravarti (1990) studied the problem as an active set identification problem, and proposed a primal algorithm. These two algorithms can be seen as each other's dual, and both have a computational complexity of O(n).[3]\nTo illustrate the above, let the \n\n\n\n\nx\n\ni\n\n\n\u2264\n\nx\n\nj\n\n\n\n\n{\\displaystyle x_{i}\\leq x_{j}}\n\n constraints be \n\n\n\n\nx\n\n1\n\n\n\u2264\n\nx\n\n2\n\n\n\u2264\n\u2026\n\u2264\n\nx\n\nn\n\n\n\n\n{\\displaystyle x_{1}\\leq x_{2}\\leq \\ldots \\leq x_{n}}\n\n.\nThe isotonic estimator, \n\n\n\n\ng\n\n\u2217\n\n\n\n\n{\\displaystyle g^{*}}\n\n, minimizes the weighted least squares-like condition:\nwhere \n\n\n\n\n\nA\n\n\n\n\n{\\displaystyle {\\mathcal {A}}}\n\n is the set of all piecewise linear, non-decreasing, continuous functions and \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is a known function.", 
    "dbpedia_url": "http://dbpedia.org/resource/Isotonic_regression", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Isotonic_regression\n"
}