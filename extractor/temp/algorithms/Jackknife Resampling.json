{
    "about": "In statistics, the jackknife is a resampling technique especially useful for variance and bias estimation. The jackknife predates other common resampling methods such as the bootstrap. The jackknife estimator of a parameter is found by systematically leaving out each observation from a dataset and calculating the estimate and then finding the average of these calculations. Given a sample of size \n\n\n\nN\n\n\n{\\displaystyle N}\n\n, the jackknife estimate is found by aggregating the estimates of each \n\n\n\nN\n\u2212\n1\n\n\n{\\displaystyle N-1}\n\n-sized sub-sample.", 
    "name": "Jackknife Resampling", 
    "classification": "Computational Statistics", 
    "full_text": "In statistics, the jackknife is a resampling technique especially useful for variance and bias estimation. The jackknife predates other common resampling methods such as the bootstrap. The jackknife estimator of a parameter is found by systematically leaving out each observation from a dataset and calculating the estimate and then finding the average of these calculations. Given a sample of size \n\n\n\nN\n\n\n{\\displaystyle N}\n\n, the jackknife estimate is found by aggregating the estimates of each \n\n\n\nN\n\u2212\n1\n\n\n{\\displaystyle N-1}\n\n-sized sub-sample.\nThe jackknife technique was developed by Maurice Quenouille (1949, 1956). John Tukey (1958) expanded on the technique and proposed the name \"jackknife\" since, like a physical jack-knife (a compact folding knife), it is a rough-and-ready tool that can improvise a solution for a variety of problems even though specific problems may be more efficiently solved with a purpose-designed tool.[1]\nThe jackknife is a linear approximation of the bootstrap.[1]\n\n\nThe jackknife estimate of a parameter can be found by estimating the parameter for each subsample omitting the ith observation to estimate the previously unknown value of a parameter (say \n\n\n\n\n\n\n\nx\n\u00af\n\n\n\n\ni\n\n\n\n\n{\\displaystyle {\\bar {x}}_{i}}\n\n).[2]\nAn estimate of the variance of an estimator can be calculated using the jackknife technique.\nwhere \n\n\n\n\n\n\n\nx\n\u00af\n\n\n\n\ni\n\n\n\n\n{\\displaystyle {\\bar {x}}_{i}}\n\n is the parameter estimate based on leaving out the ith observation, and \n\n\n\n\n\n\n\nx\n\u00af\n\n\n\n\n\n(\n.\n)\n\n\n\n=\n\n\n1\nn\n\n\n\n\u2211\n\ni\n\n\nn\n\n\n\n\n\n\nx\n\u00af\n\n\n\n\ni\n\n\n\n\n{\\displaystyle {\\bar {x}}_{\\mathrm {(.)} }={\\frac {1}{n}}\\sum _{i}^{n}{\\bar {x}}_{i}}\n\n is the estimator based on all of the subsamples.[3][4]\nThe jackknife technique can be used to estimate the bias of an estimator calculated over the entire sample. Say \n\n\n\n\n\n\n\u03b8\n^\n\n\n\n\n\n{\\displaystyle {\\hat {\\theta }}}\n\n is the calculated estimator of the parameter of interest based on all \n\n\n\n\nn\n\n\n\n{\\displaystyle {n}}\n\n observations. Let\nwhere \n\n\n\n\n\n\n\n\u03b8\n^\n\n\n\n\n\n(\ni\n)\n\n\n\n\n\n{\\displaystyle {\\hat {\\theta }}_{\\mathrm {(i)} }}\n\n is the estimate of interest based on the sample with the ith observation removed, and \n\n\n\n\n\n\n\n\u03b8\n^\n\n\n\n\n\n(\n.\n)\n\n\n\n\n\n{\\displaystyle {\\hat {\\theta }}_{\\mathrm {(.)} }}\n\n is the average of these \"leave-one-out\" estimates. The jackknife estimate of the bias of \n\n\n\n\n\n\n\u03b8\n^\n\n\n\n\n\n{\\displaystyle {\\hat {\\theta }}}\n\n is given by:\nand the resulting bias-corrected jackknife estimate of \n\n\n\n\u03b8\n\n\n{\\displaystyle \\theta }\n\n is given by:\nThis removes the bias in the special case that the bias is \n\n\n\nO\n(\n\nN\n\n\u2212\n1\n\n\n)\n\n\n{\\displaystyle O(N^{-1})}\n\n and to \n\n\n\nO\n(\n\nN\n\n\u2212\n2\n\n\n)\n\n\n{\\displaystyle O(N^{-2})}\n\n in other cases.[1]\nThis provides an estimated correction of bias due to the estimation method. The jackknife does not correct for a biased sample.", 
    "dbpedia_url": "http://dbpedia.org/resource/Jackknife_resampling", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Jackknife_resampling\n"
}