{
    "about": "Sequential minimal optimization (SMO) is an algorithm for solving the quadratic programming (QP) problem that arises during the training of support vector machines. It was invented by John Platt in 1998 at Microsoft Research.[1] SMO is widely used for training support vector machines and is implemented by the popular LIBSVM tool.[2][3] The publication of the SMO algorithm in 1998 has generated a lot of excitement in the SVM community, as previously available methods for SVM training were much more complex and required expensive third-party QP solvers.[4]", 
    "name": "Sequential Minimal Optimization", 
    "classification": "Optimization Algorithms And Methods", 
    "full_text": "Sequential minimal optimization (SMO) is an algorithm for solving the quadratic programming (QP) problem that arises during the training of support vector machines. It was invented by John Platt in 1998 at Microsoft Research.[1] SMO is widely used for training support vector machines and is implemented by the popular LIBSVM tool.[2][3] The publication of the SMO algorithm in 1998 has generated a lot of excitement in the SVM community, as previously available methods for SVM training were much more complex and required expensive third-party QP solvers.[4]\n\n\nConsider a binary classification problem with a dataset (x1, y1), ..., (xn, yn), where xi is an input vector and yi \u2208 {-1, +1} is a binary label corresponding to it. A soft-margin support vector machine is trained by solving a quadratic programming problem, which is expressed in the dual form as follows:\nwhere C is an SVM hyperparameter and K(xi, xj) is the kernel function, both supplied by the user; and the variables \n\n\n\n\n\u03b1\n\ni\n\n\n\n\n{\\displaystyle \\alpha _{i}}\n\n are Lagrange multipliers.\nSMO is an iterative algorithm for solving the optimization problem described above. SMO breaks this problem into a series of smallest possible sub-problems, which are then solved analytically. Because of the linear equality constraint involving the Lagrange multipliers \n\n\n\n\n\u03b1\n\ni\n\n\n\n\n{\\displaystyle \\alpha _{i}}\n\n, the smallest possible problem involves two such multipliers. Then, for any two multipliers \n\n\n\n\n\u03b1\n\n1\n\n\n\n\n{\\displaystyle \\alpha _{1}}\n\n and \n\n\n\n\n\u03b1\n\n2\n\n\n\n\n{\\displaystyle \\alpha _{2}}\n\n, the constraints are reduced to:\nand this reduced problem can be solved analytically: one needs to find a minimum of a one-dimensional quadratic function. \n\n\n\nk\n\n\n{\\displaystyle k}\n\n is the negative of the sum over the rest of terms in the equality constraint, which is fixed in each iteration.\nThe algorithm proceeds as follows:\nWhen all the Lagrange multipliers satisfy the KKT conditions (within a user-defined tolerance), the problem has been solved. Although this algorithm is guaranteed to converge, heuristics are used to choose the pair of multipliers so as to accelerate the rate of convergence. This is critical for large data sets since there are n(n \u2212 1) possible choices for \n\n\n\n\n\u03b1\n\ni\n\n\n\n\n{\\displaystyle \\alpha _{i}}\n\n and \n\n\n\n\n\u03b1\n\nj\n\n\n\n\n{\\displaystyle \\alpha _{j}}\n\n.\nThe first approach to splitting large SVM learning problems into a series of smaller optimization tasks was proposed by Bernhard E Boser, Isabelle M Guyon, Vladimir Vapnik.[5] It is known as the \"chunking algorithm\". The algorithm starts with a random subset of the data, solves this problem, and iteratively adds examples which violate the optimality conditions. One disadvantage of this algorithm is that it is necessary to solve QP-problems scaling with the number of SVs. On real world sparse data sets, SMO can be more than 1000 times faster than the chunking algorithm.[1]\nIn 1997, E. Osuna, R. Freund, and F. Girosi proved a theorem which suggests a whole new set of QP algorithms for SVMs.[6] By the virtue of this theorem a large QP problem can be broken down into a series of smaller QP sub-problems. A sequence of QP sub-problems that always add at least one violator of the Karush\u2013Kuhn\u2013Tucker (KKT) conditions is guaranteed to converge. The chunking algorithm obeys the conditions of the theorem, and hence will converge.[1] The SMO algorithm can be considered a special case of the Osuna algorithm, where the size of the optimization is two and both Lagrange multipliers are replaced at every step with new multipliers that are chosen via good heuristics.[1]\nThe SMO algorithm is closely related to a family of optimization algorithms called Bregman methods or row-action methods. These methods solve convex programming problems with linear constraints. They are iterative methods where each step projects the current primal point onto each constraint.[1]", 
    "dbpedia_url": "http://dbpedia.org/resource/Sequential_minimal_optimization", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Sequential_minimal_optimization\n"
}