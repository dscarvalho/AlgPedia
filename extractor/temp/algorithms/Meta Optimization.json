{
    "about": "In numerical optimization, meta-optimization is the use of one optimization method to tune another optimization method. Meta-optimization is reported to have been used as early as in the late 1970s by Mercer and Sampson[1] for finding optimal parameter settings of a genetic algorithm.", 
    "classification": "Evolutionary Algorithms", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Meta-optimization\n", 
    "full_text": "In numerical optimization, meta-optimization is the use of one optimization method to tune another optimization method. Meta-optimization is reported to have been used as early as in the late 1970s by Mercer and Sampson[1] for finding optimal parameter settings of a genetic algorithm.\nMeta-optimization and related concepts are also known in the literature as meta-evolution, super-optimization, automated parameter calibration, hyper-heuristics, etc.\n\n\nOptimization methods such as genetic algorithm and differential evolution have several parameters that govern their behaviour and efficiency in optimizing a given problem and these parameters must be chosen by the practitioner to achieve satisfactory results. Selecting the behavioural parameters by hand is a laborious task that is susceptible to human misconceptions of what makes the optimizer perform well.\nThe behavioural parameters of an optimizer can be varied and the optimization performance plotted as a landscape. This is computationally feasible for optimizers with few behavioural parameters and optimization problems that are fast to compute, but when the number of behavioural parameters increases the time usage for computing such a performance landscape increases exponentially. This is the curse of dimensionality for the search-space consisting of an optimizer's behavioural parameters. An efficient method is therefore needed to search the space of behavioural parameters.\nA simple way of finding good behavioural parameters for an optimizer is to employ another overlaying optimizer, called the meta-optimizer. There are different ways of doing this depending on whether the behavioural parameters to be tuned are real-valued or discrete-valued, and depending on what performance measure is being used, etc.\nMeta-optimizing the parameters of a genetic algorithm was done by Grefenstette [2] and Keane,[3] amongst others, and experiments with meta-optimizing both the parameters and the genetic operators were reported by B\u00e4ck.[4] Meta-optimization of the COMPLEX-RF algorithm was done by Krus and Andersson,[5] and, [6] where performance index of optimization based on information theory was introduced and further developed. Meta-optimization of particle swarm optimization was done by Meissner et al.[7] as well as by Pedersen and Chipperfield,[8] who also meta-optimized differential evolution.[9] Birattari et al.[10][11] meta-optimized ant colony optimization. Statistical models have also been used to reveal more about the relationship between choices of behavioural parameters and optimization performance, see for example Francois and Lavergne,[12] and Nannen and Eiben.[13] A comparison of various meta-optimization techniques was done by Smit and Eiben.[14]", 
    "name": "Meta Optimization"
}