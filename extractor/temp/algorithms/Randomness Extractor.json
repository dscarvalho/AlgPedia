{
    "about": "A randomness extractor, often simply called an \"extractor\", is a function, which being applied to output from a weakly random entropy source, together with a short, uniformly random seed, generates a highly random output that appears independent from the source and uniformly distributed.[1] Examples of weakly random sources include radioactive decay or thermal noise; the only restriction on possible sources is that there is no way they can be fully controlled, calculated or predicted, and that a lower bound on their entropy rate can be established. For a given source, a randomness extractor can even be considered to be a true random number generator (TRNG); but there is no single extractor that has been proven to produce truly random output from any type of weakly random source.", 
    "name": "Randomness Extractor", 
    "classification": "Cryptographic Algorithms", 
    "full_text": "A randomness extractor, often simply called an \"extractor\", is a function, which being applied to output from a weakly random entropy source, together with a short, uniformly random seed, generates a highly random output that appears independent from the source and uniformly distributed.[1] Examples of weakly random sources include radioactive decay or thermal noise; the only restriction on possible sources is that there is no way they can be fully controlled, calculated or predicted, and that a lower bound on their entropy rate can be established. For a given source, a randomness extractor can even be considered to be a true random number generator (TRNG); but there is no single extractor that has been proven to produce truly random output from any type of weakly random source.\nSometimes the term \"bias\" is used to denote a weakly random source's departure from uniformity, and in older literature, some extractors are called unbiasing algorithms,[2] as they take the randomness from a so-called \"biased\" source and output a distribution that appears unbiased. The weakly random source will always be longer than the extractor's output, but an efficient extractor is one that lowers this ratio of lengths as much as possible, while simultaneously keeping the seed length low. Intuitively, this means that as much randomness as possible has been \"extracted\" from the source.\nNote that an extractor has some conceptual similarities with a pseudorandom generator (PRG), but the two concepts are not identical. Both are functions that take as input a small, uniformly random seed and produce a longer output that \"looks\" uniformly random. Some pseudorandom generators are, in fact, also extractors. (When a PRG is based on the existence of hard-core predicates, one can think of the weakly random source as a set of truth tables of such predicates and prove that the output is statistically close to uniform.[3]) However, the general PRG definition does not specify that a weakly random source must be used, and while in the case of an extractor, the output should be statistically close to uniform, in a PRG it is only required to be computationally indistinguishable from uniform, a somewhat weaker concept.\nNIST Special Publication 800-90B (draft) recommends several extractors, including the SHA hash family and states that if the amount of entropy input is twice the number of bits output from them, that output can be considered essentially fully random.[4]\n\n\nThe min-entropy of a distribution \n\n\n\nX\n\n\n{\\displaystyle X}\n\n (denoted \n\n\n\n\nH\n\n\u221e\n\n\n(\nX\n)\n\n\n{\\displaystyle H_{\\infty }(X)}\n\n), is the largest real number \n\n\n\nk\n\n\n{\\displaystyle k}\n\n such that \n\n\n\nPr\n[\nX\n=\nx\n]\n\u2264\n\n2\n\n\u2212\nk\n\n\n\n\n{\\displaystyle \\Pr[X=x]\\leq 2^{-k}}\n\n for every \n\n\n\nx\n\n\n{\\displaystyle x}\n\n in the range of \n\n\n\nX\n\n\n{\\displaystyle X}\n\n. In essence, this measures how likely \n\n\n\nX\n\n\n{\\displaystyle X}\n\n is to take its most likely value, giving a worst-case bound on how random \n\n\n\nX\n\n\n{\\displaystyle X}\n\n appears. Letting \n\n\n\n\nU\n\n\u2113\n\n\n\n\n{\\displaystyle U_{\\ell }}\n\n denote the uniform distribution over \n\n\n\n{\n0\n,\n1\n\n}\n\n\u2113\n\n\n\n\n{\\displaystyle \\{0,1\\}^{\\ell }}\n\n, clearly \n\n\n\n\nH\n\n\u221e\n\n\n(\n\nU\n\n\u2113\n\n\n)\n=\n\u2113\n\n\n{\\displaystyle H_{\\infty }(U_{\\ell })=\\ell }\n\n.\nFor an n-bit distribution \n\n\n\nX\n\n\n{\\displaystyle X}\n\n with min-entropy k, we say that \n\n\n\nX\n\n\n{\\displaystyle X}\n\n is an \n\n\n\n(\nn\n,\nk\n)\n\n\n{\\displaystyle (n,k)}\n\n distribution.\nDefinition (Extractor): (k,\u00a0\u03b5)-extractor\nLet \n\n\n\n\nExt\n\n:\n{\n0\n,\n1\n\n}\n\nn\n\n\n\u00d7\n{\n0\n,\n1\n\n}\n\nd\n\n\n\u2192\n{\n0\n,\n1\n\n}\n\nm\n\n\n\n\n{\\displaystyle {\\text{Ext}}:\\{0,1\\}^{n}\\times \\{0,1\\}^{d}\\to \\{0,1\\}^{m}}\n\n be a function that takes as input a sample from an \n\n\n\n(\nn\n,\nk\n)\n\n\n{\\displaystyle (n,k)}\n\n distribution \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and a d-bit seed from \n\n\n\n\nU\n\nd\n\n\n\n\n{\\displaystyle U_{d}}\n\n, and outputs an m-bit string. \n\n\n\n\nExt\n\n\n\n{\\displaystyle {\\text{Ext}}}\n\n is a (k,\u00a0\u03b5)-extractor, if for all \n\n\n\n(\nn\n,\nk\n)\n\n\n{\\displaystyle (n,k)}\n\n distributions \n\n\n\nX\n\n\n{\\displaystyle X}\n\n, the output distribution of \n\n\n\n\nExt\n\n\n\n{\\displaystyle {\\text{Ext}}}\n\n is \u03b5-close to \n\n\n\n\nU\n\nm\n\n\n\n\n{\\displaystyle U_{m}}\n\n.\nIn the above definition, \u03b5-close refers to statistical distance.\nIntuitively, an extractor takes a weakly random n-bit input and a short, uniformly random seed and produces an m-bit output that looks uniformly random. The aim is to have a low \n\n\n\nd\n\n\n{\\displaystyle d}\n\n (i.e. to use as little uniform randomness as possible) and as high an \n\n\n\nm\n\n\n{\\displaystyle m}\n\n as possible (i.e. to get out as many close-to-random bits of output as we can).\nAn extractor is strong if concatenating the seed with the extractor's output yields a distribution that is still close to uniform.\nDefinition (Strong Extractor): A \n\n\n\n(\nk\n,\n\u03f5\n)\n\n\n{\\displaystyle (k,\\epsilon )}\n\n-strong extractor is a function\nsuch that for every \n\n\n\n(\nn\n,\nk\n)\n\n\n{\\displaystyle (n,k)}\n\n distribution \n\n\n\nX\n\n\n{\\displaystyle X}\n\n the distribution \n\n\n\n\nU\n\nd\n\n\n\u2218\n\nExt\n\n(\nX\n,\n\nU\n\nd\n\n\n)\n\n\n{\\displaystyle U_{d}\\circ {\\text{Ext}}(X,U_{d})}\n\n (the two copies of \n\n\n\n\nU\n\nd\n\n\n\n\n{\\displaystyle U_{d}}\n\n denote the same random variable) is \n\n\n\n\u03f5\n\n\n{\\displaystyle \\epsilon }\n\n-close to the uniform distribution on \n\n\n\n{\n0\n,\n1\n\n}\n\nm\n+\nd\n\n\n\n\n{\\displaystyle \\{0,1\\}^{m+d}}\n\n.\nUsing the probabilistic method, it can be shown that there exists a (k,\u00a0\u03b5)-extractor, i.e. that the construction is possible. However, it is usually not enough merely to show that an extractor exists. An explicit construction is needed, which is given as follows:\nDefinition (Explicit Extractor): For functions k(n), \u03b5(n), d(n), m(n) a family Ext\u00a0=\u00a0{Extn} of functions\nis an explicit (k,\u00a0\u03b5)-extractor, if Ext(x,\u00a0y) can be computed in polynomial time (in its input length) and for every n, Extn is a (k(n),\u00a0\u03b5(n))-extractor.\nBy the probabilistic method, it can be shown that there exists a (k,\u00a0\u03b5)-extractor with seed length\nand output length\nA variant of the randomness extractor with weaker properties is the disperser.\nOne of the most important aspects of cryptography is random key generation.[6] It is often necessary to generate secret and random keys from sources that are semi-secret or which may be compromised to some degree. By taking a single, short (and secret) random key as a source, an extractor can be used to generate a longer pseudo-random key, which then can be used for public key encryption. More specifically, when a strong extractor is used its output will appear be uniformly random, even to someone who sees part (but not all) of the source. For example, if the source is known but the seed is not known (or vice versa). This property of extractors is particularly useful in what is commonly called Exposure-Resilient cryptography in which the desired extractor is used as an Exposure-Resilient Function (ERF). Exposure-Resilient cryptography takes into account that the fact that it is difficult to keep secret the initial exchange of data which often takes place during the initialization of an encryption application e.g., the sender of encrypted information has to provide the receivers with information which is required for decryption.\nThe following paragraphs define and establish an important relationship between two kinds of ERF--k-ERF and k-APRF--which are useful in Exposure-Resilient cryptography.\nDefinition (k-ERF): An adaptive k-ERF is a function \n\n\n\nf\n\n\n{\\displaystyle f}\n\n where, for a random input \n\n\n\nr\n\n\n{\\displaystyle r}\n\n , when a computationally unbounded adversary \n\n\n\nA\n\n\n{\\displaystyle A}\n\n can adaptively read all of \n\n\n\nr\n\n\n{\\displaystyle r}\n\n except for \n\n\n\nk\n\n\n{\\displaystyle k}\n\n bits, \n\n\n\n\n|\n\nPr\n{\n\nA\n\nr\n\n\n(\nf\n(\nr\n)\n)\n=\n1\n}\n\u2212\nPr\n{\n\nA\n\nr\n\n\n(\nR\n)\n=\n1\n}\n\n|\n\n\u2264\n\u03f5\n(\nn\n)\n\n\n{\\displaystyle |\\Pr\\{A^{r}(f(r))=1\\}-\\Pr\\{A^{r}(R)=1\\}|\\leq \\epsilon (n)}\n\n for some negligible function \n\n\n\n\u03f5\n(\nn\n)\n\n\n{\\displaystyle \\epsilon (n)}\n\n (defined below).\nThe goal is to construct an adaptive ERF whose output is highly random and uniformly distributed. But a stronger condition is often needed in which every output occurs with almost uniform probability. For this purpose Almost-Perfect Resilient Functions (APRF) are used. The definition of an APRF is as follows:\nDefinition (k-APRF): A \n\n\n\nk\n=\nk\n(\nn\n)\n\n\n{\\displaystyle k=k(n)}\n\n APRF is a function \n\n\n\nf\n\n\n{\\displaystyle f}\n\n where, for any setting of \n\n\n\nn\n\u2212\nk\n\n\n{\\displaystyle n-k}\n\n bits of the input \n\n\n\nr\n\n\n{\\displaystyle r}\n\n to any fixed values, the probability vector \n\n\n\np\n\n\n{\\displaystyle p}\n\n of the output \n\n\n\nf\n(\nr\n)\n\n\n{\\displaystyle f(r)}\n\n over the random choices for the \n\n\n\nk\n\n\n{\\displaystyle k}\n\n remaining bits satisfies \n\n\n\n\n|\n\n\np\n\ni\n\n\n\u2212\n\n2\n\n\u2212\nm\n\n\n\n|\n\n<\n\n2\n\n\u2212\nm\n\n\n\u03f5\n(\nn\n)\n\n\n{\\displaystyle |p_{i}-2^{-m}|<2^{-m}\\epsilon (n)}\n\n for all \n\n\n\ni\n\n\n{\\displaystyle i}\n\n and for some negligible function \n\n\n\n\u03f5\n(\nn\n)\n\n\n{\\displaystyle \\epsilon (n)}\n\n.\nKamp and Zuckerman[7] have proved a theorem stating that if a function \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is a k-APRF, then \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is also a k-ERF. More specifically, any extractor having sufficiently small error and taking as input an oblivious, bit-fixing source is also an APRF and therefore also a k-ERF. A more specific extractor is expressed in this lemma:\nLemma: Any \n\n\n\n\n2\n\n\u2212\nm\n\n\n\u03f5\n(\nn\n)\n\n\n{\\displaystyle 2^{-m}\\epsilon (n)}\n\n-extractor \n\n\n\nf\n:\n{\n0\n,\n1\n\n}\n\nn\n\n\n\u2192\n{\n0\n,\n1\n\n}\n\nm\n\n\n\n\n{\\displaystyle f:\\{0,1\\}^{n}\\rightarrow \\{0,1\\}^{m}}\n\n for the set of \n\n\n\n(\nn\n,\nk\n)\n\n\n{\\displaystyle (n,k)}\n\n oblivious bit-fixing sources, where \n\n\n\n\u03f5\n(\nn\n)\n\n\n{\\displaystyle \\epsilon (n)}\n\n is negligible, is also a k-APRF.\nThis lemma is proved by Kamp and Zuckerman.[7] The lemma is proved by examining the distance from uniform of the output, which in a \n\n\n\n\n2\n\n\u2212\nm\n\n\n\u03f5\n(\nn\n)\n\n\n{\\displaystyle 2^{-m}\\epsilon (n)}\n\n-extractor obviously is at most\n\n\n\n\n2\n\n\u2212\nm\n\n\n\u03f5\n(\nn\n)\n\n\n{\\displaystyle 2^{-m}\\epsilon (n)}\n\n, which satisfies the condition of the APRF.\nThe lemma leads to the following theorem, stating that there in fact exists a k-APRF function as described:\nTheorem (existence): For any positive constant \n\n\n\n\u03b3\n\u2264\n\n\n1\n2\n\n\n\n\n{\\displaystyle \\gamma \\leq {\\frac {1}{2}}}\n\n, there exists an explicit k-APRF \n\n\n\nf\n:\n{\n0\n,\n1\n\n}\n\nn\n\n\n\u2192\n{\n0\n,\n1\n\n}\n\nm\n\n\n\n\n{\\displaystyle f:\\{0,1\\}^{n}\\rightarrow \\{0,1\\}^{m}}\n\n, computable in a linear number of arithmetic operations on \n\n\n\nm\n\n\n{\\displaystyle m}\n\n-bit strings, with \n\n\n\nm\n=\n\u03a9\n(\n\nn\n\n2\n\u03b3\n\n\n)\n\n\n{\\displaystyle m=\\Omega (n^{2\\gamma })}\n\n and \n\n\n\nk\n=\n\nn\n\n\n\n1\n2\n\n\n+\n\u03b3\n\n\n\n\n{\\displaystyle k=n^{{\\frac {1}{2}}+\\gamma }}\n\n.\nDefinition (negligible function): In the proof of this theorem, we need a definition of a negligible function. A function \n\n\n\n\u03f5\n(\nn\n)\n\n\n{\\displaystyle \\epsilon (n)}\n\n is defined as being negligible if \n\n\n\n\u03f5\n(\nn\n)\n=\nO\n(\n\n\n1\n\nn\n\nc\n\n\n\n\n)\n\n\n{\\displaystyle \\epsilon (n)=O({\\frac {1}{n^{c}}})}\n\n for all constants \n\n\n\nc\n\n\n{\\displaystyle c}\n\n.\nProof: Consider the following \n\n\n\n\u03f5\n\n\n{\\displaystyle \\epsilon }\n\n-extractor: The function \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is an extractor for the set of \n\n\n\n(\nn\n,\n\u03b4\nn\n)\n\n\n{\\displaystyle (n,\\delta n)}\n\n oblivious bit-fixing source: \n\n\n\nf\n:\n{\n0\n,\n1\n\n}\n\nn\n\n\n\u2192\n{\n0\n,\n1\n\n}\n\nm\n\n\n\n\n{\\displaystyle f:\\{0,1\\}^{n}\\rightarrow \\{0,1\\}^{m}}\n\n. \n\n\n\nf\n\n\n{\\displaystyle f}\n\n has \n\n\n\nm\n=\n\u03a9\n(\n\n\u03b4\n\n2\n\n\nn\n)\n\n\n{\\displaystyle m=\\Omega (\\delta ^{2}n)}\n\n, \n\n\n\n\u03f5\n=\n\n2\n\n\u2212\nc\nm\n\n\n\n\n{\\displaystyle \\epsilon =2^{-cm}}\n\n and \n\n\n\nc\n>\n1\n\n\n{\\displaystyle c>1}\n\n.\nThe proof of this extractor's existence with \n\n\n\n\u03b4\n\u2264\n1\n\n\n{\\displaystyle \\delta \\leq 1}\n\n, as well as the fact that it is computable in linear computing time on the length of \n\n\n\nm\n\n\n{\\displaystyle m}\n\n can be found in the paper by Jesse Kamp and David Zuckerman (p.\u00a01240).\nThat this extractor fulfills the criteria of the lemma is trivially true as \n\n\n\n\u03f5\n=\n\n2\n\n\u2212\nc\nm\n\n\n\n\n{\\displaystyle \\epsilon =2^{-cm}}\n\n is a negligible function.\nThe size of \n\n\n\nm\n\n\n{\\displaystyle m}\n\n is:\nSince we know \n\n\n\n\u03b4\n\u2264\n1\n\n\n{\\displaystyle \\delta \\leq 1}\n\n then the lower bound on \n\n\n\nm\n\n\n{\\displaystyle m}\n\n is dominated by \n\n\n\nn\n\n\n{\\displaystyle n}\n\n. In the last step we use the fact that \n\n\n\n\u03b3\n\u2264\n\n\n1\n2\n\n\n\n\n{\\displaystyle \\gamma \\leq {\\frac {1}{2}}}\n\n which means that the power of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n is at most \n\n\n\n1\n\n\n{\\displaystyle 1}\n\n. And since \n\n\n\nn\n\n\n{\\displaystyle n}\n\n is a positive integer we know that \n\n\n\n\nn\n\n2\n\u03b3\n\n\n\n\n{\\displaystyle n^{2\\gamma }}\n\n is at most \n\n\n\nn\n\n\n{\\displaystyle n}\n\n.\nThe value of \n\n\n\nk\n\n\n{\\displaystyle k}\n\n is calculated by using the definition of the extractor, where we know:\nand by using the value of \n\n\n\nm\n\n\n{\\displaystyle m}\n\n we have:\nUsing this value of \n\n\n\nm\n\n\n{\\displaystyle m}\n\n we account for the worst case, where \n\n\n\nk\n\n\n{\\displaystyle k}\n\n is on its lower bound. Now by algebraic calculations we get:\nWhich inserted in the value of \n\n\n\nk\n\n\n{\\displaystyle k}\n\n gives\nwhich proves that there exists an explicit k-APRF extractor with the given properties. \n\n\n\n\u25fb\n\n\n{\\displaystyle \\Box }\n\n\nPerhaps the earliest example is due to John von Neumann. His extractor took successive pairs of consecutive bits (non-overlapping) from the input stream. If the two bits matched, no output was generated. If the bits differed, the value of the first bit was output. The Von Neumann extractor can be shown to produce a uniform output even if the distribution of input bits is not uniform so long as each bit has the same probability of being one and there is no correlation between successive bits.[8]\nThus, it takes as input a Bernoulli sequence with p not necessarily equal to 1/2, and outputs a Bernoulli sequence with \n\n\n\np\n=\n1\n\n/\n\n2.\n\n\n{\\displaystyle p=1/2.}\n\n More generally, it applies to any exchangeable sequence \u2013 it only relies on the fact that for any pair, 01 and 10 are equally likely: for independent trials, these have probabilities \n\n\n\np\n\u22c5\nq\n=\nq\n\u22c5\np\n\n\n{\\displaystyle p\\cdot q=q\\cdot p}\n\n, while for an exchangeable sequence the probability may be more complicated, but both are equally likely.\nAnother approach is to use the output of a chaos machine applied to the input stream. This approach generally relies on properties of the chaotic systems. Input bits are pushed to the machine, evolving orbits and trajectories in multiple dynamical systems. Thus, small difference in the input produces a very different output. Machine have a uniform output even if the distribution of input bits is not uniform or have serious flaws, therefore they could be used with weak entropy sources. Additionally, scheme allows specification by three parameters: time cost, memory required, and secret key; that can be used to increase complexity, quality, and security of output stream.\nIt is also possible to use cryptographic hash function as randomness extractor. However, not every algorithm is suitable to use in such application.\nRandomness extractors are used widely in cryptographic applications, whereby a cryptographic hash function is applied to a high-entropy, but non-uniform source, such as disk drive timing information or keyboard delays, to yield a uniformly random result.\nRandomness extractors have played a part in recent developments in quantum cryptography, where photons are used by the randomness extractor to generate secure random bits.[1]\nRandomness extraction is also used in some branches of computational complexity theory.\nRandom extraction is also used to convert data to a simple random sample, which is normally distributed, and independent, which is desired by statistics.", 
    "dbpedia_url": "http://dbpedia.org/resource/Randomness_extractor", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Randomness_extractor\n"
}