{
    "about": "In mathematics, a low-discrepancy sequence is a sequence with the property that for all values of N, its subsequence x1, ..., xN has a low discrepancy.", 
    "classification": "Numerical Analysis", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Low-discrepancy_sequence\n", 
    "full_text": "In mathematics, a low-discrepancy sequence is a sequence with the property that for all values of N, its subsequence x1, ..., xN has a low discrepancy.\nRoughly speaking, the discrepancy of a sequence is low if the proportion of points in the sequence falling into an arbitrary set B is close to proportional to the measure of B, as would happen on average (but not for particular samples) in the case of an equidistributed sequence. Specific definitions of discrepancy differ regarding the choice of B (hyperspheres, hypercubes, etc.) and how the discrepancy for every B is computed (usually normalized) and combined (usually by taking the worst value).\nLow-discrepancy sequences are also called quasi-random or sub-random sequences, due to their common use as a replacement of uniformly distributed random numbers. The \"quasi\" modifier is used to denote more clearly that the values of a low-discrepancy sequence are neither random nor pseudorandom, but such sequences share some properties of random variables and in certain applications such as the quasi-Monte Carlo method their lower discrepancy is an important advantage.\n\n\nSubrandom numbers have an advantage over pure random numbers in that they cover the domain of interest quickly and evenly. They have an advantage over purely deterministic methods in that deterministic methods only give high accuracy when the number of datapoints is pre-set whereas in using subrandom sequences the accuracy typically improves continually as more datapoints are added, with full reuse of the existing points. On the other hand, subrandom sets can have a significant lower discrepancy for a given number of points than subrandom sequences.\nTwo useful applications are in finding the characteristic function of a probability density function, and in finding the derivative function of a deterministic function with a small amount of noise. Subrandom numbers allow higher-order moments to be calculated to high accuracy very quickly.\nApplications that don't involve sorting would be in finding the mean, standard deviation, skewness and kurtosis of a statistical distribution, and in finding the integral and global maxima and minima of difficult deterministic functions. Subrandom numbers can also be used for providing starting points for deterministic algorithms that only work locally, such as Newton\u2013Raphson iteration.\nSubrandom numbers can also be combined with search algorithms. A binary tree Quicksort-style algorithm ought to work exceptionally well because subrandom numbers flatten the tree far better than random numbers, and the flatter the tree the faster the sorting. With a search algorithm, subrandom numbers can be used to find the mode, median, confidence intervals and cumulative distribution of a statistical distribution, and all local minima and all solutions of deterministic functions.\nAt least three methods of numerical integration can be phrased as follows. Given a set {x1, ..., xN} in the interval [0,1], approximate the integral of a function f as the average of the function evaluated at those points:\nIf the points are chosen as xi = i/N, this is the rectangle rule. If the points are chosen to be randomly (or pseudorandomly) distributed, this is the Monte Carlo method. If the points are chosen as elements of a low-discrepancy sequence, this is the quasi-Monte Carlo method. A remarkable result, the Koksma\u2013Hlawka inequality (stated below), shows that the error of such a method can be bounded by the product of two terms, one of which depends only on f, and the other one is the discrepancy of the set {x1, ..., xN}.\nIt is convenient to construct the set {x1, ..., xN} in such a way that if a set with N+1 elements is constructed, the previous N elements need not be recomputed. The rectangle rule uses points set which have low discrepancy, but in general the elements must be recomputed if N is increased. Elements need not be recomputed in the random Monte Carlo method if N is increased, but the point sets do not have minimal discrepancy. By using low-discrepancy sequences we aim for low discrepancy and no need for recomputations, but actually LDS can only be incrementally good on discrepancy if we allow no recomputation.\nThe discrepancy of a set P = {x1, ..., xN} is defined, using Niederreiter's notation, as\nwhere \u03bbs is the s-dimensional Lebesgue measure, A(B;P) is the number of points in P that fall into B, and J is the set of s-dimensional intervals or boxes of the form\nwhere \n\n\n\n0\n\u2264\n\na\n\ni\n\n\n<\n\nb\n\ni\n\n\n\u2264\n1\n\n\n{\\displaystyle 0\\leq a_{i}<b_{i}\\leq 1}\n\n.\nThe star-discrepancy D*N(P) is defined similarly, except that the supremum is taken over the set J* of rectangular boxes of the form\nwhere ui is in the half-open interval [0, 1).\nThe two are related by\nLet \u012as be the s-dimensional unit cube, \u012as = [0, 1] \u00d7 ... \u00d7 [0, 1]. Let f have bounded variation V(f) on \u012as in the sense of Hardy and Krause. Then for any x1, ..., xN in Is = [0, 1) \u00d7 ... \u00d7 [0, 1),\nThe Koksma\u2013Hlawka inequality is sharp in the following sense: For any point set {x1,...,xN} in Is and any \n\n\n\n\u03b5\n>\n0\n\n\n{\\displaystyle \\varepsilon >0}\n\n, there is a function f with bounded variation and V(f)\u00a0=\u00a01 such that\nTherefore, the quality of a numerical integration rule depends only on the discrepancy D*N(x1,...,xN).\nLet \n\n\n\nD\n=\n{\n1\n,\n2\n,\n\u2026\n,\nd\n}\n\n\n{\\displaystyle D=\\{1,2,\\ldots ,d\\}}\n\n. For \n\n\n\n\u2205\n\u2260\nu\n\u2286\nD\n\n\n{\\displaystyle \\emptyset \\neq u\\subseteq D}\n\n we write\nand denote by \n\n\n\n(\n\nx\n\nu\n\n\n,\n1\n)\n\n\n{\\displaystyle (x_{u},1)}\n\n the point obtained from x by replacing the coordinates not in u by \n\n\n\n1\n\n\n{\\displaystyle 1}\n\n. Then\nwhere \n\n\n\ndisc\n\u2061\n(\nz\n)\n=\n\n\n1\nN\n\n\n\n\u2211\n\ni\n=\n1\n\n\nN\n\n\n\n\u220f\n\nj\n=\n1\n\n\nd\n\n\n\n1\n\n[\n0\n,\n\nz\n\nj\n\n\n)\n\n\n(\n\nx\n\ni\n,\nj\n\n\n)\n\u2212\n\n\u220f\n\nj\n=\n1\n\n\nd\n\n\n\nz\n\ni\n\n\n\n\n{\\displaystyle \\operatorname {disc} (z)={\\frac {1}{N}}\\sum _{i=1}^{N}\\prod _{j=1}^{d}1_{[0,z_{j})}(x_{i,j})-\\prod _{j=1}^{d}z_{i}}\n\n is the discrepancy function.\nApplying the Cauchy\u2013Schwarz inequality for integrals and sums to the Hlawka\u2013Zaremba identity, we obtain an \n\n\n\n\nL\n\n2\n\n\n\n\n{\\displaystyle L^{2}}\n\n version of the Koksma\u2013Hlawka inequality:\nwhere\nand\nIt is computationally hard to find the exact value of the discrepancy of large point sets. The Erd\u0151s\u2013Tur\u00e1n\u2013Koksma inequality provides an upper bound.\nLet x1,...,xN be points in Is and H be an arbitrary positive integer. Then\nwhere\nConjecture 1. There is a constant cs depending only on the dimension s, such that\nfor any finite point set {x1,...,xN}.\nConjecture 2. There is a constant c's depending only on s, such that\nfor at least one N for any infinite sequence x1,x2,x3,....\nThese conjectures are equivalent. They have been proved for s \u2264 2 by W. M. Schmidt. In higher dimensions, the corresponding problem is still open. The best-known lower bounds are due to K. F. Roth.\nLet s\u00a0=\u00a01. Then\nfor any finite point set {x1,\u00a0...,\u00a0xN}.\nLet s\u00a0=\u00a02. W. M. Schmidt proved that for any finite point set {x1,\u00a0...,\u00a0xN},\nwhere\nFor arbitrary dimensions s\u00a0>\u00a01, K.F. Roth proved that\nfor any finite point set {x1,\u00a0...,\u00a0xN}. This bound is the best known for s\u00a0>\u00a03.\nBecause any distribution of random numbers can be mapped onto a uniform distribution, and subrandom numbers are mapped in the same way, this article only concerns generation of subrandom numbers on a multidimensional uniform distribution.\nThere are constructions of sequences known such that\nwhere C is a certain constant, depending on the sequence. After Conjecture 2, these sequences are believed to have the best possible order of convergence. Examples below are the van der Corput sequence, the Halton sequences, and the Sobol sequences.\nSequences of subrandom numbers can be generated from random numbers by imposing a negative correlation on those random numbers. One way to do this is to start with a set of random numbers \n\n\n\n\nr\n\ni\n\n\n\n\n{\\displaystyle r_{i}}\n\n on \n\n\n\n[\n0\n,\n0.5\n)\n\n\n{\\displaystyle [0,0.5)}\n\n and construct subrandom numbers \n\n\n\n\ns\n\ni\n\n\n\n\n{\\displaystyle s_{i}}\n\n which are uniform on \n\n\n\n[\n0\n,\n1\n)\n\n\n{\\displaystyle [0,1)}\n\n using:\n\n\n\n\n\ns\n\ni\n\n\n=\n\nr\n\ni\n\n\n\n\n{\\displaystyle s_{i}=r_{i}}\n\n for \n\n\n\ni\n\n\n{\\displaystyle i}\n\n odd and \n\n\n\n\ns\n\ni\n\n\n=\n0.5\n+\n\nr\n\ni\n\n\n\n\n{\\displaystyle s_{i}=0.5+r_{i}}\n\n for \n\n\n\ni\n\n\n{\\displaystyle i}\n\n even.\nA second way to do it with the starting random numbers is to construct a random walk with offset 0.5 as in:\nThat is, take the previous subrandom number, add 0.5 and the random number, and take the result modulo\u00a01.\nFor more than one dimension, Latin squares of the appropriate dimension can be used to provide offsets to ensure that the whole domain is covered evenly.\nFor any irrational \n\n\n\n\u03b1\n\n\n{\\displaystyle \\alpha }\n\n, the sequence\nhas discrepancy tending to 0. (Note the sequence can be defined recursively by \n\n\n\n\ns\n\nn\n+\n1\n\n\n=\n(\n\ns\n\nn\n\n\n+\n\u03b1\n)\n\nmod\n\n\n\n\n1\n\n\n{\\displaystyle s_{n+1}=(s_{n}+\\alpha ){\\bmod {\\,}}1}\n\n.) A good value of \n\n\n\n\u03b1\n\n\n{\\displaystyle \\alpha }\n\n gives lower discrepancy than a sequence of independent uniform random numbers.\nThe discrepancy can be bounded by the approximation exponent of \n\n\n\n\u03b1\n\n\n{\\displaystyle \\alpha }\n\n. If the approximation exponent is \n\n\n\n\u03bc\n\n\n{\\displaystyle \\mu }\n\n, then for any \n\n\n\n\u03b5\n>\n0\n\n\n{\\displaystyle \\varepsilon >0}\n\n, the following bound holds:[1]\nBy the Thue\u2013Siegel\u2013Roth theorem, the approximation exponent of any irrational algebraic number is 2, giving a bound of \n\n\n\n\nN\n\n\u2212\n1\n+\n\u03b5\n\n\n\n\n{\\displaystyle N^{-1+\\varepsilon }}\n\n above.\nThe value of \n\n\n\nc\n\n\n{\\displaystyle c}\n\n with lowest discrepancy is [2]\nAnother value that is nearly as good is:\nIn more than one dimension, separate subrandom numbers are needed for each dimension. In higher dimensions, one set of values that can be used is the square roots of primes from two up, all taken modulo 1:\nThe recurrence relation above is similar to the recurrence relation used by a Linear congruential generator, a poor-quality pseudorandom number generator:[3]\nFor the low discrepancy additive recurrence above, a and m are chosen to be 1. Note, however, that this will not generate independent random numbers, so should not be used for purposes requiring independence. The list of pseudorandom number generators lists methods for generating independent pseudorandom numbers.\nLet\nbe the b-ary representation of the positive integer n \u2265 1, i.e. 0 \u2264 dk(n) < b. Set\nThen there is a constant C depending only on b such that (gb(n))n \u2265 1satisfies\nwhere D*N is the star discrepancy.\nThe Halton sequence is a natural generalization of the van der Corput sequence to higher dimensions. Let s be an arbitrary dimension and b1, ..., bs be arbitrary coprime integers greater than 1. Define\nThen there is a constant C depending only on b1, ..., bs, such that sequence {x(n)}n\u22651 is a s-dimensional sequence with\nLet b1,...,bs-1 be coprime positive integers greater than 1. For given s and N, the s-dimensional Hammersley set of size N is defined by[4]\nfor n = 1, ..., N. Then\nwhere C is a constant depending only on b1, ..., bs\u22121.\nThe Antonov\u2013Saleev variant of the Sobol sequence generates numbers between zero and one directly as binary fractions of length \n\n\n\nw\n\n\n{\\displaystyle w}\n\n, from a set of \n\n\n\nw\n\n\n{\\displaystyle w}\n\n special binary fractions, \n\n\n\n\nV\n\ni\n\n\n,\ni\n=\n1\n,\n2\n,\n\u2026\n,\nw\n\n\n{\\displaystyle V_{i},i=1,2,\\dots ,w}\n\n called direction numbers. The bits of the Gray code of \n\n\n\ni\n\n\n{\\displaystyle i}\n\n, \n\n\n\nG\n(\ni\n)\n\n\n{\\displaystyle G(i)}\n\n, are used to select direction numbers. To get the Sobol sequence value \n\n\n\n\ns\n\ni\n\n\n\n\n{\\displaystyle s_{i}}\n\n take the exclusive or of the binary value of the Gray code of\n\n\n\ni\n\n\n{\\displaystyle i}\n\n with the appropriate direction number. The number of dimensions required affects the choice of \n\n\n\n\nV\n\ni\n\n\n\n\n{\\displaystyle V_{i}}\n\n.\nPoisson disk sampling is popular in video games to rapidly placing objects in a way that appears random-looking but guarantees that every two points are separated by at least the specified minimum distance.[5]\nThe points plotted below are the first 100, 1000, and 10000 elements in a sequence of the Sobol' type. For comparison, 10000 elements of a sequence of pseudorandom points are also shown. The low-discrepancy sequence was generated by TOMS algorithm 659.[6] An implementation of the algorithm in Fortran is available from Netlib.", 
    "name": "Low Discrepancy Sequence"
}