{
    "about": "The golden-section search is a technique for finding the extremum (minimum or maximum) of a strictly unimodal function by successively narrowing the range of values inside which the extremum is known to exist. The technique derives its name from the fact that the algorithm maintains the function values for triples of points whose distances form a golden ratio. The algorithm is the limit of Fibonacci search (also described below) for a large number of function evaluations. Fibonacci search and golden-section search were discovered by Kiefer (1953) (see also Avriel and Wilde (1966)).", 
    "name": "Golden Section Search", 
    "classification": "Optimization Algorithms And Methods", 
    "full_text": "The golden-section search is a technique for finding the extremum (minimum or maximum) of a strictly unimodal function by successively narrowing the range of values inside which the extremum is known to exist. The technique derives its name from the fact that the algorithm maintains the function values for triples of points whose distances form a golden ratio. The algorithm is the limit of Fibonacci search (also described below) for a large number of function evaluations. Fibonacci search and golden-section search were discovered by Kiefer (1953) (see also Avriel and Wilde (1966)).\n\n\nThe discussion here is posed in terms of searching for a minimum (searching for a maximum is similar) of a unimodal function. Unlike finding a zero, where two function evaluations with opposite sign are sufficient to bracket a root, when searching for a minimum, three values are necessary. The golden-section search is an efficient way to reduce progressively the interval locating the minimum. The key is to observe that regardless of how many points have been evaluated, the minimum lies within the interval defined by the two points adjacent to the point with the least value so far evaluated.\nThe diagram above illustrates a single step in the technique for finding a minimum. The functional values of \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n are on the vertical axis, and the horizontal axis is the x parameter. The value of \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n has already been evaluated at the three points: \n\n\n\n\nx\n\n1\n\n\n\n\n{\\displaystyle x_{1}}\n\n, \n\n\n\n\nx\n\n2\n\n\n\n\n{\\displaystyle x_{2}}\n\n, and \n\n\n\n\nx\n\n3\n\n\n\n\n{\\displaystyle x_{3}}\n\n. Since \n\n\n\n\nf\n\n2\n\n\n\n\n{\\displaystyle f_{2}}\n\n is smaller than either \n\n\n\n\nf\n\n1\n\n\n\n\n{\\displaystyle f_{1}}\n\n or \n\n\n\n\nf\n\n3\n\n\n\n\n{\\displaystyle f_{3}}\n\n, it is clear that a minimum lies inside the interval from \n\n\n\n\nx\n\n1\n\n\n\n\n{\\displaystyle x_{1}}\n\n to \n\n\n\n\nx\n\n3\n\n\n\n\n{\\displaystyle x_{3}}\n\n.\nThe next step in the minimization process is to \"probe\" the function by evaluating it at a new value of x, namely \n\n\n\n\nx\n\n4\n\n\n\n\n{\\displaystyle x_{4}}\n\n. It is most efficient to choose \n\n\n\n\nx\n\n4\n\n\n\n\n{\\displaystyle x_{4}}\n\n somewhere inside the largest interval, i.e. between \n\n\n\n\nx\n\n2\n\n\n\n\n{\\displaystyle x_{2}}\n\n and \n\n\n\n\nx\n\n3\n\n\n\n\n{\\displaystyle x_{3}}\n\n. From the diagram, it is clear that if the function yields \n\n\n\n\nf\n\n4\na\n\n\n\n\n{\\displaystyle f_{4a}}\n\n, then a minimum lies between \n\n\n\n\nx\n\n1\n\n\n\n\n{\\displaystyle x_{1}}\n\n and \n\n\n\n\nx\n\n4\n\n\n\n\n{\\displaystyle x_{4}}\n\n, and the new triplet of points will be \n\n\n\n\nx\n\n1\n\n\n\n\n{\\displaystyle x_{1}}\n\n, \n\n\n\n\nx\n\n2\n\n\n\n\n{\\displaystyle x_{2}}\n\n, and \n\n\n\n\nx\n\n4\n\n\n\n\n{\\displaystyle x_{4}}\n\n. However, if the function yields the value \n\n\n\n\nf\n\n4\nb\n\n\n\n\n{\\displaystyle f_{4b}}\n\n, then a minimum lies between \n\n\n\n\nx\n\n2\n\n\n\n\n{\\displaystyle x_{2}}\n\n and \n\n\n\n\nx\n\n3\n\n\n\n\n{\\displaystyle x_{3}}\n\n, and the new triplet of points will be \n\n\n\n\nx\n\n2\n\n\n\n\n{\\displaystyle x_{2}}\n\n, \n\n\n\n\nx\n\n4\n\n\n\n\n{\\displaystyle x_{4}}\n\n, and \n\n\n\n\nx\n\n3\n\n\n\n\n{\\displaystyle x_{3}}\n\n. Thus, in either case, we can construct a new narrower search interval that is guaranteed to contain the function's minimum.\nFrom the diagram above, it is seen that the new search interval will be either between \n\n\n\n\nx\n\n1\n\n\n\n\n{\\displaystyle x_{1}}\n\n and \n\n\n\n\nx\n\n4\n\n\n\n\n{\\displaystyle x_{4}}\n\n with a length of a\u00a0+\u00a0c, or between \n\n\n\n\nx\n\n2\n\n\n\n\n{\\displaystyle x_{2}}\n\n and \n\n\n\n\nx\n\n3\n\n\n\n\n{\\displaystyle x_{3}}\n\n with a length of b. The golden-section search requires that these intervals be equal. If they are not, a run of \"bad luck\" could lead to the wider interval being used many times, thus slowing down the rate of convergence. To ensure that b = a\u00a0+\u00a0c, the algorithm should choose \n\n\n\n\nx\n\n4\n\n\n=\n\nx\n\n1\n\n\n+\n(\n\nx\n\n3\n\n\n\u2212\n\nx\n\n2\n\n\n)\n\n\n{\\displaystyle x_{4}=x_{1}+(x_{3}-x_{2})}\n\n.\nHowever, there still remains the question of where \n\n\n\n\nx\n\n2\n\n\n\n\n{\\displaystyle x_{2}}\n\n should be placed in relation to \n\n\n\n\nx\n\n1\n\n\n\n\n{\\displaystyle x_{1}}\n\n and \n\n\n\n\nx\n\n3\n\n\n\n\n{\\displaystyle x_{3}}\n\n. The golden-section search chooses the spacing between these points in such a way that these points have the same proportion of spacing as the subsequent triple \n\n\n\n\nx\n\n1\n\n\n,\n\nx\n\n2\n\n\n,\n\nx\n\n4\n\n\n\n\n{\\displaystyle x_{1},x_{2},x_{4}}\n\n or \n\n\n\n\nx\n\n2\n\n\n,\n\nx\n\n4\n\n\n,\n\nx\n\n3\n\n\n\n\n{\\displaystyle x_{2},x_{4},x_{3}}\n\n. By maintaining the same proportion of spacing throughout the algorithm, we avoid a situation in which \n\n\n\n\nx\n\n2\n\n\n\n\n{\\displaystyle x_{2}}\n\n is very close to \n\n\n\n\nx\n\n1\n\n\n\n\n{\\displaystyle x_{1}}\n\n or \n\n\n\n\nx\n\n3\n\n\n\n\n{\\displaystyle x_{3}}\n\n and guarantee that the interval width shrinks by the same constant proportion in each step.\nMathematically, to ensure that the spacing after evaluating \n\n\n\nf\n(\n\nx\n\n4\n\n\n)\n\n\n{\\displaystyle f(x_{4})}\n\n is proportional to the spacing prior to that evaluation, if \n\n\n\nf\n(\n\nx\n\n4\n\n\n)\n\n\n{\\displaystyle f(x_{4})}\n\n is \n\n\n\n\nf\n\n4\na\n\n\n\n\n{\\displaystyle f_{4a}}\n\n and our new triplet of points is \n\n\n\n\nx\n\n1\n\n\n\n\n{\\displaystyle x_{1}}\n\n, \n\n\n\n\nx\n\n2\n\n\n\n\n{\\displaystyle x_{2}}\n\n, and \n\n\n\n\nx\n\n4\n\n\n\n\n{\\displaystyle x_{4}}\n\n, then we want\nHowever, if \n\n\n\nf\n(\n\nx\n\n4\n\n\n)\n\n\n{\\displaystyle f(x_{4})}\n\n is \n\n\n\n\nf\n\n4\nb\n\n\n\n\n{\\displaystyle f_{4b}}\n\n and our new triplet of points is \n\n\n\n\nx\n\n2\n\n\n\n\n{\\displaystyle x_{2}}\n\n, \n\n\n\n\nx\n\n4\n\n\n\n\n{\\displaystyle x_{4}}\n\n, and \n\n\n\n\nx\n\n3\n\n\n\n\n{\\displaystyle x_{3}}\n\n, then we want\nEliminating c from these two simultaneous equations yields\nor\nwhere \u03c6 is the golden ratio:\nThe appearance of the golden ratio in the proportional spacing of the evaluation points is how this search algorithm gets its name.\nBecause smooth functions are flat (their first derivative is close to zero) near a minimum, attention must be paid not to expect too great an accuracy in locating the minimum. The termination condition provided in the book Numerical Recipes in C is based on testing the gaps among \n\n\n\n\nx\n\n1\n\n\n\n\n{\\displaystyle x_{1}}\n\n, \n\n\n\n\nx\n\n2\n\n\n\n\n{\\displaystyle x_{2}}\n\n, \n\n\n\n\nx\n\n3\n\n\n\n\n{\\displaystyle x_{3}}\n\n and \n\n\n\n\nx\n\n4\n\n\n\n\n{\\displaystyle x_{4}}\n\n, terminating when within the relative accuracy bounds\nwhere \n\n\n\n\u03c4\n\n\n{\\displaystyle \\tau }\n\n is a tolerance parameter of the algorithm, and \n\n\n\n\n|\n\nx\n\n|\n\n\n\n{\\displaystyle |x|}\n\n is the absolute value of \n\n\n\nx\n\n\n{\\displaystyle x}\n\n. The check is based on the bracket size relative to its central value, because that relative error in \n\n\n\nx\n\n\n{\\displaystyle x}\n\n is approximately proportional to the squared absolute error in \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n in typical cases. For that same reason, the Numerical Recipes text recommends that \n\n\n\n\u03c4\n=\n\n\n\u03b5\n\n\n\n\n{\\displaystyle \\tau ={\\sqrt {\\varepsilon }}}\n\n, where \n\n\n\n\u03b5\n\n\n{\\displaystyle \\varepsilon }\n\n is the required absolute precision of \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n.\nTo realise the advantage of golden section search, the function \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n would be implemented with caching, so that in all invocations of goldenSectionSearch(..) above, except the first, \n\n\n\nf\n(\n\nx\n\n2\n\n\n)\n\n\n{\\displaystyle f(x_{2})}\n\n would have already been evaluated previously \u2014 the result of the calculation will be re-used, bypassing the (perhaps expensive) explicit evaluation of the function. Together with a slightly smaller number of recursions, this 50% saving in the number of calls to \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n is the main algorithmic advantage over Ternary search.\nA very similar algorithm can also be used to find the extremum (minimum or maximum) of a sequence of values that has a single local minimum or local maximum. In order to approximate the probe positions of golden section search while probing only integer sequence indices, the variant of the algorithm for this case typically maintains a bracketing of the solution in which the length of the bracketed interval is a Fibonacci number. For this reason, the sequence variant of golden section search is often called Fibonacci search.\nFibonacci search was first devised by Kiefer (1953) as a minimax search for the maximum (minimum) of a unimodal function in an interval.", 
    "dbpedia_url": "http://dbpedia.org/resource/Golden_section_search", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Golden_section_search\n"
}