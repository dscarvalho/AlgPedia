{
    "about": "The k-medoids algorithm is a clustering algorithm related to the k-means algorithm and the medoidshift algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) and both attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses datapoints as centers (medoids or exemplars) and works with an arbitrary metrics of distances between datapoints instead of \n\n\n\n\nl\n\n2\n\n\n\n\n{\\displaystyle l_{2}}\n\n. This method was proposed in 1987[1] for the work with \n\n\n\n\nl\n\n1\n\n\n\n\n{\\displaystyle l_{1}}\n\n norm and other distances.", 
    "classification": "Statistical Algorithms", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/K-medoids\n", 
    "full_text": "The k-medoids algorithm is a clustering algorithm related to the k-means algorithm and the medoidshift algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) and both attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses datapoints as centers (medoids or exemplars) and works with an arbitrary metrics of distances between datapoints instead of \n\n\n\n\nl\n\n2\n\n\n\n\n{\\displaystyle l_{2}}\n\n. This method was proposed in 1987[1] for the work with \n\n\n\n\nl\n\n1\n\n\n\n\n{\\displaystyle l_{1}}\n\n norm and other distances.\nk-medoid is a classical partitioning technique of clustering that clusters the data set of n objects into k clusters known a priori. A useful tool for determining k is the silhouette.\nIt is more robust to noise and outliers as compared to k-means because it minimizes a sum of pairwise dissimilarities instead of a sum of squared Euclidean distances.\nA medoid can be defined as the object of a cluster whose average dissimilarity to all the objects in the cluster is minimal. i.e. it is a most centrally located point in the cluster.\n\n\nThe most common realisation of k-medoid clustering is the Partitioning Around Medoids (PAM) algorithm. PAM uses a greedy search which may not find the optimum solution, but it is faster than exhaustive search[citation needed]. It works as follows:\nAlgorithms other than PAM have also been suggested in the literature, including the following Voronoi iteration method: [2] [3]\nCluster the following data set of ten objects into two clusters i.e. k = 2.\nConsider a data set of ten objects as follows\u00a0:\nInitialize k centers.\nLet us assume x2 and x8 are selected as medoids, so the centers are c1 = (3,4) and c2 = (7,4)\nCalculate distances to each center so as to associate each data object to its nearest medoid. Cost is calculated using Manhattan distance (Minkowski distance metric with r = 1). Costs to the nearest medoid are shown bold in the table.\nThen the clusters become:\nSince the points (2,6) (3,8) and (4,7) are closer to c1 hence they form one cluster whilst remaining points form another cluster.\nSo the total cost involved is 20.\nWhere cost between any two points is found using formula\nwhere x is any data object, c is the medoid, and d is the dimension of the object which in this case is 2.\nTotal cost is the summation of the cost of data object from its medoid in its cluster so here:\nSelect one of the nonmedoids O\u2032\nLet us assume O\u2032 = (7,3), i.e. x7.\nSo now the medoids are c1(3,4) and O\u2032(7,3)\nIf c1 and O\u2032 are new medoids, calculate the total cost involved\nBy using the formula in the step 1\n\n\n\n\n\n\n\n\n\ntotal cost\n\n\n\n\n=\n3\n+\n4\n+\n4\n+\n2\n+\n2\n+\n1\n+\n3\n+\n3\n\n\n\n\n\n\n=\n22\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}{\\mbox{total cost}}&=3+4+4+2+2+1+3+3\\\\&=22\\\\\\end{aligned}}}\n\n\nSo cost of swapping medoid from c2 to O\u2032 is\n\n\n\n\n\n\n\n\nS\n\n\n\n=\n\ncurrent total cost\n\n\u2212\n\npast total cost\n\n\n\n\n\n\n\n=\n22\n\u2212\n20\n\n\n\n\n\n\n=\n2\n>\n0.\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}S&={\\mbox{current total cost}}-{\\mbox{past total cost}}\\\\&=22-20\\\\&=2>0.\\end{aligned}}}\n\n\nSo moving to O\u2032 would be a bad idea, so the previous choice was good. So we try other nonmedoids and found that our first choice was the best. So the configuration does not change and algorithm terminates here (i.e. there is no change in the medoids).\nIt may happen some data points may shift from one cluster to another cluster depending upon their closeness to medoid.\nIn some standard situations, k-medoids demonstrate better performance than k-means. An example is presented in Fig. 2. The most time-consuming part of the k-medoids algorithm is the calculation of the distances between objects. If a quadratic preprocessing and storage is applicable, the distances matrix can be precomputed to achieve consequent speed-up. See for example,[3] where the authors also introduce a heuristic to choose the initial k medoids.", 
    "name": "K Medoids"
}