{
    "about": "The following tables list the running time of various algorithms for common mathematical operations.", 
    "name": "Computational Complexity Of Mathematical Operations", 
    "classification": "Computer Arithmetic Algorithms", 
    "full_text": "The following tables list the running time of various algorithms for common mathematical operations.\nHere, complexity refers to the time complexity of performing computations on a multitape Turing machine.[1] See big O notation for an explanation of the notation used.\nNote: Due to the variety of multiplication algorithms, M(n) below stands in for the complexity of the chosen multiplication algorithm.\n\n\nMany of the methods in this section are given in Borwein & Borwein.[5]\nThe elementary functions are constructed by composing arithmetic operations, the exponential function (exp), the natural logarithm (log), trigonometric functions (sin, cos), and their inverses. The complexity of an elementary function is equivalent to that of its inverse, since all elementary functions are analytic and hence invertible by means of Newton's method. In particular, if either exp or log in the complex domain can be computed with some complexity, then that complexity is attainable for all other elementary functions.\nBelow, the size n refers to the number of digits of precision at which the function is to be evaluated.\nIt is not known whether O(M(n) log n) is the optimal complexity for elementary functions. The best known lower bound is the trivial bound \u03a9(M(n)).\nThis table gives the complexity of computing approximations to the given constants to n correct digits.\nAlgorithms for number theoretical calculations are studied in computational number theory.\nThe following complexity figures assume that arithmetic with individual elements has complexity O(1), as is the case with fixed-precision floating-point arithmetic or operations on a finite field.\none m\u00d7p matrix\nIn 2005, Henry Cohn, Robert Kleinberg, Bal\u00e1zs Szegedy, and Chris Umans showed that either of two different conjectures would imply that the exponent of matrix multiplication is 2.[20]\n^* Because of the possibility of blockwise inverting a matrix, where an inversion of an n\u00d7n matrix requires inversion of two half-sized matrices and six multiplications between two half-sized matrices, and since matrix multiplication has a lower bound of \u03a9(n2 log n) operations,[21] it can be shown that a divide and conquer algorithm that uses blockwise inversion to invert a matrix runs with the same time complexity as the matrix multiplication algorithm that is used internally.[22]", 
    "dbpedia_url": "http://dbpedia.org/resource/Computational_complexity_of_mathematical_operations", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations\n"
}