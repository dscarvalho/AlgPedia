{
    "about": "In numerical optimization, the nonlinear conjugate gradient method generalizes the conjugate gradient method to nonlinear optimization. For a quadratic function \n\n\n\n\nf\n(\nx\n)\n\n\n\n{\\displaystyle \\displaystyle f(x)}\n\n:", 
    "name": "Nonlinear Conjugate Gradient Method", 
    "classification": "Optimization Algorithms And Methods", 
    "full_text": "In numerical optimization, the nonlinear conjugate gradient method generalizes the conjugate gradient method to nonlinear optimization. For a quadratic function \n\n\n\n\nf\n(\nx\n)\n\n\n\n{\\displaystyle \\displaystyle f(x)}\n\n:\nThe minimum of \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is obtained when the gradient is 0:\nWhereas linear conjugate gradient seeks a solution to the linear equation \n\n\n\n\n\nA\n\nT\n\n\nA\nx\n=\n\nA\n\nT\n\n\nb\n\n\n\n{\\displaystyle \\displaystyle A^{T}Ax=A^{T}b}\n\n, the nonlinear conjugate gradient method is generally used to find the local minimum of a nonlinear function using its gradient \n\n\n\n\n\u2207\n\nx\n\n\nf\n\n\n{\\displaystyle \\nabla _{x}f}\n\n alone. It works when the function is approximately quadratic near the minimum, which is the case when the function is twice differentiable at the minimum and the second derivative is non-singular there.\nGiven a function \n\n\n\n\nf\n(\nx\n)\n\n\n\n{\\displaystyle \\displaystyle f(x)}\n\n of \n\n\n\nN\n\n\n{\\displaystyle N}\n\n variables to minimize, its gradient \n\n\n\n\n\u2207\n\nx\n\n\nf\n\n\n{\\displaystyle \\nabla _{x}f}\n\n indicates the direction of maximum increase. One simply starts in the opposite (steepest descent) direction:\nwith an adjustable step length \n\n\n\n\n\u03b1\n\n\n\n{\\displaystyle \\displaystyle \\alpha }\n\n and performs a line search in this direction until it reaches the minimum of \n\n\n\n\nf\n\n\n\n{\\displaystyle \\displaystyle f}\n\n:\nAfter this first iteration in the steepest direction \n\n\n\n\n\u0394\n\nx\n\n0\n\n\n\n\n\n{\\displaystyle \\displaystyle \\Delta x_{0}}\n\n, the following steps constitute one iteration of moving along a subsequent conjugate direction \n\n\n\n\n\ns\n\nn\n\n\n\n\n\n{\\displaystyle \\displaystyle s_{n}}\n\n, where \n\n\n\n\n\ns\n\n0\n\n\n=\n\u0394\n\nx\n\n0\n\n\n\n\n\n{\\displaystyle \\displaystyle s_{0}=\\Delta x_{0}}\n\n:\nWith a pure quadratic function the minimum is reached within N iterations (excepting roundoff error), but a non-quadratic function will make slower progress. Subsequent search directions lose conjugacy requiring the search direction to be reset to the steepest descent direction at least every N iterations, or sooner if progress stops. However, resetting every iteration turns the method into steepest descent. The algorithm stops when it finds the minimum, determined when no progress is made after a direction reset (i.e. in the steepest descent direction), or when some tolerance criterion is reached.\nWithin a linear approximation, the parameters \n\n\n\n\n\u03b1\n\n\n\n{\\displaystyle \\displaystyle \\alpha }\n\n and \n\n\n\n\n\u03b2\n\n\n\n{\\displaystyle \\displaystyle \\beta }\n\n are the same as in the linear conjugate gradient method but have been obtained with line searches. The conjugate gradient method can follow narrow (ill-conditioned) valleys, where the steepest descent method slows down and follows a criss-cross pattern.\nFour of the best known formulas for \n\n\n\n\n\n\u03b2\n\nn\n\n\n\n\n\n{\\displaystyle \\displaystyle \\beta _{n}}\n\n are named after their developers:\nThese formulas are equivalent for a quadratic function, but for nonlinear optimization the preferred formula is a matter of heuristics or taste. A popular choice is \n\n\n\n\n\u03b2\n=\nmax\n{\n0\n,\n\n\u03b2\n\nP\nR\n\n\n}\n\n\n\n{\\displaystyle \\displaystyle \\beta =\\max\\{0,\\beta ^{PR}\\}}\n\n, which provides a direction reset automatically.[5]\nNewton-based methods \u2013 Newton-Raphson Algorithm, Quasi-Newton methods (e.g., BFGS method) \u2013 tend to converge in fewer iterations, although each iteration typically requires more computation than a conjugate gradient iteration, as Newton-like methods require computing the Hessian (matrix of second derivatives) in addition to the gradient. Quasi-Newton methods also require more memory to operate (see also the limited-memory L-BFGS method).", 
    "dbpedia_url": "http://dbpedia.org/resource/Nonlinear_conjugate_gradient_method", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method\n"
}