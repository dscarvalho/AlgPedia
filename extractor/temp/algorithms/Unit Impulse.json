{
    "about": "In science and mathematics, the Dirac delta function, or \u03b4 function, is a generalized function, or distribution, on the real number line that is zero everywhere except at zero, with an integral of one over the entire real line.[1][2][3] The delta function is sometimes thought of as a hypothetical function whose graph is an infinitely high, infinitely thin spike at the origin, with total area one under the spike, and physically represents the density of an idealized point mass or point charge.[4] It was introduced by theoretical physicist Paul Dirac.", 
    "name": "Unit Impulse", 
    "classification": "Digital Signal Processing", 
    "full_text": "In science and mathematics, the Dirac delta function, or \u03b4 function, is a generalized function, or distribution, on the real number line that is zero everywhere except at zero, with an integral of one over the entire real line.[1][2][3] The delta function is sometimes thought of as a hypothetical function whose graph is an infinitely high, infinitely thin spike at the origin, with total area one under the spike, and physically represents the density of an idealized point mass or point charge.[4] It was introduced by theoretical physicist Paul Dirac.\nFrom a purely mathematical viewpoint, the Dirac delta is not strictly a function, because any extended-real function that is equal to zero everywhere but a single point must have total integral zero.[5][6] The delta function only makes sense as a mathematical object when it appears inside an integral. From this perspective the Dirac delta can usually be manipulated as though it were a function. The formal rules obeyed by this \"function\" are part of the operational calculus, a standard tool kit of physics and engineering. The operational calculus, and in particular the delta function, was viewed with suspicion by mathematicians of the early 20th century, until a satisfactory rigorous theory was introduced by Laurent Schwartz in the 1950s.[7] Formally, the delta function must be defined as the distribution that corresponds to a probability measure supported at the origin. In many applications, the Dirac delta is regarded as a kind of limit (a weak limit) of a sequence of functions having a tall spike at the origin. The approximating functions of the sequence are thus \"approximate\" or \"nascent\" delta functions.\nIn the context of signal processing the delta function is often referred to as the unit impulse symbol (or function).[8] Its discrete analog is the Kronecker delta function, which is usually defined on a discrete domain and takes values 0 and 1.\n\n\nThe graph of the delta function is usually thought of as following the whole x-axis and the positive y-axis. Despite its name, the delta function is not truly a function, at least not a usual one with range in real numbers. For example, the objects f(x) = \u03b4(x) and g(x) = 0 are equal everywhere except at x = 0 yet have integrals that are different. According to Lebesgue integration theory, if f and g are functions such that f = g almost everywhere, then f is integrable if and only if g is integrable and the integrals of f and g are identical. Rigorous treatment of the Dirac delta requires measure theory or the theory of distributions.\nThe Dirac delta is used to model a tall narrow spike function (an impulse), and other similar abstractions such as a point charge, point mass or electron point. For example, to calculate the dynamics of a baseball being hit by a bat, one can approximate the force of the bat hitting the baseball by a delta function. In doing so, one not only simplifies the equations, but one also is able to calculate the motion of the baseball by only considering the total impulse of the bat against the ball rather than requiring knowledge of the details of how the bat transferred energy to the ball.\nIn applied mathematics, the delta function is often manipulated as a kind of limit (a weak limit) of a sequence of functions, each member of which has a tall spike at the origin: for example, a sequence of Gaussian distributions centered at the origin with variance tending to zero.\nJoseph Fourier presented what is now called the Fourier integral theorem in his treatise Th\u00e9orie analytique de la chaleur in the form:[9]\nwhich is tantamount to the introduction of the \u03b4-function in the form:[10]\nLater, Augustin Cauchy expressed the theorem using exponentials:[11][12]\nCauchy pointed out that in some circumstances the order of integration in this result was significant.[13][14]\nAs justified using the theory of distributions, the Cauchy equation can be rearranged to resemble Fourier's original formulation and expose the \u03b4-function as:\nwhere the \u03b4-function is expressed as:\nA rigorous interpretation of the exponential form and the various limitations upon the function f necessary for its application extended over several centuries. The problems with a classical interpretation are explained as follows:[15]\nFurther developments included generalization of the Fourier integral, \"beginning with Plancherel's pathbreaking L2-theory (1910), continuing with Wiener's and Bochner's works (around 1930) and culminating with the amalgamation into L. Schwartz's theory of distributions (1945)\u00a0...\",[16] and leading to the formal development of the Dirac delta function.\nAn infinitesimal formula for an infinitely tall, unit impulse delta function (infinitesimal version of Cauchy distribution) explicitly appears in an 1827 text of Augustin Louis Cauchy.[17] Sim\u00e9on Denis Poisson considered the issue in connection with the study of wave propagation as did Gustav Kirchhoff somewhat later. Kirchhoff and Hermann von Helmholtz also introduced the unit impulse as a limit of Gaussians, which also corresponded to Lord Kelvin's notion of a point heat source. At the end of the 19th century, Oliver Heaviside used formal Fourier series to manipulate the unit impulse.[18] The Dirac delta function as such was introduced as a \"convenient notation\" by Paul Dirac in his influential 1930 book The Principles of Quantum Mechanics.[19] He called it the \"delta function\" since he used it as a continuous analogue of the discrete Kronecker delta.\nThe Dirac delta can be loosely thought of as a function on the real line which is zero everywhere except at the origin, where it is infinite,\nand which is also constrained to satisfy the identity\nThis is merely a heuristic characterization. The Dirac delta is not a function in the traditional sense as no function defined on the real numbers has these properties.[19] The Dirac delta function can be rigorously defined either as a distribution or as a measure.\nOne way to rigorously capture the notion of the Dirac delta function is to define a measure, which accepts as an argument a subset A of the real line R, and returns \u03b4(A) = 1 if 0 \u2208 A, and \u03b4(A) = 0 otherwise.[21] If the delta function is conceptualized as modeling an idealized point mass at 0, then \u03b4(A) represents the mass contained in the set A. One may then define the integral against \u03b4 as the integral of a function against this mass distribution. Formally, the Lebesgue integral provides the necessary analytic device. The Lebesgue integral with respect to the measure \u03b4 satisfies\nfor all continuous compactly supported functions f. The measure \u03b4 is not absolutely continuous with respect to the Lebesgue measure \u2014 in fact, it is a singular measure. Consequently, the delta measure has no Radon\u2013Nikodym derivative \u2014 no true function for which the property\nholds.[22] As a result, the latter notation is a convenient abuse of notation, and not a standard (Riemann or Lebesgue) integral.\nAs a probability measure on R, the delta measure is characterized by its cumulative distribution function, which is the unit step function[23]\nThis means that H(x) is the integral of the cumulative indicator function 1(\u2212\u221e, x] with respect to the measure \u03b4; to wit,\nThus in particular the integral of the delta function against a continuous function can be properly understood as a Stieltjes integral:[24]\nAll higher moments of \u03b4 are zero. In particular, characteristic function and moment generating function are both equal to one.\nIn the theory of distributions a generalized function is thought of not as a function itself, but only in relation to how it affects other functions when it is \"integrated\" against them. In keeping with this philosophy, to define the delta function properly, it is enough to say what the \"integral\" of the delta function against a sufficiently \"good\" test function is. If the delta function is already understood as a measure, then the Lebesgue integral of a test function against that measure supplies the necessary integral.\nA typical space of test functions consists of all smooth functions on R with compact support. As a distribution, the Dirac delta is a linear functional on the space of test functions and is defined by[25]\n\n\n\n\n\u03b4\n[\n\u03c6\n]\n=\n\u03c6\n(\n0\n)\n\n\n\n{\\displaystyle \\delta [\\varphi ]=\\varphi (0)\\,}\n\n\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n(1)\nfor every test function \u03c6.\nFor \u03b4 to be properly a distribution, it must be continuous in a suitable topology on the space of test functions. In general, for a linear functional S on the space of test functions to define a distribution, it is necessary and sufficient that, for every positive integer N there is an integer MN and a constant CN such that for every test function \u03c6, one has the inequality[26]\nWith the \u03b4 distribution, one has such an inequality (with CN = 1) with MN = 0 for all N. Thus \u03b4 is a distribution of order zero. It is, furthermore, a distribution with compact support (the support being {0}).\nThe delta distribution can also be defined in a number of equivalent ways. For instance, it is the distributional derivative of the Heaviside step function. This means that, for every test function \u03c6, one has\nIntuitively, if integration by parts were permitted, then the latter integral should simplify to\nand indeed, a form of integration by parts is permitted for the Stieltjes integral, and in that case one does have\nIn the context of measure theory, the Dirac measure gives rise to a distribution by integration. Conversely, equation (1) defines a Daniell integral on the space of all compactly supported continuous functions \u03c6 which, by the Riesz representation theorem, can be represented as the Lebesgue integral of \u03c6 with respect to some Radon measure.\nGenerally, when the term \"Dirac delta function\" is used, it is in the sense of distributions rather than measures, the Dirac measure being among several terms for the corresponding notion in measure theory. Some sources may also use the term Dirac delta distribution.\nThe delta function can be defined in n-dimensional Euclidean space Rn as the measure such that\nfor every compactly supported continuous function f. As a measure, the n-dimensional delta function is the product measure of the 1-dimensional delta functions in each variable separately. Thus, formally, with x = (x1, x2, ..., xn), one has[8]\n\n\n\n\n\u03b4\n(\n\nx\n\n)\n=\n\u03b4\n(\n\nx\n\n1\n\n\n)\n\u03b4\n(\n\nx\n\n2\n\n\n)\n\u22ef\n\u03b4\n(\n\nx\n\nn\n\n\n)\n.\n\n\n{\\displaystyle \\delta (\\mathbf {x} )=\\delta (x_{1})\\delta (x_{2})\\cdots \\delta (x_{n}).}\n\n\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n(2)\nThe delta function can also be defined in the sense of distributions exactly as above in the one-dimensional case.[27] However, despite widespread use in engineering contexts, (2) should be manipulated with care, since the product of distributions can only be defined under quite narrow circumstances.[28]\nThe notion of a Dirac measure makes sense on any set.[21] Thus if X is a set, x0 \u2208 X is a marked point, and \u03a3 is any sigma algebra of subsets of X, then the measure defined on sets A \u2208 \u03a3 by\nis the delta measure or unit mass concentrated at x0.\nAnother common generalization of the delta function is to a differentiable manifold where most of its properties as a distribution can also be exploited because of the differentiable structure. The delta function on a manifold M centered at the point x0 \u2208 M is defined as the following distribution:\n\n\n\n\n\n\u03b4\n\n\nx\n\n0\n\n\n\n\n[\n\u03c6\n]\n=\n\u03c6\n(\n\nx\n\n0\n\n\n)\n\n\n{\\displaystyle \\delta _{x_{0}}[\\varphi ]=\\varphi (x_{0})}\n\n\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n(3)\nfor all compactly supported smooth real-valued functions \u03c6 on M.[29] A common special case of this construction is when M is an open set in the Euclidean space Rn.\nOn a locally compact Hausdorff space X, the Dirac delta measure concentrated at a point x is the Radon measure associated with the Daniell integral (3) on compactly supported continuous functions \u03c6. At this level of generality, calculus as such is no longer possible, however a variety of techniques from abstract analysis are available. For instance, the mapping \n\n\n\n\nx\n\n0\n\n\n\u21a6\n\n\u03b4\n\n\nx\n\n0\n\n\n\n\n\n\n{\\displaystyle x_{0}\\mapsto \\delta _{x_{0}}}\n\n is a continuous embedding of X into the space of finite Radon measures on X, equipped with its vague topology. Moreover, the convex hull of the image of X under this embedding is dense in the space of probability measures on X.[30]\nThe delta function satisfies the following scaling property for a non-zero scalar \u03b1:[31]\nand so\n\n\n\n\n\u03b4\n(\n\u03b1\nx\n)\n=\n\n\n\n\u03b4\n(\nx\n)\n\n\n\n|\n\n\u03b1\n\n|\n\n\n\n\n.\n\n\n{\\displaystyle \\delta (\\alpha x)={\\frac {\\delta (x)}{|\\alpha |}}.}\n\n\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n(4)\nIn particular, the delta function is an even distribution, in the sense that\nwhich is homogeneous of degree \u22121.\nThe distributional product of \u03b4 with x is equal to zero:\nConversely, if xf(x) = xg(x), where f and g are distributions, then\nfor some constant c.[32]\nThe integral of the time-delayed Dirac delta is given by:\nThis is sometimes referred to as the sifting property[33] or the sampling property. The delta function is said to \"sift out\" the value at t = T.\nIt follows that the effect of convolving a function f(t) with the time-delayed Dirac delta is to time-delay f(t) by the same amount:\nThis holds under the precise condition that f be a tempered distribution (see the discussion of the Fourier transform below). As a special case, for instance, we have the identity (understood in the distribution sense)\nMore generally, the delta distribution may be composed with a smooth function g(x) in such a way that the familiar change of variables formula holds, that\nprovided that g is a continuously differentiable function with g\u2032 nowhere zero.[34] That is, there is a unique way to assign meaning to the distribution \n\n\n\n\u03b4\n\u2218\ng\n\n\n{\\displaystyle \\delta \\circ g}\n\n so that this identity holds for all compactly supported test functions f. Therefore, the domain must be broken up to exclude the g\u2032 = 0 point. This distribution satisfies \u03b4(g(x)) = 0 if g is nowhere zero, and otherwise if g has a real root at x0, then\nIt is natural therefore to define the composition \u03b4(g(x)) for continuously differentiable functions g by\nwhere the sum extends over all roots of g(x), which are assumed to be simple.[34] Thus, for example\nIn the integral form the generalized scaling property may be written as\nThe delta distribution in an n-dimensional space satisfies the following scaling property instead:\nso that \u03b4 is a homogeneous distribution of degree \u2212n. Under any reflection or rotation \u03c1, the delta function is invariant:\nAs in the one-variable case, it is possible to define the composition of \u03b4 with a bi-Lipschitz function[35] g: Rn \u2192 Rn uniquely so that the identity\nfor all compactly supported functions f.\nUsing the coarea formula from geometric measure theory, one can also define the composition of the delta function with a submersion from one Euclidean space to another one of different dimension; the result is a type of current. In the special case of a continuously differentiable function g: Rn \u2192 R such that the gradient of g is nowhere zero, the following identity holds[36]\nwhere the integral on the right is over g\u22121(0), the (n \u2212 1)-dimensional surface defined by g(x) = 0 with respect to the Minkowski content measure. This is known as a simple layer integral.\nMore generally, if S is a smooth hypersurface of Rn, then we can associate to S the distribution that integrates any compactly supported smooth function g over S:\nwhere \u03c3 is the hypersurface measure associated to S. This generalization is associated with the potential theory of simple layer potentials on S. If D is a domain in Rn with smooth boundary S, then \u03b4S is equal to the normal derivative of the indicator function of D in the distribution sense:\nwhere n is the outward normal.[37][38] For a proof, see e.g. the article on the surface delta function.\nThe delta function is a tempered distribution, and therefore it has a well-defined Fourier transform. Formally, one finds[39]\nProperly speaking, the Fourier transform of a distribution is defined by imposing self-adjointness of the Fourier transform under the duality pairing \n\n\n\n\u27e8\n\u22c5\n,\n\u22c5\n\u27e9\n\n\n{\\displaystyle \\langle \\cdot ,\\cdot \\rangle }\n\n of tempered distributions with Schwartz functions. Thus \n\n\n\n\n\n\n\u03b4\n^\n\n\n\n\n\n{\\displaystyle {\\hat {\\delta }}}\n\n is defined as the unique tempered distribution satisfying\nfor all Schwartz functions \u03c6. And indeed it follows from this that \n\n\n\n\n\n\n\u03b4\n^\n\n\n\n=\n1.\n\n\n{\\displaystyle {\\hat {\\delta }}=1.}\n\n\nAs a result of this identity, the convolution of the delta function with any other tempered distribution S is simply S:\nThat is to say that \u03b4 is an identity element for the convolution on tempered distributions, and in fact the space of compactly supported distributions under convolution is an associative algebra with identity the delta function. This property is fundamental in signal processing, as convolution with a tempered distribution is a linear time-invariant system, and applying the linear time-invariant system measures its impulse response. The impulse response can be computed to any desired degree of accuracy by choosing a suitable approximation for \u03b4, and once it is known, it characterizes the system completely. See LTI system theory:Impulse response and convolution.\nThe inverse Fourier transform of the tempered distribution f(\u03be) = 1 is the delta function. Formally, this is expressed\nand more rigorously, it follows since\nfor all Schwartz functions f.\nIn these terms, the delta function provides a suggestive statement of the orthogonality property of the Fourier kernel on R. Formally, one has\nThis is, of course, shorthand for the assertion that the Fourier transform of the tempered distribution\nis\nwhich again follows by imposing self-adjointness of the Fourier transform.\nBy analytic continuation of the Fourier transform, the Laplace transform of the delta function is found to be[40]\nThe distributional derivative of the Dirac delta distribution is the distribution \u03b4\u2032 defined on compactly supported smooth test functions \u03c6 by[41]\nThe first equality here is a kind of integration by parts, for if \u03b4 were a true function then\nThe k-th derivative of \u03b4 is defined similarly as the distribution given on test functions by\nIn particular, \u03b4 is an infinitely differentiable distribution.\nThe first derivative of the delta function is the distributional limit of the difference quotients:[42]\nMore properly, one has\nwhere \u03c4h is the translation operator, defined on functions by \u03c4h\u03c6(x) = \u03c6(x + h), and on a distribution S by\nIn the theory of electromagnetism, the first derivative of the delta function represents a point magnetic dipole situated at the origin. Accordingly, it is referred to as a dipole or the doublet function.[43]\nThe derivative of the delta function satisfies a number of basic properties, including:\nFurthermore, the convolution of \u03b4\u2032 with a compactly supported smooth function f is\nwhich follows from the properties of the distributional derivative of a convolution.\nMore generally, on an open set U in the n-dimensional Euclidean space Rn, the Dirac delta distribution centered at a point a \u2208 U is defined by[45]\nfor all \u03c6 \u2208 S(U), the space of all smooth compactly supported functions on U. If \u03b1 = (\u03b11, ..., \u03b1n) is any multi-index and \u2202\u03b1 denotes the associated mixed partial derivative operator, then the \u03b1th derivative \u2202\u03b1\u03b4a of \u03b4a is given by[45]\nThat is, the \u03b1th derivative of \u03b4a is the distribution whose value on any test function \u03c6 is the \u03b1th derivative of \u03c6 at a (with the appropriate positive or negative sign).\nThe first partial derivatives of the delta function are thought of as double layers along the coordinate planes. More generally, the normal derivative of a simple layer supported on a surface is a double layer supported on that surface, and represents a laminar magnetic monopole. Higher derivatives of the delta function are known in physics as multipoles.\nHigher derivatives enter into mathematics naturally as the building blocks for the complete structure of distributions with point support. If S is any distribution on U supported on the set {a} consisting of a single point, then there is an integer m and coefficients c\u03b1 such that[46]\nThe delta function can be viewed as the limit of a sequence of functions\nwhere \u03b7\u03b5(x) is sometimes called a nascent delta function. This limit is meant in a weak sense: either that\n\n\n\n\n\nlim\n\n\u03b5\n\u2192\n\n0\n\n+\n\n\n\n\n\n\u222b\n\n\u2212\n\u221e\n\n\n\u221e\n\n\n\n\u03b7\n\n\u03b5\n\n\n(\nx\n)\nf\n(\nx\n)\n\nd\nx\n=\nf\n(\n0\n)\n\u00a0\n\n\n{\\displaystyle \\lim _{\\varepsilon \\to 0^{+}}\\int _{-\\infty }^{\\infty }\\eta _{\\varepsilon }(x)f(x)\\,dx=f(0)\\ }\n\n\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n(5)\nfor all continuous functions f having compact support, or that this limit holds for all smooth functions f with compact support. The difference between these two slightly different modes of weak convergence is often subtle: the former is convergence in the vague topology of measures, and the latter is convergence in the sense of distributions.\nTypically a nascent delta function \u03b7\u03b5 can be constructed in the following manner. Let \u03b7 be an absolutely integrable function on R of total integral 1, and define\nIn n dimensions, one uses instead the scaling\nThen a simple change of variables shows that \u03b7\u03b5 also has integral 1.[47] One shows easily that (5) holds for all continuous compactly supported functions f, and so \u03b7\u03b5 converges weakly to \u03b4 in the sense of measures.\nThe \u03b7\u03b5 constructed in this way are known as an approximation to the identity.[48] This terminology is because the space L1(R) of absolutely integrable functions is closed under the operation of convolution of functions: f \u2217 g \u2208 L1(R) whenever f and g are in L1(R). However, there is no identity in L1(R) for the convolution product: no element h such that f \u2217 h = f for all f. Nevertheless, the sequence \u03b7\u03b5 does approximate such an identity in the sense that\nThis limit holds in the sense of mean convergence (convergence in L1). Further conditions on the \u03b7\u03b5, for instance that it be a mollifier associated to a compactly supported function,[49] are needed to ensure pointwise convergence almost everywhere.\nIf the initial \u03b7 = \u03b71 is itself smooth and compactly supported then the sequence is called a mollifier. The standard mollifier is obtained by choosing \u03b7 to be a suitably normalized bump function, for instance\nIn some situations such as numerical analysis, a piecewise linear approximation to the identity is desirable. This can be obtained by taking \u03b71 to be a hat function. With this choice of \u03b71, one has\nwhich are all continuous and compactly supported, although not smooth and so not a mollifier.\nIn the context of probability theory, it is natural to impose the additional condition that the initial \u03b71 in an approximation to the identity should be positive, as such a function then represents a probability distribution. Convolution with a probability distribution is sometimes favorable because it does not result in overshoot or undershoot, as the output is a convex combination of the input values, and thus falls between the maximum and minimum of the input function. Taking \u03b71 to be any probability distribution at all, and letting \u03b7\u03b5(x) = \u03b71(x/\u03b5)/\u03b5 as above will give rise to an approximation to the identity. In general this converges more rapidly to a delta function if, in addition, \u03b7 has mean 0 and has small higher moments. For instance, if \u03b71 is the uniform distribution on [\u22121/2, 1/2], also known as the rectangular function, then:[50]\nAnother example is with the Wigner semicircle distribution\nThis is continuous and compactly supported, but not a mollifier because it is not smooth.\nNascent delta functions often arise as convolution semigroups. This amounts to the further constraint that the convolution of \u03b7\u03b5 with \u03b7\u03b4 must satisfy\nfor all \u03b5, \u03b4 > 0. Convolution semigroups in L1 that form a nascent delta function are always an approximation to the identity in the above sense, however the semigroup condition is quite a strong restriction.\nIn practice, semigroups approximating the delta function arise as fundamental solutions or Green's functions to physically motivated elliptic or parabolic partial differential equations. In the context of applied mathematics, semigroups arise as the output of a linear time-invariant system. Abstractly, if A is a linear operator acting on functions of x, then a convolution semigroup arises by solving the initial value problem\nin which the limit is as usual understood in the weak sense. Setting \u03b7\u03b5(x) = \u03b7(\u03b5, x) gives the associated nascent delta function.\nSome examples of physically important convolution semigroups arising from such a fundamental solution include the following.\nThe heat kernel, defined by\nrepresents the temperature in an infinite wire at time t > 0, if a unit of heat energy is stored at the origin of the wire at time t = 0. This semigroup evolves according to the one-dimensional heat equation:\nIn probability theory, \u03b7\u03b5(x) is a normal distribution of variance \u03b5 and mean 0. It represents the probability density at time t = \u03b5 of the position of a particle starting at the origin following a standard Brownian motion. In this context, the semigroup condition is then an expression of the Markov property of Brownian motion.\nIn higher-dimensional Euclidean space Rn, the heat kernel is\nand has the same physical interpretation, mutatis mutandis. It also represents a nascent delta function in the sense that \u03b7\u03b5 \u2192 \u03b4 in the distribution sense as \u03b5 \u2192 0.\nThe Poisson kernel\nis the fundamental solution of the Laplace equation in the upper half-plane.[51] It represents the electrostatic potential in a semi-infinite plate whose potential along the edge is held at fixed at the delta function. The Poisson kernel is also closely related to the Cauchy distribution. This semigroup evolves according to the equation\nwhere the operator is rigorously defined as the Fourier multiplier\nIn areas of physics such as wave propagation and wave mechanics, the equations involved are hyperbolic and so may have more singular solutions. As a result, the nascent delta functions that arise as fundamental solutions of the associated Cauchy problems are generally oscillatory integrals. An example, which comes from a solution of the Euler\u2013Tricomi equation of transonic gas dynamics,[52] is the rescaled Airy function\nAlthough using the Fourier transform, it is easy to see that this generates a semigroup in some sense, it is not absolutely integrable and so cannot define a semigroup in the above strong sense. Many nascent delta functions constructed as oscillatory integrals only converge in the sense of distributions (an example is the Dirichlet kernel below), rather than in the sense of measures.\nAnother example is the Cauchy problem for the wave equation in R1+1:[53]\nThe solution u represents the displacement from equilibrium of an infinite elastic string, with an initial disturbance at the origin.\nOther approximations to the identity of this kind include the sinc function (used widely in electronics and telecommunications)\nand the Bessel function\nOne approach to the study of a linear partial differential equation\nwhere L is a differential operator on Rn, is to seek first a fundamental solution, which is a solution of the equation\nWhen L is particularly simple, this problem can often be resolved using the Fourier transform directly (as in the case of the Poisson kernel and heat kernel already mentioned). For more complicated operators, it is sometimes easier first to consider an equation of the form\nwhere h is a plane wave function, meaning that it has the form\nfor some vector \u03be. Such an equation can be resolved (if the coefficients of L are analytic functions) by the Cauchy\u2013Kovalevskaya theorem or (if the coefficients of L are constant) by quadrature. So, if the delta function can be decomposed into plane waves, then one can in principle solve linear partial differential equations.\nSuch a decomposition of the delta function into plane waves was part of a general technique first introduced essentially by Johann Radon, and then developed in this form by Fritz John (1955).[54] Choose k so that n + k is an even integer, and for a real number s, put\nThen \u03b4 is obtained by applying a power of the Laplacian to the integral with respect to the unit sphere measure d\u03c9 of g(x \u00b7 \u03be) for \u03be in the unit sphere Sn\u22121:\nThe Laplacian here is interpreted as a weak derivative, so that this equation is taken to mean that, for any test function \u03c6,\nThe result follows from the formula for the Newtonian potential (the fundamental solution of Poisson's equation). This is essentially a form of the inversion formula for the Radon transform, because it recovers the value of \u03c6(x) from its integrals over hyperplanes. For instance, if n is odd and k = 1, then the integral on the right hand side is\nwhere R\u03c6(\u03be, p) is the Radon transform of \u03c6:\nAn alternative equivalent expression of the plane wave decomposition, from Gel'fand & Shilov (1966\u20131968, I, \u00a73.10), is\nfor n even, and\nfor n odd.\nIn the study of Fourier series, a major question consists of determining whether and in what sense the Fourier series associated with a periodic function converges to the function. The nth partial sum of the Fourier series of a function f of period 2\u03c0 is defined by convolution (on the interval [\u2212\u03c0,\u03c0]) with the Dirichlet kernel:\nThus,\nwhere\nA fundamental result of elementary Fourier series states that the Dirichlet kernel tends to the a multiple of the delta function as N \u2192 \u221e. This is interpreted in the distribution sense, that\nfor every compactly supported smooth function f. Thus, formally one has\non the interval [\u2212\u03c0,\u03c0].\nIn spite of this, the result does not hold for all compactly supported continuous functions: that is DN does not converge weakly in the sense of measures. The lack of convergence of the Fourier series has led to the introduction of a variety of summability methods in order to produce convergence. The method of Ces\u00e0ro summation leads to the Fej\u00e9r kernel[55]\nThe Fej\u00e9r kernels tend to the delta function in a stronger sense that[56]\nfor every compactly supported continuous function f. The implication is that the Fourier series of any continuous function is Ces\u00e0ro summable to the value of the function at every point.\nThe Dirac delta distribution is a densely defined unbounded linear functional on the Hilbert space L2 of square integrable functions. Indeed, smooth compactly support functions are dense in L2, and the action of the delta distribution on such functions is well-defined. In many applications, it is possible to identify subspaces of L2 and to give a stronger topology on which the delta function defines a bounded linear functional.\nThe Sobolev embedding theorem for Sobolev spaces on the real line R implies that any square-integrable function f such that\nis automatically continuous, and satisfies in particular\nThus \u03b4 is a bounded linear functional on the Sobolev space H1. Equivalently \u03b4 is an element of the continuous dual space H\u22121 of H1. More generally, in n dimensions, one has \u03b4 \u2208 H\u2212s(Rn) provided\u00a0s > n\u2009/\u20092.\nIn complex analysis, the delta function enters via Cauchy's integral formula which asserts that if D is a domain in the complex plane with smooth boundary, then\nfor all holomorphic functions f in D that are continuous on the closure of D. As a result, the delta function \u03b4z is represented on this class of holomorphic functions by the Cauchy integral:\nMore generally, let H2(\u2202D) be the Hardy space consisting of the closure in L2(\u2202D) of all holomorphic functions in D continuous up to the boundary of D. Then functions in H2(\u2202D) uniquely extend to holomorphic functions in D, and the Cauchy integral formula continues to hold. In particular for z \u2208 D, the delta function \u03b4z is a continuous linear functional on H2(\u2202D). This is a special case of the situation in several complex variables in which, for smooth domains D, the Szeg\u0151 kernel plays the role of the Cauchy integral.\nGiven a complete orthonormal basis set of functions {\u03c6n} in a separable Hilbert space, for example, the normalized eigenvectors of a compact self-adjoint operator, any vector f can be expressed as:\nThe coefficients {\u03b1n} are found as:\nwhich may be represented by the notation:\na form of the bra\u2013ket notation of Dirac.[57] Adopting this notation, the expansion of f takes the dyadic form:[58]\nLetting I denote the identity operator on the Hilbert space, the expression\nis called a resolution of the identity. When the Hilbert space is the space L2(D) of square-integrable functions on a domain D, the quantity:\nis an integral operator, and the expression for f can be rewritten as:\nThe right-hand side converges to f in the L2 sense. It need not hold in a pointwise sense, even when f is a continuous function. Nevertheless, it is common to abuse notation and write\nresulting in the representation of the delta function:[59]\nWith a suitable rigged Hilbert space (\u03a6, L2(D), \u03a6*) where \u03a6 \u2282 L2(D) contains all compactly supported smooth functions, this summation may converge in \u03a6*, depending on the properties of the basis \u03c6n. In most cases of practical interest, the orthonormal basis comes from an integral or differential operator, in which case the series converges in the distribution sense.[60]\nCauchy used an infinitesimal \u03b1 to write down a unit impulse, infinitely tall and narrow Dirac-type delta function \u03b4\u03b1 satisfying \n\n\n\n\u222b\nF\n(\nx\n)\n\n\u03b4\n\n\u03b1\n\n\n(\nx\n)\n=\nF\n(\n0\n)\n\n\n{\\displaystyle \\int F(x)\\delta _{\\alpha }(x)=F(0)}\n\n in a number of articles in 1827.[61] Cauchy defined an infinitesimal in Cours d'Analyse (1827) in terms of a sequence tending to zero. Namely, such a null sequence becomes an infinitesimal in Cauchy's and Lazare Carnot's terminology.\nNon-standard analysis allows one to rigorously treat infinitesimals. The article by Yamashita (2007) contains a bibliography on modern Dirac delta functions in the context of an infinitesimal-enriched continuum provided by the hyperreals. Here the Dirac delta can be given by an actual function, having the property that for every real function F one has \n\n\n\n\u222b\nF\n(\nx\n)\n\n\u03b4\n\n\u03b1\n\n\n(\nx\n)\n=\nF\n(\n0\n)\n\n\n{\\displaystyle \\int F(x)\\delta _{\\alpha }(x)=F(0)}\n\n as anticipated by Fourier and Cauchy.\nA so-called uniform \"pulse train\" of Dirac delta measures, which is known as a Dirac comb, or as the Shah distribution, creates a sampling function, often used in digital signal processing (DSP) and discrete time signal analysis. The Dirac comb is given as the infinite sum, whose limit is understood in the distribution sense,\nwhich is a sequence of point masses at each of the integers.\nUp to an overall normalizing constant, the Dirac comb is equal to its own Fourier transform. This is significant because if f is any Schwartz function, then the periodization of f is given by the convolution\nIn particular,\nis precisely the Poisson summation formula.[62]\nThe Sokhotski\u2013Plemelj theorem, important in quantum mechanics, relates the delta function to the distribution p.v.1/x, the Cauchy principal value of the function 1/x, defined by\nSokhotsky's formula states that[63]\nHere the limit is understood in the distribution sense, that for all compactly supported smooth functions f,\nThe Kronecker delta \u03b4ij is the quantity defined by\nfor all integers i, j. This function then satisfies the following analog of the sifting property: if \n\n\n\n(\n\na\n\ni\n\n\n\n)\n\ni\n\u2208\n\nZ\n\n\n\n\n\n{\\displaystyle (a_{i})_{i\\in \\mathbf {Z} }}\n\n is any doubly infinite sequence, then\nSimilarly, for any real or complex valued continuous function f on R, the Dirac delta satisfies the sifting property\nThis exhibits the Kronecker delta function as a discrete analog of the Dirac delta function.[64]\nIn probability theory and statistics, the Dirac delta function is often used to represent a discrete distribution, or a partially discrete, partially continuous distribution, using a probability density function (which is normally used to represent fully continuous distributions). For example, the probability density function f(x) of a discrete distribution consisting of points x = {x1, ..., xn}, with corresponding probabilities p1, ..., pn, can be written as\nAs another example, consider a distribution which 6/10 of the time returns a standard normal distribution, and 4/10 of the time returns exactly the value 3.5 (i.e. a partly continuous, partly discrete mixture distribution). The density function of this distribution can be written as\nThe delta function is also used in a completely different way to represent the local time of a diffusion process (like Brownian motion). The local time of a stochastic process B(t) is given by\nand represents the amount of time that the process spends at the point x in the range of the process. More precisely, in one dimension this integral can be written\nwhere 1[x\u2212\u03b5, x+\u03b5] is the indicator function of the interval [x\u2212\u03b5, x+\u03b5].\nWe give an example of how the delta function is expedient in quantum mechanics. The wave function of a particle gives the probability amplitude of finding a particle within a given region of space. Wave functions are assumed to be elements of the Hilbert space L2 of square-integrable functions, and the total probability of finding a particle within a given interval is the integral of the magnitude of the wave function squared over the interval. A set {\u03c6n} of wave functions is orthonormal if they are normalized by\nwhere \u03b4 here refers to the Kronecker delta. A set of orthonormal wave functions is complete in the space of square-integrable functions if any wave function \u03c8 can be expressed as a combination of the \u03c6n:\nwith \n\n\n\n\nc\n\nn\n\n\n=\n\u27e8\n\n\u03c6\n\nn\n\n\n\n|\n\n\u03c8\n\u27e9\n\n\n{\\displaystyle c_{n}=\\langle \\varphi _{n}|\\psi \\rangle }\n\n. Complete orthonormal systems of wave functions appear naturally as the eigenfunctions of the Hamiltonian (of a bound system) in quantum mechanics that measures the energy levels, which are called the eigenvalues. The set of eigenvalues, in this case, is known as the spectrum of the Hamiltonian. In bra\u2013ket notation, as above, this equality implies the resolution of the identity:\nHere the eigenvalues are assumed to be discrete, but the set of eigenvalues of an observable may be continuous rather than discrete. An example is the position observable, Q\u03c8(x) = x\u03c8(x). The spectrum of the position (in one dimension) is the entire real line, and is called a continuous spectrum. However, unlike the Hamiltonian, the position operator lacks proper eigenfunctions. The conventional way to overcome this shortcoming is to widen the class of available functions by allowing distributions as well: that is, to replace the Hilbert space of quantum mechanics by an appropriate rigged Hilbert space.[65] In this context, the position operator has a complete set of eigen-distributions, labeled by the points y of the real line, given by\nThe eigenfunctions of position are denoted by \n\n\n\n\n\u03c6\n\ny\n\n\n=\n\n|\n\ny\n\u27e9\n\n\n{\\displaystyle \\varphi _{y}=|y\\rangle }\n\n in Dirac notation, and are known as position eigenstates.\nSimilar considerations apply to the eigenstates of the momentum operator, or indeed any other self-adjoint unbounded operator P on the Hilbert space, provided the spectrum of P is continuous and there are no degenerate eigenvalues. In that case, there is a set \u03a9 of real numbers (the spectrum), and a collection \u03c6y of distributions indexed by the elements of \u03a9, such that\nThat is, \u03c6y are the eigenvectors of P. If the eigenvectors are normalized so that\nin the distribution sense, then for any test function \u03c8,\nwhere\nThat is, as in the discrete case, there is a resolution of the identity\nwhere the operator-valued integral is again understood in the weak sense. If the spectrum of P has both continuous and discrete parts, then the resolution of the identity involves a summation over the discrete spectrum and an integral over the continuous spectrum.\nThe delta function also has many more specialized applications in quantum mechanics, such as the delta potential models for a single and double potential well.\nThe delta function can be used in structural mechanics to describe transient loads or point loads acting on structures. The governing equation of a simple mass\u2013spring system excited by a sudden force impulse I at time t = 0 can be written\nwhere m is the mass, \u03be the deflection and k the spring constant.\nAs another example, the equation governing the static deflection of a slender beam is, according to Euler\u2013Bernoulli theory,\nwhere EI is the bending stiffness of the beam, w the deflection, x the spatial coordinate and q(x) the load distribution. If a beam is loaded by a point force F at x = x0, the load distribution is written\nAs integration of the delta function results in the Heaviside step function, it follows that the static deflection of a slender beam subject to multiple point loads is described by a set of piecewise polynomials.\nAlso a point moment acting on a beam can be described by delta functions. Consider two opposing point forces F at a distance d apart. They then produce a moment M = Fd acting on the beam. Now, let the distance d approach the limit zero, while M is kept constant. The load distribution, assuming a clockwise moment acting at x = 0, is written\nPoint moments can thus be represented by the derivative of the delta function. Integration of the beam equation again results in piecewise polynomial deflection.", 
    "dbpedia_url": "http://dbpedia.org/resource/Unit_impulse", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Unit_impulse\n"
}