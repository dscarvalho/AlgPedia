{
    "about": "In numerical analysis, some methods for the numerical solution of ordinary differential equations (including the special case of numerical integration) use an adaptive stepsize in order to control the errors of the method and to ensure stability properties such as A-stability. Romberg's method is an example of a numerical integration method which uses an adaptive stepsize.", 
    "name": "Adaptive Stepsize", 
    "classification": "Numerical Analysis", 
    "full_text": "In numerical analysis, some methods for the numerical solution of ordinary differential equations (including the special case of numerical integration) use an adaptive stepsize in order to control the errors of the method and to ensure stability properties such as A-stability. Romberg's method is an example of a numerical integration method which uses an adaptive stepsize.\n\n\nFor simplicity, the following example uses the simplest integration method, the Euler method; in practice, higher-order methods such as Runge\u2013Kutta methods are preferred due to their superior convergence and stability properties.\nConsider the initial value problem\nwhere y and f may denote vectors (in which case this equation represents a system of coupled ODEs in several variables).\nWe are given the function f(t,y) and the initial conditions (a, ya), and we are interested in finding the solution at t=b. Let y(b) denote the exact solution at b, and let yb denote the solution that we compute. We write \n\n\n\n\ny\n\nb\n\n\n+\n\u03f5\n=\ny\n(\nb\n)\n\n\n{\\displaystyle y_{b}+\\epsilon =y(b)}\n\n, where \n\n\n\n\u03f5\n\n\n{\\displaystyle \\epsilon }\n\n is the error in the numerical solution.\nFor a sequence (tn) of values of t, with tn = a + nh, the Euler method gives approximations to the corresponding values of y(tn) as\nThe local truncation error of this approximation is defined by\nand by Taylor's theorem, it can be shown that (provided f is sufficiently smooth) the local truncation error is proportional to the square of the step size:\nwhere c is some constant of proportionality.\nWe have marked this solution and its error with a \n\n\n\n(\n0\n)\n\n\n{\\displaystyle (0)}\n\n.\nThe value of c is not known to us. Let us now apply Euler's method again with a different step size to generate a second approximation to y(tn+1). We get a second solution, which we label with a \n\n\n\n(\n1\n)\n\n\n{\\displaystyle (1)}\n\n. Take the new step size to be one half of the original step size, and apply two steps of Euler's method. This second solution is presumably more accurate. Since we have to apply Euler's method twice, the local error is (in the worst case) twice the original error.\nHere, we assume error factor \n\n\n\nc\n\n\n{\\displaystyle c}\n\n is constant over the interval \n\n\n\n[\nt\n,\nt\n+\nh\n]\n\n\n{\\displaystyle [t,t+h]}\n\n. In reality its rate of change is proportional to \n\n\n\n\ny\n\n(\n3\n)\n\n\n(\nt\n)\n\n\n{\\displaystyle y^{(3)}(t)}\n\n. Subtracting solutions gives the error estimate:\nThis local error estimate is third order accurate.\nThe local error estimate can be used to decide how stepsize \n\n\n\nh\n\n\n{\\displaystyle h}\n\n should be modified to achieve the desired accuracy. For example, if a local tolerance of \n\n\n\nt\no\nl\n\n\n{\\displaystyle tol}\n\n is allowed, we could let h evolve like:\nThe \n\n\n\n0.9\n\n\n{\\displaystyle 0.9}\n\n is a safety factor to ensure success on the next try. The minimum and maximum are to prevent extreme changes from the previous stepsize. This should, in principle give an error of about \n\n\n\n0.9\n\u00d7\nt\no\nl\n\n\n{\\displaystyle 0.9\\times tol}\n\n in the next try. If \n\n\n\n\n|\n\n\n\u03c4\n\nn\n+\n1\n\n\n(\n1\n)\n\n\n\n|\n\n<\nt\no\nl\n\n\n{\\displaystyle |\\tau _{n+1}^{(1)}|<tol}\n\n, we consider the step successful, and the error estimate is used to improve the solution:\nThis solution is actually third order accurate in the local scope (second order in the global scope), but since there is no error estimate for it, this doesn't help in reducing the number of steps. This technique is called Richardson extrapolation.\nBeginning with an initial stepsize of \n\n\n\nh\n=\nb\n\u2212\na\n\n\n{\\displaystyle h=b-a}\n\n, this theory facilitates our controllable integration of the ODE from point \n\n\n\na\n\n\n{\\displaystyle a}\n\n to \n\n\n\nb\n\n\n{\\displaystyle b}\n\n, using an optimal number of steps given a local error tolerance. A drawback is that the step size may become prohibitively small, especially when using the low-order Euler method.\nSimilar methods can be developed for higher order methods, such as the 4th order Runge-Kutta method. Also, a global error tolerance can be achieved by scaling the local error to global scope.\nAdaptive stepsize methods that use a so-called 'embedded' error estimate include the Runge\u2013Kutta\u2013Fehlberg, Cash\u2013Karp and Dormand\u2013Prince methods. These methods are considered to be more computationally efficient, but have lower accuracy in their error estimates.", 
    "dbpedia_url": "http://dbpedia.org/resource/Adaptive_stepsize", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Adaptive_stepsize\n"
}