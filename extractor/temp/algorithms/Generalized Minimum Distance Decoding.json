{
    "about": "In coding theory, generalized minimum-distance (GMD) decoding provides an efficient algorithm for decoding concatenated codes, which is based on using an errors-and-erasures decoder for the outer code.", 
    "classification": "Error Detection And Correction", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Generalized_minimum-distance_decoding\n", 
    "full_text": "In coding theory, generalized minimum-distance (GMD) decoding provides an efficient algorithm for decoding concatenated codes, which is based on using an errors-and-erasures decoder for the outer code.\nA naive decoding algorithm for concatenated codes can not be an optimal way of decoding because it does not take into account the information that maximum likelihood decoding (MLD) gives. In other words, in the naive algorithm, inner received codewords are treated the same regardless of the difference between their hamming distances. Intuitively, the outer decoder should place higher confidence in symbols whose inner encodings are close to the received word. David Forney in 1966 devised a better algorithm called generalized minimum distance (GMD) decoding which makes use of those information better. This method is achieved by measuring confidence of each received codeword, and erasing symbols whose confidence is below a desired value. And GMD decoding algorithm was one of the first examples of soft-decision decoders. We will present three versions of the GMD decoding algorithm. The first two will be randomized algorithms while the last one will be a deterministic algorithm.\n\n\nConsider the received word \n\n\n\n\ny\n\n=\n(\n\ny\n\n1\n\n\n,\n\u2026\n,\n\ny\n\nN\n\n\n)\n\u2208\n[\n\nq\n\nn\n\n\n\n]\n\nN\n\n\n\n\n{\\displaystyle \\mathbf {y} =(y_{1},\\ldots ,y_{N})\\in [q^{n}]^{N}}\n\n which corrupted by noisy channel. The following is the algorithm description for the general case. In this algorithm, we can decode y by just declaring an erasure at every bad position and running the errors and erasure decoding algorithm for \n\n\n\n\nC\n\nout\n\n\n\n\n{\\displaystyle C_{\\text{out}}}\n\n on the resulting vector.\nRandomized_Decoder\nGiven\u00a0: \n\n\n\n\ny\n\n=\n(\n\ny\n\n1\n\n\n,\n\u2026\n,\n\ny\n\nN\n\n\n)\n\u2208\n[\n\nq\n\nn\n\n\n\n]\n\nN\n\n\n\n\n{\\displaystyle \\mathbf {y} =(y_{1},\\dots ,y_{N})\\in [q^{n}]^{N}}\n\n.\nTheorem 1. Let y be a received word such that there exists a codeword \n\n\n\n\nc\n\n=\n(\n\nc\n\n1\n\n\n,\n\u22ef\n,\n\nc\n\nN\n\n\n)\n\u2208\n\nC\n\nout\n\n\n\u2218\n\n\nC\n\nin\n\n\n\n\u2286\n[\n\nq\n\nn\n\n\n\n]\n\nN\n\n\n\n\n{\\displaystyle \\mathbf {c} =(c_{1},\\cdots ,c_{N})\\in C_{\\text{out}}\\circ {C_{\\text{in}}}\\subseteq [q^{n}]^{N}}\n\n such that \n\n\n\n\u0394\n(\n\nc\n\n,\n\ny\n\n)\n<\n\n\n\n\nD\nd\n\n2\n\n\n\n\n\n{\\displaystyle \\Delta (\\mathbf {c} ,\\mathbf {y} )<{\\tfrac {Dd}{2}}}\n\n. Then the deterministic GMD algorithm outputs \n\n\n\n\nc\n\n\n\n{\\displaystyle \\mathbf {c} }\n\n.\nNote that a naive decoding algorithm for concatenated codes can correct up to \n\n\n\n\n\n\n\nD\nd\n\n4\n\n\n\n\n\n{\\displaystyle {\\tfrac {Dd}{4}}}\n\n errors.\nRemark. If \n\n\n\n2\n\ne\n\u2032\n\n+\n\ns\n\u2032\n\n<\nD\n\n\n{\\displaystyle 2e'+s'<D}\n\n, then the algorithm in Step 2 will output \n\n\n\n\nc\n\n\n\n{\\displaystyle \\mathbf {c} }\n\n. The lemma above says that in expectation, this is indeed the case. Note that this is not enough to prove Theorem 1, but can be crucial in developing future variations of the algorithm.\nProof of lemma 1. For every \n\n\n\n1\n\u2264\ni\n\u2264\nN\n,\n\n\n{\\displaystyle 1\\leq i\\leq N,}\n\n define \n\n\n\n\ne\n\ni\n\n\n=\n\u0394\n(\n\ny\n\ni\n\n\n,\n\nc\n\ni\n\n\n)\n.\n\n\n{\\displaystyle e_{i}=\\Delta (y_{i},c_{i}).}\n\n This implies that\nNext for every \n\n\n\n1\n\u2264\ni\n\u2264\nN\n\n\n{\\displaystyle 1\\leq i\\leq N}\n\n, we define two indicator variables:\nWe claim that we are done if we can show that for every \n\n\n\n1\n\u2264\ni\n\u2264\nN\n\n\n{\\displaystyle 1\\leq i\\leq N}\n\n:\nClearly, by definition\nFurther, by the linearity of expectation, we get\nTo prove (2) we consider two cases: \n\n\n\ni\n\n\n{\\displaystyle i}\n\n-th block is correctly decoded (Case 1), \n\n\n\ni\n\n\n{\\displaystyle i}\n\n-th block is incorrectly decoded (Case 2):\nCase 1: \n\n\n\n(\n\nc\n\ni\n\n\n=\n\nC\n\nin\n\n\n(\n\ny\n\ni\n\n\u2032\n\n)\n)\n\n\n{\\displaystyle (c_{i}=C_{\\text{in}}(y_{i}'))}\n\n\nNote that if \n\n\n\n\ny\n\ni\n\n\u2033\n\n=\n?\n\n\n{\\displaystyle y_{i}''=?}\n\n then \n\n\n\n\nX\n\ni\n\n\ne\n\n\n=\n0\n\n\n{\\displaystyle X_{i}^{e}=0}\n\n, and \n\n\n\nPr\n[\n\ny\n\ni\n\n\u2033\n\n=\n?\n]\n=\n\n\n\n\n2\n\n\u03c9\n\ni\n\n\n\nd\n\n\n\n\n\n{\\displaystyle \\Pr[y_{i}''=?]={\\tfrac {2\\omega _{i}}{d}}}\n\n implies \n\n\n\n\nE\n\n[\n\nX\n\ni\n\n\n?\n\n\n]\n=\nPr\n[\n\nX\n\ni\n\n\n?\n\n\n=\n1\n]\n=\n\n\n\n\n2\n\n\u03c9\n\ni\n\n\n\nd\n\n\n\n,\n\n\n{\\displaystyle \\mathbb {E} [X_{i}^{?}]=\\Pr[X_{i}^{?}=1]={\\tfrac {2\\omega _{i}}{d}},}\n\n and \n\n\n\n\nE\n\n[\n\nX\n\ni\n\n\ne\n\n\n]\n=\nPr\n[\n\nX\n\ni\n\n\ne\n\n\n=\n1\n]\n=\n0\n\n\n{\\displaystyle \\mathbb {E} [X_{i}^{e}]=\\Pr[X_{i}^{e}=1]=0}\n\n.\nFurther, by definition we have\nCase 2: \n\n\n\n(\n\nc\n\ni\n\n\n\u2260\n\nC\n\nin\n\n\n(\n\ny\n\ni\n\n\u2032\n\n)\n)\n\n\n{\\displaystyle (c_{i}\\neq C_{\\text{in}}(y_{i}'))}\n\n\nIn this case, \n\n\n\n\nE\n\n[\n\nX\n\ni\n\n\n?\n\n\n]\n=\n\n\n\n\n2\n\n\u03c9\n\ni\n\n\n\nd\n\n\n\n\n\n{\\displaystyle \\mathbb {E} [X_{i}^{?}]={\\tfrac {2\\omega _{i}}{d}}}\n\n and \n\n\n\n\nE\n\n[\n\nX\n\ni\n\n\ne\n\n\n]\n=\nPr\n[\n\nX\n\ni\n\n\ne\n\n\n=\n1\n]\n=\n1\n\u2212\n\n\n\n\n2\n\n\u03c9\n\ni\n\n\n\nd\n\n\n\n.\n\n\n{\\displaystyle \\mathbb {E} [X_{i}^{e}]=\\Pr[X_{i}^{e}=1]=1-{\\tfrac {2\\omega _{i}}{d}}.}\n\n\nSince \n\n\n\n\nc\n\ni\n\n\n\u2260\n\nC\n\nin\n\n\n(\n\ny\n\ni\n\n\u2032\n\n)\n,\n\ne\n\ni\n\n\n+\n\n\u03c9\n\ni\n\n\n\u2a7e\nd\n\n\n{\\displaystyle c_{i}\\neq C_{\\text{in}}(y_{i}'),e_{i}+\\omega _{i}\\geqslant d}\n\n. This follows another case analysis when \n\n\n\n(\n\n\u03c9\n\ni\n\n\n=\n\u0394\n(\n\nC\n\nin\n\n\n(\n\ny\n\ni\n\n\u2032\n\n)\n,\n\ny\n\ni\n\n\n)\n<\n\n\n\nd\n2\n\n\n\n)\n\n\n{\\displaystyle (\\omega _{i}=\\Delta (C_{\\text{in}}(y_{i}'),y_{i})<{\\tfrac {d}{2}})}\n\n or not.\nFinally, this implies\nIn the following sections, we will finally show that the deterministic version of the algorithm above can do unique decoding of \n\n\n\n\nC\n\nout\n\n\n\u2218\n\nC\n\nin\n\n\n\n\n{\\displaystyle C_{\\text{out}}\\circ C_{\\text{in}}}\n\n up to half its design distance.\nNote that, in the previous version of the GMD algorithm in step \"3\", we do not really need to use \"fresh\" randomness for each \n\n\n\ni\n\n\n{\\displaystyle i}\n\n. Now we come up with another randomized version of the GMD algorithm that uses the same randomness for every \n\n\n\ni\n\n\n{\\displaystyle i}\n\n. This idea follows the algorithm below.\nModified_Randomized_Decoder\nGiven\u00a0: \n\n\n\n\ny\n\n=\n(\n\ny\n\n1\n\n\n,\n\u2026\n,\n\ny\n\nN\n\n\n)\n\u2208\n[\n\nq\n\nn\n\n\n\n]\n\nN\n\n\n\n\n{\\displaystyle \\mathbf {y} =(y_{1},\\ldots ,y_{N})\\in [q^{n}]^{N}}\n\n, pick \n\n\n\n\u03b8\n\u2208\n[\n0\n,\n1\n]\n\n\n{\\displaystyle \\theta \\in [0,1]}\n\n at random. Then every for every \n\n\n\n1\n\u2264\ni\n\u2264\nN\n\n\n{\\displaystyle 1\\leq i\\leq N}\n\n:\nFor the proof of Lemma 1, we only use the randomness to show that\nIn this version of the GMD algorithm, we note that\nThe second equality above follows from the choice of \n\n\n\n\u03b8\n\n\n{\\displaystyle \\theta }\n\n. The proof of Lemma 1 can be also used to show \n\n\n\n\nE\n\n[\n2\n\ne\n\u2032\n\n+\n\ns\n\u2032\n\n]\n<\nD\n\n\n{\\displaystyle \\mathbb {E} [2e'+s']<D}\n\n for version2 of GMD. In the next section, we will see how to get a deterministic version of the GMD algorithm by choosing \n\n\n\n\u03b8\n\n\n{\\displaystyle \\theta }\n\n from a polynomially sized set as opposed to the current infinite set \n\n\n\n[\n0\n,\n1\n]\n\n\n{\\displaystyle [0,1]}\n\n.\nLet \n\n\n\nQ\n=\n{\n0\n,\n1\n}\n\u222a\n{\n\n\n\n2\n\n\u03c9\n\n1\n\n\n\nd\n\n\n,\n\u2026\n,\n\n\n\n2\n\n\u03c9\n\nN\n\n\n\nd\n\n\n}\n\n\n{\\displaystyle Q=\\{0,1\\}\\cup \\{{2\\omega _{1} \\over d},\\ldots ,{2\\omega _{N} \\over d}\\}}\n\n. Since for each \n\n\n\ni\n,\n\n\u03c9\n\ni\n\n\n=\nmin\n(\n\u0394\n(\n\n\ny\n\ni\n\n\u2032\n\n\n,\n\n\ny\n\ni\n\n\n\n)\n,\n\n\nd\n2\n\n\n)\n\n\n{\\displaystyle i,\\omega _{i}=\\min(\\Delta (\\mathbf {y_{i}'} ,\\mathbf {y_{i}} ),{d \\over 2})}\n\n, we have\nwhere \n\n\n\n\nq\n\n1\n\n\n<\n\u22ef\n<\n\nq\n\nm\n\n\n\n\n{\\displaystyle q_{1}<\\cdots <q_{m}}\n\n for some \n\n\n\nm\n\u2264\n\n\u230a\n\n\nd\n2\n\n\n\u230b\n\n\n\n{\\displaystyle m\\leq \\left\\lfloor {\\frac {d}{2}}\\right\\rfloor }\n\n. Note that for every \n\n\n\n\u03b8\n\u2208\n[\n\nq\n\ni\n\n\n,\n\nq\n\ni\n+\n1\n\n\n]\n\n\n{\\displaystyle \\theta \\in [q_{i},q_{i+1}]}\n\n, the step 1 of the second version of randomized algorithm outputs the same \n\n\n\n\n\ny\n\n\u2033\n\n.\n\n\n{\\displaystyle \\mathbf {y} ''.}\n\n. Thus, we need to consider all possible value of \n\n\n\n\u03b8\n\u2208\nQ\n\n\n{\\displaystyle \\theta \\in Q}\n\n. This gives the deterministic algorithm below.\nDeterministic_Decoder\nGiven\u00a0: \n\n\n\n\ny\n\n=\n(\n\ny\n\n1\n\n\n,\n\u2026\n,\n\ny\n\nN\n\n\n)\n\u2208\n[\n\nq\n\nn\n\n\n\n]\n\nN\n\n\n\n\n{\\displaystyle \\mathbf {y} =(y_{1},\\ldots ,y_{N})\\in [q^{n}]^{N}}\n\n, for every \n\n\n\n\u03b8\n\u2208\nQ\n\n\n{\\displaystyle \\theta \\in Q}\n\n, repeat the following.\nEvery loop of 1~4 can be run in polynomial time, the algorithm above can also be computed in polynomial time. Specifically, each call to an errors and erasures decoder of \n\n\n\n<\nd\nD\n\n/\n\n2\n\n\n{\\displaystyle <dD/2}\n\n errors takes \n\n\n\nO\n(\nd\n)\n\n\n{\\displaystyle O(d)}\n\n time. Finally, the runtime of the algorithm above is \n\n\n\nO\n(\nN\nQ\n\nn\n\nO\n(\n1\n)\n\n\n+\nN\n\nT\n\nout\n\n\n)\n\n\n{\\displaystyle O(NQn^{O(1)}+NT_{\\text{out}})}\n\n where \n\n\n\n\nT\n\nout\n\n\n\n\n{\\displaystyle T_{\\text{out}}}\n\n is the running time of the outer errors and erasures decoder.", 
    "name": "Generalized Minimum Distance Decoding"
}