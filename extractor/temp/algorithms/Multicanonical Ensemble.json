{
    "about": "In statistics and physics, multicanonical ensemble (also called multicanonical sampling or flat histogram) is a Markov chain Monte Carlo sampling technique that uses the Metropolis\u2013Hastings algorithm to compute integrals where the integrand has a rough landscape with multiple local minima. It samples states according to the inverse of the density of states,[1] which has to be known a priori or be computed using other techniques like the Wang and Landau algorithm.[2] Multicanonical sampling is an important technique for spin systems like the Ising model or spin glasses.[1][3][4]", 
    "name": "Multicanonical Ensemble", 
    "classification": "Statistical Algorithms", 
    "full_text": "In statistics and physics, multicanonical ensemble (also called multicanonical sampling or flat histogram) is a Markov chain Monte Carlo sampling technique that uses the Metropolis\u2013Hastings algorithm to compute integrals where the integrand has a rough landscape with multiple local minima. It samples states according to the inverse of the density of states,[1] which has to be known a priori or be computed using other techniques like the Wang and Landau algorithm.[2] Multicanonical sampling is an important technique for spin systems like the Ising model or spin glasses.[1][3][4]\n\n\nIn systems with a large number of degrees of freedom, like spin systems, Monte Carlo integration is required. In this integration, importance sampling and in particular the Metropolis algorithm, is a very important technique.[3] However, the Metropolis algorithm samples states according to \n\n\n\nexp\n\u2061\n(\n\u2212\n\u03b2\nE\n)\n\n\n{\\displaystyle \\exp(-\\beta E)}\n\n where beta is the inverse of the temperature. This means that an energy barrier of \n\n\n\n\u0394\nE\n\n\n{\\displaystyle \\Delta E}\n\n on the energy spectrum is exponentially difficult to overcome.[1] Systems with multiple local energy minima like the Potts model become hard to sample as the algorithm gets stuck in the system's local minima.[3] This motivates other approaches, namely, other sampling distributions.\nMulticanonical ensemble uses the Metropolis\u2013Hastings algorithm with a sampling distribution given by the inverse of the density of states of the system, contrary to the sampling distribution \n\n\n\nexp\n\u2061\n(\n\u2212\n\u03b2\nE\n)\n\n\n{\\displaystyle \\exp(-\\beta E)}\n\n of the Metropolis algorithm.[1] With this choice, on average, the number of states sampled at each energy is constant, i.e. it is a simulation with a \"flat histogram\" on energy. This leads to an algorithm for which the energy barriers are no longer difficult to overcome. Another advantage over the Metropolis algorithm is that the sampling is independent of the temperature of the system, which means that one simulation allows the estimation of thermodynamical variables for all temperatures (thus the name \"multicanonical\": several temperatures). This is a great improvement in the study of first order phase transitions.[1]\nThe biggest problem in performing a multicanonical ensemble is that the density of states has to be known a priori.[2][3] One important contribution to multicanonical sampling was the Wang and Landau algorithm, which asymptotically converges to a multicanonical ensemble while calculating the density of states during the convergence.[2]\nThe multicanonical ensemble is not restricted to physical systems. It can be employed on abstract systems which have a cost function F. By using the density of states with respect to F, the method becomes general for computing higher-dimensional integrals or finding local minima.[5]\nConsider a system and it phase-space \n\n\n\n\u03a9\n\n\n{\\displaystyle \\Omega }\n\n characterized by a configuration \n\n\n\n\nr\n\n\n\n{\\displaystyle {\\boldsymbol {r}}}\n\n in \n\n\n\n\u03a9\n\n\n{\\displaystyle \\Omega }\n\n and a \"cost\" function F from the system's phase-space to a one-dimensional space \n\n\n\n\u0393\n\n\n{\\displaystyle \\Gamma }\n\n: \n\n\n\nF\n(\n\u03a9\n)\n=\n\u0393\n=\n[\n\n\u0393\n\nmin\n\n\n,\n\n\u0393\n\nmax\n\n\n]\n\n\n{\\displaystyle F(\\Omega )=\\Gamma =[\\Gamma _{\\min },\\Gamma _{\\max }]}\n\n, the spectrum of F.\nThe Ising model with N sites is an example of such a system; the phase-space is a discrete phase-space defined by all possible configurations of N spins \n\n\n\n\nr\n\n=\n(\n\n\u03c3\n\n1\n\n\n,\n\u2026\n,\n\n\u03c3\n\ni\n\n\n,\n\u2026\n,\n\n\u03c3\n\nN\n\n\n)\n\n\n{\\displaystyle {\\boldsymbol {r}}=(\\sigma _{1},\\ldots ,\\sigma _{i},\\ldots ,\\sigma _{N})}\n\n where \n\n\n\n\n\u03c3\n\ni\n\n\n\u2208\n{\n\u2212\n1\n,\n1\n}\n\n\n{\\displaystyle \\sigma _{i}\\in \\{-1,1\\}}\n\n. The cost function is the Hamiltonian of the system:\nwhere \n\n\n\n<\ni\n,\nj\n>\n\n\n{\\displaystyle <i,j>}\n\n is the sum over neighborhoods and \n\n\n\n\nJ\n\ni\nj\n\n\n\n\n{\\displaystyle J_{ij}}\n\n is the interaction matrix.\nThe energy spectrum is \n\n\n\n\u0393\n=\n[\n\nE\n\nmin\n\n\n,\n\nE\n\nmax\n\n\n]\n\n\n{\\displaystyle \\Gamma =[E_{\\min },E_{\\max }]}\n\n which, in this case, depends on the particular \n\n\n\n\nJ\n\ni\nj\n\n\n\n\n{\\displaystyle J_{ij}}\n\n used. If all \n\n\n\n\nJ\n\ni\nj\n\n\n\n\n{\\displaystyle J_{ij}}\n\n are 1 (the ferromagnetic Ising model), \n\n\n\n\nE\n\nmin\n\n\n=\n0\n\n\n{\\displaystyle E_{\\min }=0}\n\n (e.g. all spins are 1.) and \n\n\n\n\nE\n\nmax\n\n\n=\n2\nD\nN\n\n\n{\\displaystyle E_{\\max }=2DN}\n\n (half spins are up, half spins are down). Also notice that in this system, \n\n\n\n\u0393\n\u2208\n\nZ\n\n\n\n{\\displaystyle \\Gamma \\in \\mathbb {Z} }\n\n\nThe computation of an average quantity \n\n\n\n\u27e8\nQ\n\u27e9\n\n\n{\\displaystyle \\langle Q\\rangle }\n\n over the phase-space requires the evaluation of an integral:\nwhere \n\n\n\n\nP\n\nr\n\n\n(\n\nr\n\n)\n\n\n{\\displaystyle P_{r}({\\boldsymbol {r}})}\n\n is the weight of each state (e.g. \n\n\n\n\nP\n\nr\n\n\n(\n\nr\n\n)\n=\n1\n\n/\n\nV\n\n\n{\\displaystyle P_{r}({\\boldsymbol {r}})=1/V}\n\n correspond to uniformly distributed states).\nWhen Q does not depend on the particular state but only on the particular F's value of the state \n\n\n\nF\n(\n\nr\n\n)\n=\n\nF\n\nr\n\n\n\n\n{\\displaystyle F({\\boldsymbol {r}})=F_{\\boldsymbol {r}}}\n\n, the formula for \n\n\n\n\u27e8\nQ\n\u27e9\n\n\n{\\displaystyle \\langle Q\\rangle }\n\n can be integrated over f by adding a dirac delta function and be written as\nwhere\nis the marginal distribution of F.\nA system in contact with a heat bath at inverse temperature \n\n\n\n\u03b2\n\n\n{\\displaystyle \\beta }\n\n is an example for computing this kind of integral. For instance, the mean energy of the system is weighted by the Boltzmann factor:\nwhere\nThe marginal distribution \n\n\n\nP\n(\nE\n)\n\n\n{\\displaystyle P(E)}\n\n is given by\nwhere \n\n\n\n\u03c1\n(\nE\n)\n\n\n{\\displaystyle \\rho (E)}\n\n is the density of states.\nThe average energy \n\n\n\n\u27e8\nE\n\u27e9\n\n\n{\\displaystyle \\langle E\\rangle }\n\n is then given by\nWhen the system has a large number of degrees of freedom, an analytical expression for \n\n\n\n\u27e8\nQ\n\u27e9\n\n\n{\\displaystyle \\langle Q\\rangle }\n\n is often hard to obtain, and Monte Carlo integration is typically employed in the computation of \n\n\n\n\u27e8\nQ\n\u27e9\n\n\n{\\displaystyle \\langle Q\\rangle }\n\n. On the simplest formulation, the method chooses N uniformly distributed states \n\n\n\n\n\nr\n\n\ni\n\n\n\u2208\n\u03a9\n\n\n{\\displaystyle {\\boldsymbol {r}}_{i}\\in \\Omega }\n\n, and uses the estimator\nfor computing \n\n\n\n\u27e8\nQ\n\u27e9\n\n\n{\\displaystyle \\langle Q\\rangle }\n\n because \n\n\n\n\n\n\nQ\n\u00af\n\n\n\nN\n\n\n\n\n{\\displaystyle {\\overline {Q}}_{N}}\n\n converges almost surely to \n\n\n\n\u27e8\nQ\n\u27e9\n\n\n{\\displaystyle \\langle Q\\rangle }\n\n by the strong law of large numbers:\nOne typical problem of this convergence is that the variance of Q can be very high, which leads to a high computational effort to achieve reasonable results.\nOn the previous example, the states that mostly contribute to the integral are the ones with low energy. If the states are sampled uniformly, on average, the number of states which are sampled with energy E is given by the density of states. This density of states can be centered far away from the energy's minima and thus the average can be difficult to obtain.\nTo improve this convergence, the Metropolis\u2013Hastings algorithm was proposed. Generally, Monte Carlo methods' idea is to use importance sampling to improve the convergence of the estimator \n\n\n\n\n\n\nQ\n\u00af\n\n\n\nN\n\n\n\n\n{\\displaystyle {\\overline {Q}}_{N}}\n\n by sampling states according to an arbitrary distribution \n\n\n\n\u03c0\n(\n\nr\n\n)\n\n\n{\\displaystyle \\pi ({\\boldsymbol {r}})}\n\n, and use the appropriate estimator:\nThis estimator generalizes the estimator of the mean for samples drawn from an arbitrary distribution. Therefore, when \n\n\n\n\u03c0\n(\n\nr\n\n)\n\n\n{\\displaystyle \\pi ({\\boldsymbol {r}})}\n\n is a uniform distribution, it corresponds the one used on a uniform sampling above.\nWhen the system is a physical system in contact with a heat bath, each state \n\n\n\n\nr\n\n\n\n{\\displaystyle {\\boldsymbol {r}}}\n\n is weighted according to the Boltzmann factor, \n\n\n\n\nP\n\nr\n\n\n(\n\n\nr\n\n\ni\n\n\n)\n\u221d\nexp\n\u2061\n(\n\u2212\n\u03b2\n\nF\n\nr\n\n\n)\n\n\n{\\displaystyle P_{r}({\\boldsymbol {r}}_{i})\\propto \\exp(-\\beta F_{\\boldsymbol {r}})}\n\n. In Monte Carlo, the canonical ensemble is defined by choosing \n\n\n\n\u03c0\n(\n\nr\n\n)\n\n\n{\\displaystyle \\pi ({\\boldsymbol {r}})}\n\n to be proportional to \n\n\n\n\nP\n\nr\n\n\n(\n\n\nr\n\n\ni\n\n\n)\n\n\n{\\displaystyle P_{r}({\\boldsymbol {r}}_{i})}\n\n. In this situation, the estimator corresponds to a simple arithmetic average:\nHistorically, this occurred because the original idea[6] was to use Metropolis\u2013Hastings algorithm to compute averages on a system in contact with a heat bath where the weight is given by the Boltzmann factor, \n\n\n\nP\n(\n\nx\n\n)\n\u221d\nexp\n\u2061\n(\n\u2212\n\u03b2\nE\n(\n\nr\n\n)\n)\n\n\n{\\displaystyle P({\\boldsymbol {x}})\\propto \\exp(-\\beta E({\\boldsymbol {r}}))}\n\n.[3]\nWhile the it is often the case that the sampling distribution \n\n\n\n\u03c0\n\n\n{\\displaystyle \\pi }\n\n is chosen to be the weight distribution \n\n\n\n\nP\n\nr\n\n\n\n\n{\\displaystyle P_{r}}\n\n, this does not need to be the case. One situation where the canonical ensemble is not an efficient choice is when it takes an arbitrarily long time to converge.[1] One situation where this happens is when the function F has multiple local minima. The computational cost for the algorithm to leave a specific region with a local minimum exponentially increases with the cost function's value of the minimum. That is, the deeper the minimum, the more time the algorithm spends there, and the harder it will be to leave (exponentially growing with the depth of the local minimum).\nOne way to avoid becoming stuck in local minima of the cost function is to make the sampling technique \"invisible\" to local minima. This is the basis of the multicanonical ensemble.\nThe multicanonical ensemble is defined by choosing the sampling distribution to be\nwhere \n\n\n\nP\n(\nf\n)\n\n\n{\\displaystyle P(f)}\n\n is the marginal distribution of F defined above. The consequence of this choice is that the average number of samples with a given value of f, m(f), is given by\nthat is, the average number of samples does not depend on f: all costs f are equally sampled regardless of whether they are more or less probable. This motivates the name \"flat-histogram\". For systems in contact with a heat bath, the sampling is independent of the temperature and one simulation allows to study all temperatures.\nOn the ferromagnetic Ising model with N sites (exemplified on previous section), the density of states can be analytically computed. In this case, a multicanonical ensemble can be used to compute any other quantity Q by sampling the system according to \n\n\n\nP\n(\n\nr\n\n)\n\n\n{\\displaystyle P({\\boldsymbol {r}})}\n\n and using the proper estimator \n\n\n\n\n\nQ\n\u00af\n\n\n\n\n{\\displaystyle {\\overline {Q}}}\n\n defined on the previous section.\nLike in any other Monte Carlo method, there are correlations of the samples being drawn from \n\n\n\nP\n(\n\nr\n\n)\n\n\n{\\displaystyle P({\\boldsymbol {r}})}\n\n. A typical measurement of the correlation is the tunneling time. The tunneling time is defined by the number of Markov steps (of the Markov chain) the simulation needs to perform a round-trip between the minimum and maximum of the spectrum of F. One motivation to use the tunneling time is that when it crosses the spectra, it passes through the region of the maximum of the density of states, thus de-correlating the process. On the other hand using round-trips ensures that the system visits all the spectrum.\nBecause the histogram is flat on the variable F, a multicanonic ensemble can be seen as a diffusion process (i.e. a random walk) on the one-dimensional line of F values. Detailed balance of the process dictates that there is no drift on the process.[7] This implies that the tunneling time, in local dynamics, should scale as a diffusion process, and thus the tunneling time should scale quadratically with the size of the spectrum, N:\nHowever, in some systems (the Ising model being the most paradigmatic), the scaling suffers from critical slowing down: it is \n\n\n\n\nN\n\n2\n+\nz\n\n\n\n\n{\\displaystyle N^{2+z}}\n\n where \n\n\n\nz\n>\n0\n\n\n{\\displaystyle z>0}\n\n depends on the particular system.[4]\nNon-local dynamics were developed to improve the scaling to a quadratic scaling[8] (see the Wolff algorithm), beating the critical slowing down. However, it is still an open question whether there is a local dynamics that does not suffer from critical slowing down in spin systems like the Ising model.", 
    "dbpedia_url": "http://dbpedia.org/resource/Multicanonical_ensemble", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Multicanonical_ensemble\n"
}