{
    "about": "In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems.[1] Given \n\n\n\nX\n\n\n{\\displaystyle X}\n\n as the vector space of all possible inputs, and Y\u00a0=\u00a0{\u20131,1} as the vector space of all possible outputs, we wish to find a function \n\n\n\nf\n:\nX\n\u21a6\n\nR\n\n\n\n{\\displaystyle f:X\\mapsto \\mathbb {R} }\n\n which best maps \n\n\n\nf\n(\n\n\n\nx\n\u2192\n\n\n\n)\n\n\n{\\displaystyle f({\\vec {x}})}\n\n to \n\n\n\ny\n\n\n{\\displaystyle y}\n\n.[2] However, because of incomplete information, noise in the measurement, or probabilistic components in the underlying process, it is possible for the same \n\n\n\n\n\n\nx\n\u2192\n\n\n\n\n\n{\\displaystyle {\\vec {x}}}\n\n to generate different \n\n\n\ny\n\n\n{\\displaystyle y}\n\n.[3] As a result, the goal of the learning problem is to minimize expected risk, defined as", 
    "name": "Loss Functions For Classification", 
    "classification": "Machine Learning Algorithms", 
    "full_text": "In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems.[1] Given \n\n\n\nX\n\n\n{\\displaystyle X}\n\n as the vector space of all possible inputs, and Y\u00a0=\u00a0{\u20131,1} as the vector space of all possible outputs, we wish to find a function \n\n\n\nf\n:\nX\n\u21a6\n\nR\n\n\n\n{\\displaystyle f:X\\mapsto \\mathbb {R} }\n\n which best maps \n\n\n\nf\n(\n\n\n\nx\n\u2192\n\n\n\n)\n\n\n{\\displaystyle f({\\vec {x}})}\n\n to \n\n\n\ny\n\n\n{\\displaystyle y}\n\n.[2] However, because of incomplete information, noise in the measurement, or probabilistic components in the underlying process, it is possible for the same \n\n\n\n\n\n\nx\n\u2192\n\n\n\n\n\n{\\displaystyle {\\vec {x}}}\n\n to generate different \n\n\n\ny\n\n\n{\\displaystyle y}\n\n.[3] As a result, the goal of the learning problem is to minimize expected risk, defined as\nwhere \n\n\n\nV\n(\nf\n(\n\n\n\nx\n\u2192\n\n\n\n)\n,\ny\n)\n\n\n{\\displaystyle V(f({\\vec {x}}),y)}\n\n represents the loss function, and \n\n\n\np\n(\n\n\n\nx\n\u2192\n\n\n\n,\ny\n)\n\n\n{\\displaystyle p({\\vec {x}},y)}\n\n represents the probability distribution of the data, which can equivalently be written using Bayes' theorem as\nIn practice, the probability distribution \n\n\n\np\n(\n\n\n\nx\n\u2192\n\n\n\n,\ny\n)\n\n\n{\\displaystyle p({\\vec {x}},y)}\n\n is unknown. Consequently, utilizing a training set of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n independently and identically distributed samples\ndrawn from the data sample space, one seeks to minimize empirical risk\nas a proxy for expected risk.[3] (See statistical learning theory for a more detailed description.)\nFor computational ease, it is standard practice to write loss functions as functions of only one variable. Within classification, loss functions are generally written solely in terms of the product of the true classifier \n\n\n\ny\n\n\n{\\displaystyle y}\n\n and the predicted value \n\n\n\nf\n(\n\n\n\nx\n\u2192\n\n\n\n)\n\n\n{\\displaystyle f({\\vec {x}})}\n\n.[4] Selection of a loss function within this framework\nimpacts the optimal \n\n\n\n\nf\n\nS\n\n\n\u2217\n\n\n\n\n{\\displaystyle f_{S}^{*}}\n\n which minimizes empirical risk, as well as the computational complexity of the learning algorithm.\nGiven the binary nature of classification, a natural selection for a loss function (assuming equal cost for false positives and false negatives) would be the 0\u20131 indicator function which takes the value of 0 if the predicted classification equals that of the true class or a 1 if the predicted classification does not match the true class. This selection is modeled by\nwhere \n\n\n\nH\n\n\n{\\displaystyle H}\n\n indicates the Heaviside step function. However, this loss function is non-convex and non-smooth, and solving for the optimal solution is an NP-hard combinatorial optimization problem.[5] As a result, it is better to substitute continuous, convex loss function surrogates which are tractable for commonly used learning algorithms. In addition to their computational tractability, one can show that the solutions to the learning problem using these loss surrogates allows for the recovery of the actual solution to the original classification problem.[6] Some of these surrogates are described below.\n\n\nUtilizing Bayes' theorem, it can be shown that the optimal \n\n\n\n\nf\n\n\u2217\n\n\n\n\n{\\displaystyle f^{*}}\n\n for a binary classification problem is equivalent to\n(when \n\n\n\np\n(\n1\n\u2223\n\n\n\nx\n\u2192\n\n\n\n)\n\u2260\np\n(\n\u2212\n1\n\u2223\n\n\n\nx\n\u2192\n\n\n\n)\n\n\n{\\displaystyle p(1\\mid {\\vec {x}})\\neq p(-1\\mid {\\vec {x}})}\n\n).\nFurthermore, it can be shown that for any convex loss function \n\n\n\nV\n(\ny\n\nf\n\n0\n\n\n(\n\n\n\nx\n\u2192\n\n\n\n)\n)\n\n\n{\\displaystyle V(yf_{0}({\\vec {x}}))}\n\n, where \n\n\n\n\nf\n\n0\n\n\n\n\n{\\displaystyle f_{0}}\n\n is the function that minimizes this loss, if \n\n\n\n\nf\n\n0\n\n\n(\n\n\n\nx\n\u2192\n\n\n\n)\n\u2260\n0\n\n\n{\\displaystyle f_{0}({\\vec {x}})\\neq 0}\n\n and \n\n\n\nV\n\n\n{\\displaystyle V}\n\n is decreasing in a neighborhood of 0, then \n\n\n\n\nf\n\n\u2217\n\n\n(\n\n\n\nx\n\u2192\n\n\n\n)\n=\nsgn\n\u2061\n(\n\nf\n\n0\n\n\n(\n\n\n\nx\n\u2192\n\n\n\n)\n)\n\n\n{\\displaystyle f^{*}({\\vec {x}})=\\operatorname {sgn} (f_{0}({\\vec {x}}))}\n\n where \n\n\n\nsgn\n\n\n{\\displaystyle \\operatorname {sgn} }\n\n is the sign function (for proof see [1]). Note also that \n\n\n\n\nf\n\n0\n\n\n(\n\n\n\nx\n\u2192\n\n\n\n)\n\u2260\n0\n\n\n{\\displaystyle f_{0}({\\vec {x}})\\neq 0}\n\n in practice when the loss function is differentiable at the origin. This fact confers a consistency property upon all convex loss functions; specifically, all convex loss functions will lead to consistent results with the 0\u20131 loss function given the presence of infinite data. Consequently, we can bound the difference of any of these convex loss function from expected risk.[1]\nGiven the properties of binary classification, it is possible to simplify the calculation of expected risk from the integral specified above. Specifically,\nThe second equality follows from the properties described above. The third equality follows since \n\n\n\n\n\n\nx\n\u2192\n\n\n\n\n\n{\\displaystyle {\\vec {x}}}\n\n is simply data and since \n\n\n\n\n\u222b\n\nX\n\n\np\n(\nx\n)\n\nd\nx\n=\n1\n\n\n{\\displaystyle \\int _{X}p(x)\\,dx=1}\n\n. Finally, the fourth equality follows from the fact that 1 and \u22121 are the only possible values for \n\n\n\ny\n\n\n{\\displaystyle y}\n\n, and the fifth because \n\n\n\np\n(\n\u2212\n1\n\u2223\nx\n)\n=\n1\n\u2212\np\n(\n1\n\u2223\nx\n)\n\n\n{\\displaystyle p(-1\\mid x)=1-p(1\\mid x)}\n\n. As a result, one can solve for the minimizers of \n\n\n\nI\n[\nf\n]\n\n\n{\\displaystyle I[f]}\n\n for any convex loss functions with these properties by differentiating the last equality with respect to \n\n\n\nf\n\n\n{\\displaystyle f}\n\n and setting the derivative equal to 0. Thus, minimizers for all of the loss function surrogates described below are easily obtained as functions of only \n\n\n\nf\n(\n\n\n\nx\n\u2192\n\n\n\n)\n\n\n{\\displaystyle f({\\vec {x}})}\n\n and \n\n\n\np\n(\n1\n\u2223\nx\n)\n\n\n{\\displaystyle p(1\\mid x)}\n\n.[3]\nWhile more commonly used in regression, the square loss function can be re-written as a function \n\n\n\n\u03d5\n(\ny\nf\n(\n\n\n\nx\n\u2192\n\n\n\n)\n)\n\n\n{\\displaystyle \\phi (yf({\\vec {x}}))}\n\n and utilized for classification. Defined as\nthe square loss function is both convex and smooth and matches the 0\u20131 indicator function when \n\n\n\ny\nf\n(\n\n\n\nx\n\u2192\n\n\n\n)\n=\n0\n\n\n{\\displaystyle yf({\\vec {x}})=0}\n\n and when \n\n\n\ny\nf\n(\n\n\n\nx\n\u2192\n\n\n\n)\n=\n1\n\n\n{\\displaystyle yf({\\vec {x}})=1}\n\n. However, the square loss function tends to penalize outliers excessively, leading to slower convergence rates (with regards to sample complexity) than for the logistic loss or hinge loss functions.[1] In addition, functions which yield high values of \n\n\n\nf\n(\n\n\n\nx\n\u2192\n\n\n\n)\n\n\n{\\displaystyle f({\\vec {x}})}\n\n for some \n\n\n\nx\n\u2208\nX\n\n\n{\\displaystyle x\\in X}\n\n will perform poorly with the square loss function, since high values of \n\n\n\ny\nf\n(\n\n\n\nx\n\u2192\n\n\n\n)\n\n\n{\\displaystyle yf({\\vec {x}})}\n\n will be penalized severely, regardless of whether the signs of \n\n\n\ny\n\n\n{\\displaystyle y}\n\n and \n\n\n\nf\n(\n\n\n\nx\n\u2192\n\n\n\n)\n\n\n{\\displaystyle f({\\vec {x}})}\n\n match.\nA benefit of the square loss function is that its structure lends itself to easy cross validation of regularization parameters. Specifically for Tikhonov regularization, one can solve for the regularization parameter using leave-one-out cross-validation in the same time as it would take to solve a single problem.[7]\nThe minimizer of \n\n\n\nI\n[\nf\n]\n\n\n{\\displaystyle I[f]}\n\n for the square loss function is\nThis function notably equals \n\n\n\n\nf\n\n\u2217\n\n\n\n\n{\\displaystyle f^{*}}\n\n for the 0\u20131 loss function when \n\n\n\np\n(\n1\n\u2223\nx\n)\n=\n1\n\n\n{\\displaystyle p(1\\mid x)=1}\n\n or \n\n\n\np\n(\n1\n\u2223\nx\n)\n=\n0\n\n\n{\\displaystyle p(1\\mid x)=0}\n\n, but predicts a value between the two classifications when the classification of \n\n\n\n\n\n\nx\n\u2192\n\n\n\n\n\n{\\displaystyle {\\vec {x}}}\n\n is not known with absolute certainty.\nThe hinge loss function is defined as\nThe hinge loss provides a relatively tight, convex upper bound on the 0\u20131 indicator function. Specifically, the hinge loss equals the 0\u20131 indicator function when \n\n\n\nsgn\n\u2061\n(\nf\n(\n\n\n\nx\n\u2192\n\n\n\n)\n)\n=\ny\n\n\n{\\displaystyle \\operatorname {sgn} (f({\\vec {x}}))=y}\n\n and \n\n\n\n\n|\n\ny\nf\n(\n\n\n\nx\n\u2192\n\n\n\n)\n\n|\n\n\u2265\n1\n\n\n{\\displaystyle |yf({\\vec {x}})|\\geq 1}\n\n. In addition, the empirical risk minimization of this loss is equivalent to the classical formulation for support vector machines (SVMs). Correctly classified points lying outside the margin boundaries of the support vectors are not penalized, whereas points within the margin boundaries or on the wrong side of the hyperplane are penalized in a linear fashion compared to their distance from the correct boundary.[5]\nWhile the hinge loss function is both convex and continuous, it is not smooth (that is not differentiable) at \n\n\n\ny\nf\n(\n\n\n\nx\n\u2192\n\n\n\n)\n=\n1\n\n\n{\\displaystyle yf({\\vec {x}})=1}\n\n. Consequently, the hinge loss function cannot be used with gradient descent methods or stochastic gradient descent methods which rely on differentiability over the entire domain. However, the hinge loss does have a subgradient at \n\n\n\ny\nf\n(\n\n\n\nx\n\u2192\n\n\n\n)\n=\n1\n\n\n{\\displaystyle yf({\\vec {x}})=1}\n\n, which allows for the utilization of subgradient descent methods.[5] SVMs utilizing the hinge loss function can also be solved using quadratic programming.\nThe minimizer of \n\n\n\nI\n[\nf\n]\n\n\n{\\displaystyle I[f]}\n\n for the hinge loss function is\nwhen \n\n\n\np\n(\n1\n\u2223\nx\n)\n\u2260\n0.5\n\n\n{\\displaystyle p(1\\mid x)\\neq 0.5}\n\n, which matches that of the 0\u20131 indicator function. This conclusion makes the hinge loss quite attractive, as bounds can be placed on the difference between expected risk and the sign of hinge loss function.[1]\nThe logistic loss function is defined as\nThis function displays a similar convergence rate to the hinge loss function, and since it is continuous, gradient descent methods can be utilized. However, the logistic loss function does not assign zero penalty to any points. Instead, functions that correctly classify points with high confidence (i.e., with high values of \n\n\n\n\n|\n\nf\n(\n\n\n\nx\n\u2192\n\n\n\n)\n\n|\n\n\n\n{\\displaystyle |f({\\vec {x}})|}\n\n) are penalized less. This structure leads the logistic loss function to be sensitive to outliers in the data.\nThe minimizer of \n\n\n\nI\n[\nf\n]\n\n\n{\\displaystyle I[f]}\n\n for the logistic loss function is\nThis function is undefined when \n\n\n\np\n(\n1\n\u2223\nx\n)\n=\n1\n\n\n{\\displaystyle p(1\\mid x)=1}\n\n or \n\n\n\np\n(\n1\n\u2223\nx\n)\n=\n0\n\n\n{\\displaystyle p(1\\mid x)=0}\n\n (tending toward \u221e and \u2212\u221e respectively), but predicts a smooth curve which grows when \n\n\n\np\n(\n1\n\u2223\nx\n)\n\n\n{\\displaystyle p(1\\mid x)}\n\n increases and equals 0 when \n\n\n\np\n(\n1\n\u2223\nx\n)\n=\n0.5\n\n\n{\\displaystyle p(1\\mid x)=0.5}\n\n.[3]\nUsing the alternative label convention \n\n\n\nt\n=\n(\n1\n+\ny\n)\n\n/\n\n2\n\n\n{\\displaystyle t=(1+y)/2}\n\n so that \n\n\n\nt\n\u2208\n{\n0\n,\n1\n}\n\n\n{\\displaystyle t\\in \\{0,1\\}}\n\n, the cross entropy loss is defined as\nThe cross entropy loss is closely related to the Kullback-Leibler divergence between the empirical distribution and the predicted distribution. This function is not naturally represented as a product of the true label and the predicted value, but is convex and can be minimized using stochastic gradient descent methods. The cross entropy loss is ubiquitous in modern deep neural networks.", 
    "dbpedia_url": "http://dbpedia.org/resource/Loss_functions_for_classification", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Loss_functions_for_classification\n"
}