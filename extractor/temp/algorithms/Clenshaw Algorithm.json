{
    "about": "In numerical analysis, the Clenshaw algorithm,[1] also called Clenshaw summation,[2] is a recursive method to evaluate a linear combination of Chebyshev polynomials. It is a generalization of Horner's method for evaluating a linear combination of monomials.", 
    "name": "Clenshaw Algorithm", 
    "classification": "Numerical Analysis", 
    "full_text": "In numerical analysis, the Clenshaw algorithm,[1] also called Clenshaw summation,[2] is a recursive method to evaluate a linear combination of Chebyshev polynomials. It is a generalization of Horner's method for evaluating a linear combination of monomials.\nIt generalizes to more than just Chebyshev polynomials; it applies to any class of functions that can be defined by a three-term recurrence relation.[3]\n\n\nIn full generality, the Clenshaw algorithm computes the weighted sum of a finite series of functions \n\n\n\n\n\u03d5\n\nk\n\n\n(\nx\n)\n\n\n{\\displaystyle \\phi _{k}(x)}\n\n:\nwhere \n\n\n\n\n\u03d5\n\nk\n\n\n,\n\nk\n=\n0\n,\n1\n,\n\u2026\n\n\n{\\displaystyle \\phi _{k},\\;k=0,1,\\ldots }\n\n is a sequence of functions that satisfy the linear recurrence relation\nwhere the coefficients \n\n\n\n\n\u03b1\n\nk\n\n\n(\nx\n)\n\n\n{\\displaystyle \\alpha _{k}(x)}\n\n and \n\n\n\n\n\u03b2\n\nk\n\n\n(\nx\n)\n\n\n{\\displaystyle \\beta _{k}(x)}\n\n are known in advance.\nThe algorithm is most useful when \n\n\n\n\n\u03d5\n\nk\n\n\n(\nx\n)\n\n\n{\\displaystyle \\phi _{k}(x)}\n\n are functions that are complicated to compute directly, but \n\n\n\n\n\u03b1\n\nk\n\n\n(\nx\n)\n\n\n{\\displaystyle \\alpha _{k}(x)}\n\n and \n\n\n\n\n\u03b2\n\nk\n\n\n(\nx\n)\n\n\n{\\displaystyle \\beta _{k}(x)}\n\n are particularly simple. In the most common applications, \n\n\n\n\u03b1\n(\nx\n)\n\n\n{\\displaystyle \\alpha (x)}\n\n does not depend on \n\n\n\nk\n\n\n{\\displaystyle k}\n\n, and \n\n\n\n\u03b2\n\n\n{\\displaystyle \\beta }\n\n is a constant that depends on neither \n\n\n\nx\n\n\n{\\displaystyle x}\n\n nor \n\n\n\nk\n\n\n{\\displaystyle k}\n\n.\nTo perform the summation for given series of coefficients \n\n\n\n\na\n\n0\n\n\n,\n\u2026\n,\n\na\n\nn\n\n\n\n\n{\\displaystyle a_{0},\\ldots ,a_{n}}\n\n, compute the values \n\n\n\n\nb\n\nk\n\n\n(\nx\n)\n\n\n{\\displaystyle b_{k}(x)}\n\n by the \"reverse\" recurrence formula:\nNote that this computation makes no direct reference to the functions \n\n\n\n\n\u03d5\n\nk\n\n\n(\nx\n)\n\n\n{\\displaystyle \\phi _{k}(x)}\n\n. After computing \n\n\n\n\nb\n\n2\n\n\n(\nx\n)\n\n\n{\\displaystyle b_{2}(x)}\n\n and \n\n\n\n\nb\n\n1\n\n\n(\nx\n)\n\n\n{\\displaystyle b_{1}(x)}\n\n, the desired sum can be expressed in terms of them and the simplest functions \n\n\n\n\n\u03d5\n\n0\n\n\n(\nx\n)\n\n\n{\\displaystyle \\phi _{0}(x)}\n\n and \n\n\n\n\n\u03d5\n\n1\n\n\n(\nx\n)\n\n\n{\\displaystyle \\phi _{1}(x)}\n\n:\nSee Fox and Parker[4] for more information and stability analyses.\nA particularly simple case occurs when evaluating a polynomial of the form\nThe functions are simply\nand are produced by the recurrence coefficients \n\n\n\n\u03b1\n(\nx\n)\n=\nx\n\n\n{\\displaystyle \\alpha (x)=x}\n\n and \n\n\n\n\u03b2\n=\n0\n\n\n{\\displaystyle \\beta =0}\n\n.\nIn this case, the recurrence formula to compute the sum is\nand, in this case, the sum is simply\nwhich is exactly the usual Horner's method.\nConsider a truncated Chebyshev series\nThe coefficients in the recursion relation for the Chebyshev polynomials are\nwith the initial conditions\nThus, the recurrence is\nand the final sum is\nOne way to evaluate this is to continue the recurrence one more step, and compute\n(note the doubled a0 coefficient) followed by\nClenshaw summation is extensively used in geodetic applications.[2] A simple application is summing the trigonometric series to compute the meridian arc distance on the surface of an ellipsoid. These have the form\nLeaving off the initial \n\n\n\n\nC\n\n0\n\n\n\n\u03b8\n\n\n{\\displaystyle C_{0}\\,\\theta }\n\n term, the remainder is a summation of the appropriate form. There is no leading term because \n\n\n\n\n\u03d5\n\n0\n\n\n(\n\u03b8\n)\n=\nsin\n\u2061\n0\n\u03b8\n=\nsin\n\u2061\n0\n=\n0\n\n\n{\\displaystyle \\phi _{0}(\\theta )=\\sin 0\\theta =\\sin 0=0}\n\n.\nThe recurrence relation for \n\n\n\nsin\n\u2061\nk\n\u03b8\n\n\n{\\displaystyle \\sin k\\theta }\n\n is\nmaking the coefficients in the recursion relation\nand the evaluation of the series is given by\nThe final step is made particularly simple because \n\n\n\n\n\u03d5\n\n0\n\n\n(\n\u03b8\n)\n=\nsin\n\u2061\n0\n=\n0\n\n\n{\\displaystyle \\phi _{0}(\\theta )=\\sin 0=0}\n\n, so the end of the recurrence is simply \n\n\n\n\nb\n\n1\n\n\n(\n\u03b8\n)\nsin\n\u2061\n(\n\u03b8\n)\n\n\n{\\displaystyle b_{1}(\\theta )\\sin(\\theta )}\n\n; the \n\n\n\n\nC\n\n0\n\n\n\n\u03b8\n\n\n{\\displaystyle C_{0}\\,\\theta }\n\n term is added separately:\nNote that the algorithm requires only the evaluation of two trigonometric quantities \n\n\n\ncos\n\u2061\n\u03b8\n\n\n{\\displaystyle \\cos \\theta }\n\n and \n\n\n\nsin\n\u2061\n\u03b8\n\n\n{\\displaystyle \\sin \\theta }\n\n.\nSometimes it necessary to compute the difference of two meridian arcs in a way that maintains high relative accuracy. This is accomplished by using trigonometric identities to write\nClenshaw summation can be applied in this case[5] provided we simultaneously compute \n\n\n\nm\n(\n\n\u03b8\n\n1\n\n\n)\n+\nm\n(\n\n\u03b8\n\n2\n\n\n)\n\n\n{\\displaystyle m(\\theta _{1})+m(\\theta _{2})}\n\n and perform a matrix summation,\nwhere\nThe first element of \n\n\n\n\n\nM\n\n\n(\n\n\u03b8\n\n1\n\n\n,\n\n\u03b8\n\n2\n\n\n)\n\n\n{\\displaystyle {\\mathsf {M}}(\\theta _{1},\\theta _{2})}\n\n is the average value of \n\n\n\nm\n\n\n{\\displaystyle m}\n\n and the second element is the average slope. \n\n\n\n\n\n\nF\n\n\n\nk\n\n\n(\n\n\u03b8\n\n1\n\n\n,\n\n\u03b8\n\n2\n\n\n)\n\n\n{\\displaystyle {\\mathsf {F}}_{k}(\\theta _{1},\\theta _{2})}\n\n satisfies the recurrence relation\nwhere\ntakes the place of \n\n\n\n\u03b1\n\n\n{\\displaystyle \\alpha }\n\n in the recurrence relation, and \n\n\n\n\u03b2\n=\n\u2212\n1\n\n\n{\\displaystyle \\beta =-1}\n\n. The standard Clenshaw algorithm can now be applied to yield\nwhere \n\n\n\n\n\n\nB\n\n\n\nk\n\n\n\n\n{\\displaystyle {\\mathsf {B}}_{k}}\n\n are 2\u00d72 matrices. Finally we have\nThis technique can be used in the limit \n\n\n\n\n\u03b8\n\n2\n\n\n=\n\n\u03b8\n\n1\n\n\n=\n\u03bc\n\n\n{\\displaystyle \\theta _{2}=\\theta _{1}=\\mu }\n\n and \n\n\n\n\u03b4\n=\n0\n\n\n\n{\\displaystyle \\delta =0\\,}\n\n to simultaneously compute \n\n\n\nm\n(\n\u03bc\n)\n\n\n{\\displaystyle m(\\mu )}\n\n and the derivative \n\n\n\nd\nm\n(\n\u03bc\n)\n\n/\n\nd\n\u03bc\n\n\n{\\displaystyle dm(\\mu )/d\\mu }\n\n, provided that, in evaluating \n\n\n\n\n\n\nF\n\n\n\n1\n\n\n\n\n{\\displaystyle {\\mathsf {F}}_{1}}\n\n and \n\n\n\n\n\nA\n\n\n\n\n{\\displaystyle {\\mathsf {A}}}\n\n, we take \n\n\n\n\nlim\n\n\u03b4\n\u2192\n0\n\n\n(\nsin\n\u2061\n\u03b4\n)\n\n/\n\n\u03b4\n=\n1\n\n\n{\\displaystyle \\lim _{\\delta \\rightarrow 0}(\\sin \\delta )/\\delta =1}\n\n.", 
    "dbpedia_url": "http://dbpedia.org/resource/Clenshaw_algorithm", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Clenshaw_algorithm\n"
}