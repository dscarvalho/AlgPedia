{
    "about": "An adaptive filter is a system with a linear filter that has a transfer function controlled by variable parameters and a means to adjust those parameters according to an optimization algorithm. Because of the complexity of the optimization algorithms, almost all adaptive filters are digital filters. Adaptive filters are required for some applications because some parameters of the desired processing operation (for instance, the locations of reflective surfaces in a reverberant space) are not known in advance or are changing. The closed loop adaptive filter uses feedback in the form of an error signal to refine its transfer function.", 
    "name": "Adaptive Filter", 
    "classification": "Digital Signal Processing", 
    "full_text": "An adaptive filter is a system with a linear filter that has a transfer function controlled by variable parameters and a means to adjust those parameters according to an optimization algorithm. Because of the complexity of the optimization algorithms, almost all adaptive filters are digital filters. Adaptive filters are required for some applications because some parameters of the desired processing operation (for instance, the locations of reflective surfaces in a reverberant space) are not known in advance or are changing. The closed loop adaptive filter uses feedback in the form of an error signal to refine its transfer function.\nGenerally speaking, the closed loop adaptive process involves the use of a cost function, which is a criterion for optimum performance of the filter, to feed an algorithm, which determines how to modify filter transfer function to minimize the cost on the next iteration. The most common cost function is the mean square of the error signal.\nAs the power of digital signal processors has increased, adaptive filters have become much more common and are now routinely used in devices such as mobile phones and other communication devices, camcorders and digital cameras, and medical monitoring equipment.\n\n\nThe recording of a heart beat (an ECG), may be corrupted by noise from the AC mains. The exact frequency of the power and its harmonics may vary from moment to moment.\nOne way to remove the noise is to filter the signal with a notch filter at the mains frequency and its vicinity, which could excessively degrade the quality of the ECG since the heart beat would also likely have frequency components in the rejected range.\nTo circumvent this potential loss of information, an adaptive filter could be used. The adaptive filter would take input both from the patient and from the mains and would thus be able to track the actual frequency of the noise as it fluctuates and subtract the noise from the recording. Such an adaptive technique generally allows for a filter with a smaller rejection range, which means, in this case, that the quality of the output signal is more accurate for medical purposes.[1][2]\nThe idea behind a closed loop adaptive filter is that a variable filter is adjusted until the error (the difference between the filter output and the desired signal) is minimized. The Least Mean Squares (LMS) filter and the Recursive Least Squares (RLS) filter are types of adaptive filter.\nThere are two input signals to the adaptive filter: dk and xk which are sometimes called the primary input and the reference input respectively.[3]\nThe filter is controlled by a set of L+1 coefficients or weights.\nThe output is usually \n\n\n\n\n\u03f5\n\nk\n\n\n\n\n{\\displaystyle \\epsilon _{k}}\n\n but it could be \n\n\n\n\ny\n\nk\n\n\n\n\n{\\displaystyle y_{k}}\n\n or it could even be the filter coefficients.[4](Widrow)\nThe input signals are defined as follows:\nThe output signals are defined as follows:\nIf the variable filter has a tapped delay line Finite Impulse Response (FIR) structure, then the impulse response is equal to the filter coefficients. The output of the filter is given by\nIn the ideal case \n\n\n\nv\n\u2261\n0\n,\n\nv\n\n\n\n\u2032\n\n\n\n\u2261\n0\n,\n\ng\n\n\n\n\u2032\n\n\n\n\u2261\n0\n\n\n{\\displaystyle v\\equiv 0,v^{'}\\equiv 0,g^{'}\\equiv 0}\n\n. All the undesired signals in \n\n\n\n\nd\n\nk\n\n\n\n\n{\\displaystyle d_{k}}\n\n are represented by \n\n\n\n\nu\n\nk\n\n\n\n\n{\\displaystyle u_{k}}\n\n. \n\n\n\n\u00a0\n\nx\n\nk\n\n\n\n\n{\\displaystyle \\ x_{k}}\n\n consists entirely of a signal correlated with the undesired signal in \n\n\n\n\nu\n\nk\n\n\n\n\n{\\displaystyle u_{k}}\n\n.\nThe output of the variable filter in the ideal case is\nThe error signal or cost function is the difference between \n\n\n\n\nd\n\nk\n\n\n\n\n{\\displaystyle d_{k}}\n\n and \n\n\n\n\ny\n\nk\n\n\n\n\n{\\displaystyle y_{k}}\n\n\nThe error signal \n\n\n\n\n\u03f5\n\nk\n\n\n\n\n{\\displaystyle \\epsilon _{k}}\n\n is minimized in the mean square sense when \n\n\n\n[\n\nu\n\nk\n\n\n\u2212\n\n\n\n\nu\n^\n\n\n\n\nk\n\n\n]\n\n\n{\\displaystyle [u_{k}-{\\hat {u}}_{k}]}\n\n is minimized. In other words, \n\n\n\n\n\n\n\nu\n^\n\n\n\n\nk\n\n\n\n\n{\\displaystyle {\\hat {u}}_{k}}\n\n is the best mean square estimate of \n\n\n\n\nu\n\nk\n\n\n\n\n{\\displaystyle u_{k}}\n\n. In the ideal case, \n\n\n\n\nu\n\nk\n\n\n=\n\n\n\n\nu\n^\n\n\n\n\nk\n\n\n\n\n{\\displaystyle u_{k}={\\hat {u}}_{k}}\n\n and \n\n\n\n\n\u03f5\n\nk\n\n\n=\n\ng\n\nk\n\n\n\n\n{\\displaystyle \\epsilon _{k}=g_{k}}\n\n, and all that is left after the subtraction is \n\n\n\ng\n\n\n{\\displaystyle g}\n\n which is the unchanged desired signal with all undesired signals removed.\nIn some situations, the reference input \n\n\n\n\nx\n\nk\n\n\n\n\n{\\displaystyle x_{k}}\n\n includes components of the desired signal. This means g' \u2260 0.\nPerfect cancelation of the undesired interference is not possible in the case, but improvement of the signal to interference ratio is possible. The output will be\nThe output signal to interference ratio has a simple formula referred to as power inversion.\nThis formula means that the output signal to interference ratio at a particular frequency is the reciprocal of the reference signal to interference ratio.[5]\nExample: A fast food restaurant has a drive-up window. Before getting to the window, customers place their order by speaking into a microphone. The microphone also picks up noise from the engine and the environment. This microphone provides the primary signal. The signal power from the customer\u2019s voice and the noise power from the engine are equal. It is difficult for the employees in the restaurant to understand the customer. To reduce the amount of interference in the primary microphone, a second microphone is located where it is intended to pick up sounds from the engine. It also picks up the customer\u2019s voice. This microphone is the source of the reference signal. In this case, the engine noise is 50 times more powerful than the customer\u2019s voice. Once the canceler has converged, the primary signal to interference ratio will be improved from 1:1 to 50:1.\nThe adaptive linear combiner (ALC) resembles the adaptive tapped delay line FIR filter except that there is no assumed relationship between the X values. If the X values were from the outputs of a tapped delay line, then the combination of tapped delay line and ALC would comprise an adaptive filter. However, the X values could be the values of an array of pixels. Or they could be the outputs of multiple tapped delay lines. The ALC finds use as an adaptive beam former for arrays of hydrophones or antennas.\nIf the variable filter has a tapped delay line FIR structure, then the LMS update algorithm is especially simple. Typically, after each sample, the coefficients of the FIR filter are adjusted as follows:[6](Widrow)\nThe LMS algorithm does not require that the X values have any particular relationship; therefor it can be used to adapt a linear combiner as well as an FIR filter. In this case the update formula is written as:\nThe effect of the LMS algorithm is at each time, k, to make a small change in each weight. The direction of the change is such that it would decrease the error if it had been applied at time k. The magnitude of the change in each weight depends on \u03bc, the associated X value and the error at time k. The weights making the largest contribution to the output, \n\n\n\n\ny\n\nk\n\n\n\n\n{\\displaystyle y_{k}}\n\n, are changed the most. If the error is zero, then there should be no change in the weights. If the associated value of X is zero, then changing the weight makes no difference, so it is not changed.\n\u03bc controls how fast and how well the algorithm converges to the optimum filter coefficients. If \u03bc is too large, the algorithm will not converge. If \u03bc is too small the algorithm converges slowly and may not be able to track changing conditions. If \u03bc is large but not too large to prevent convergence, the algorithm reaches steady state rapidly but continuously overshoots the optimum weight vector. Sometimes, \u03bc is made large at first for rapid convergence and then decreased to minimize overshoot.\nWidrow and Stearns state in 1985 that they have no knowledge of a proof that the LMS algorithm will converge in all cases.[7]\nHowever under certain assumptions about stationarity and independence it can be shown that the algorithm will converge if\nIn the case of the tapped delay line filter, each input has the same RMS value because they are simply the same values delayed. In this case the total power is\nThis leads to a normalized LMS algorithm:", 
    "dbpedia_url": "http://dbpedia.org/resource/Adaptive_filter", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Adaptive_filter\n"
}