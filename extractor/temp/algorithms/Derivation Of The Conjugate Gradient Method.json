{
    "about": "In numerical linear algebra, the conjugate gradient method is an iterative method for numerically solving the linear system", 
    "name": "Derivation Of The Conjugate Gradient Method", 
    "classification": "Optimization Algorithms And Methods", 
    "full_text": "In numerical linear algebra, the conjugate gradient method is an iterative method for numerically solving the linear system\nwhere \n\n\n\n\nA\n\n\n\n{\\displaystyle {\\boldsymbol {A}}}\n\n is symmetric positive-definite. The conjugate gradient method can be derived from several different perspectives, including specialization of the conjugate direction method for optimization, and variation of the Arnoldi/Lanczos iteration for eigenvalue problems.\nThe intent of this article is to document the important steps in these derivations.\n\n\nThe conjugate gradient method can be seen as a special case of the conjugate direction method applied to minimization of the quadratic function\nIn the conjugate direction method for minimizing\none starts with an initial guess \n\n\n\n\n\nx\n\n\n0\n\n\n\n\n{\\displaystyle {\\boldsymbol {x}}_{0}}\n\n and the corresponding residual \n\n\n\n\n\nr\n\n\n0\n\n\n=\n\nb\n\n\u2212\n\n\nA\nx\n\n\n0\n\n\n\n\n{\\displaystyle {\\boldsymbol {r}}_{0}={\\boldsymbol {b}}-{\\boldsymbol {Ax}}_{0}}\n\n, and computes the iterate and residual by the formulae\nwhere \n\n\n\n\n\np\n\n\n0\n\n\n,\n\n\np\n\n\n1\n\n\n,\n\n\np\n\n\n2\n\n\n,\n\u2026\n\n\n{\\displaystyle {\\boldsymbol {p}}_{0},{\\boldsymbol {p}}_{1},{\\boldsymbol {p}}_{2},\\ldots }\n\n are a series of mutually conjugate directions, i.e.,\nfor any \n\n\n\ni\n\u2260\nj\n\n\n{\\displaystyle i\\neq j}\n\n.\nThe conjugate direction method is imprecise in the sense that no formulae are given for selection of the directions \n\n\n\n\n\np\n\n\n0\n\n\n,\n\n\np\n\n\n1\n\n\n,\n\n\np\n\n\n2\n\n\n,\n\u2026\n\n\n{\\displaystyle {\\boldsymbol {p}}_{0},{\\boldsymbol {p}}_{1},{\\boldsymbol {p}}_{2},\\ldots }\n\n. Specific choices lead to various methods including the conjugate gradient method and Gaussian elimination.\nThe conjugate gradient method can also be seen as a variant of the Arnoldi/Lanczos iteration applied to solving linear systems.\nIn the Arnoldi iteration, one starts with a vector \n\n\n\n\n\nr\n\n\n0\n\n\n\n\n{\\displaystyle {\\boldsymbol {r}}_{0}}\n\n and gradually builds an orthonormal basis \n\n\n\n{\n\n\nv\n\n\n1\n\n\n,\n\n\nv\n\n\n2\n\n\n,\n\n\nv\n\n\n3\n\n\n,\n\u2026\n}\n\n\n{\\displaystyle \\{{\\boldsymbol {v}}_{1},{\\boldsymbol {v}}_{2},{\\boldsymbol {v}}_{3},\\ldots \\}}\n\n of the Krylov subspace\nby defining \n\n\n\n\n\nv\n\n\ni\n\n\n=\n\n\nw\n\n\ni\n\n\n\n/\n\n\u2225\n\n\nw\n\n\ni\n\n\n\n\u2225\n\n2\n\n\n\n\n{\\displaystyle {\\boldsymbol {v}}_{i}={\\boldsymbol {w}}_{i}/\\lVert {\\boldsymbol {w}}_{i}\\rVert _{2}}\n\n where\nIn other words, for \n\n\n\ni\n>\n1\n\n\n{\\displaystyle i>1}\n\n, \n\n\n\n\n\nv\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {v}}_{i}}\n\n is found by Gram-Schmidt orthogonalizing \n\n\n\n\n\nA\nv\n\n\ni\n\u2212\n1\n\n\n\n\n{\\displaystyle {\\boldsymbol {Av}}_{i-1}}\n\n against \n\n\n\n{\n\n\nv\n\n\n1\n\n\n,\n\n\nv\n\n\n2\n\n\n,\n\u2026\n,\n\n\nv\n\n\ni\n\u2212\n1\n\n\n}\n\n\n{\\displaystyle \\{{\\boldsymbol {v}}_{1},{\\boldsymbol {v}}_{2},\\ldots ,{\\boldsymbol {v}}_{i-1}\\}}\n\n followed by normalization.\nPut in matrix form, the iteration is captured by the equation\nwhere\nwith\nWhen applying the Arnoldi iteration to solving linear systems, one starts with \n\n\n\n\n\nr\n\n\n0\n\n\n=\n\nb\n\n\u2212\n\n\nA\nx\n\n\n0\n\n\n\n\n{\\displaystyle {\\boldsymbol {r}}_{0}={\\boldsymbol {b}}-{\\boldsymbol {Ax}}_{0}}\n\n, the residual corresponding to an initial guess \n\n\n\n\n\nx\n\n\n0\n\n\n\n\n{\\displaystyle {\\boldsymbol {x}}_{0}}\n\n. After each step of iteration, one computes \n\n\n\n\n\ny\n\n\ni\n\n\n=\n\n\nH\n\n\ni\n\n\n\u2212\n1\n\n\n(\n\u2225\n\n\nr\n\n\n0\n\n\n\n\u2225\n\n2\n\n\n\n\ne\n\n\n1\n\n\n)\n\n\n{\\displaystyle {\\boldsymbol {y}}_{i}={\\boldsymbol {H}}_{i}^{-1}(\\lVert {\\boldsymbol {r}}_{0}\\rVert _{2}{\\boldsymbol {e}}_{1})}\n\n and the new iterate \n\n\n\n\n\nx\n\n\ni\n\n\n=\n\n\nx\n\n\n0\n\n\n+\n\n\nV\n\n\ni\n\n\n\n\ny\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {x}}_{i}={\\boldsymbol {x}}_{0}+{\\boldsymbol {V}}_{i}{\\boldsymbol {y}}_{i}}\n\n.\nFor the rest of discussion, we assume that \n\n\n\n\nA\n\n\n\n{\\displaystyle {\\boldsymbol {A}}}\n\n is symmetric positive-definite. With symmetry of \n\n\n\n\nA\n\n\n\n{\\displaystyle {\\boldsymbol {A}}}\n\n, the upper Hessenberg matrix \n\n\n\n\n\nH\n\n\ni\n\n\n=\n\n\nV\n\n\ni\n\n\n\nT\n\n\n\n\n\nA\nV\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {H}}_{i}={\\boldsymbol {V}}_{i}^{\\mathrm {T} }{\\boldsymbol {AV}}_{i}}\n\n becomes symmetric and thus tridiagonal. It then can be more clearly denoted by\nThis enables a short three-term recurrence for \n\n\n\n\n\nv\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {v}}_{i}}\n\n in the iteration, and the Arnoldi iteration is reduced to the Lanczos iteration.\nSince \n\n\n\n\nA\n\n\n\n{\\displaystyle {\\boldsymbol {A}}}\n\n is symmetric positive-definite, so is \n\n\n\n\n\nH\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {H}}_{i}}\n\n. Hence, \n\n\n\n\n\nH\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {H}}_{i}}\n\n can be LU factorized without partial pivoting into\nwith convenient recurrences for \n\n\n\n\nc\n\ni\n\n\n\n\n{\\displaystyle c_{i}}\n\n and \n\n\n\n\nd\n\ni\n\n\n\n\n{\\displaystyle d_{i}}\n\n:\nRewrite \n\n\n\n\n\nx\n\n\ni\n\n\n=\n\n\nx\n\n\n0\n\n\n+\n\n\nV\n\n\ni\n\n\n\n\ny\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {x}}_{i}={\\boldsymbol {x}}_{0}+{\\boldsymbol {V}}_{i}{\\boldsymbol {y}}_{i}}\n\n as\nwith\nIt is now important to observe that\nIn fact, there are short recurrences for \n\n\n\n\n\np\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {p}}_{i}}\n\n and \n\n\n\n\n\u03b6\n\ni\n\n\n\n\n{\\displaystyle \\zeta _{i}}\n\n as well:\nWith this formulation, we arrive at a simple recurrence for \n\n\n\n\n\nx\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {x}}_{i}}\n\n:\nThe relations above straightforwardly lead to the direct Lanczos method, which turns out to be slightly more complex.\nIf we allow \n\n\n\n\n\np\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {p}}_{i}}\n\n to scale and compensate for the scaling in the constant factor, we potentially can have simpler recurrences of the form:\nAs premises for the simplification, we now derive the orthogonality of \n\n\n\n\n\nr\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {r}}_{i}}\n\n and conjugacy of \n\n\n\n\n\np\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {p}}_{i}}\n\n, i.e., for \n\n\n\ni\n\u2260\nj\n\n\n{\\displaystyle i\\neq j}\n\n,\nThe residuals are mutually orthogonal because \n\n\n\n\n\nr\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {r}}_{i}}\n\n is essentially a multiple of \n\n\n\n\n\nv\n\n\ni\n+\n1\n\n\n\n\n{\\displaystyle {\\boldsymbol {v}}_{i+1}}\n\n since for \n\n\n\ni\n=\n0\n\n\n{\\displaystyle i=0}\n\n, \n\n\n\n\n\nr\n\n\n0\n\n\n=\n\u2225\n\n\nr\n\n\n0\n\n\n\n\u2225\n\n2\n\n\n\n\nv\n\n\n1\n\n\n\n\n{\\displaystyle {\\boldsymbol {r}}_{0}=\\lVert {\\boldsymbol {r}}_{0}\\rVert _{2}{\\boldsymbol {v}}_{1}}\n\n, for \n\n\n\ni\n>\n0\n\n\n{\\displaystyle i>0}\n\n,\nTo see the conjugacy of \n\n\n\n\n\np\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {p}}_{i}}\n\n, it suffices to show that \n\n\n\n\n\nP\n\n\ni\n\n\n\nT\n\n\n\n\n\nA\nP\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {P}}_{i}^{\\mathrm {T} }{\\boldsymbol {AP}}_{i}}\n\n is diagonal:\nis symmetric and lower triangular simultaneously and thus must be diagonal.\nNow we can derive the constant factors \n\n\n\n\n\u03b1\n\ni\n\n\n\n\n{\\displaystyle \\alpha _{i}}\n\n and \n\n\n\n\n\u03b2\n\ni\n\n\n\n\n{\\displaystyle \\beta _{i}}\n\n with respect to the scaled \n\n\n\n\n\np\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {p}}_{i}}\n\n by solely imposing the orthogonality of \n\n\n\n\n\nr\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {r}}_{i}}\n\n and conjugacy of \n\n\n\n\n\np\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {p}}_{i}}\n\n.\nDue to the orthogonality of \n\n\n\n\n\nr\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {r}}_{i}}\n\n, it is necessary that \n\n\n\n\n\nr\n\n\ni\n+\n1\n\n\n\nT\n\n\n\n\n\nr\n\n\ni\n\n\n=\n(\n\n\nr\n\n\ni\n\n\n\u2212\n\n\u03b1\n\ni\n\n\n\n\nA\np\n\n\ni\n\n\n\n)\n\n\nT\n\n\n\n\n\nr\n\n\ni\n\n\n=\n0\n\n\n{\\displaystyle {\\boldsymbol {r}}_{i+1}^{\\mathrm {T} }{\\boldsymbol {r}}_{i}=({\\boldsymbol {r}}_{i}-\\alpha _{i}{\\boldsymbol {Ap}}_{i})^{\\mathrm {T} }{\\boldsymbol {r}}_{i}=0}\n\n. As a result,\nSimilarly, due to the conjugacy of \n\n\n\n\n\np\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {p}}_{i}}\n\n, it is necessary that \n\n\n\n\n\np\n\n\ni\n+\n1\n\n\n\nT\n\n\n\n\n\nA\np\n\n\ni\n\n\n=\n(\n\n\nr\n\n\ni\n+\n1\n\n\n+\n\n\u03b2\n\ni\n\n\n\n\np\n\n\ni\n\n\n\n)\n\n\nT\n\n\n\n\n\nA\np\n\n\ni\n\n\n=\n0\n\n\n{\\displaystyle {\\boldsymbol {p}}_{i+1}^{\\mathrm {T} }{\\boldsymbol {Ap}}_{i}=({\\boldsymbol {r}}_{i+1}+\\beta _{i}{\\boldsymbol {p}}_{i})^{\\mathrm {T} }{\\boldsymbol {Ap}}_{i}=0}\n\n. As a result,\nThis completes the derivation.", 
    "dbpedia_url": "http://dbpedia.org/resource/Derivation_of_the_conjugate_gradient_method", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Derivation_of_the_conjugate_gradient_method\n"
}