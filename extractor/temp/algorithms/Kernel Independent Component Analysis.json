{
    "about": "In statistics, kernel-independent component analysis (kernel ICA) is an efficient algorithm for independent component analysis which estimates source components by optimizing a generalized variance contrast function, which is based on representations in a reproducing kernel Hilbert space.[1][2] Those contrast functions use the notion of mutual information as a measure of statistical independence.", 
    "classification": "Statistical Algorithms", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Kernel-independent_component_analysis\n", 
    "full_text": "In statistics, kernel-independent component analysis (kernel ICA) is an efficient algorithm for independent component analysis which estimates source components by optimizing a generalized variance contrast function, which is based on representations in a reproducing kernel Hilbert space.[1][2] Those contrast functions use the notion of mutual information as a measure of statistical independence.\nKernel ICA is based on the idea that correlations between two random variables can be represented in a reproducing kernel Hilbert space (RKHS), denoted by \n\n\n\n\n\nF\n\n\n\n\n{\\displaystyle {\\mathcal {F}}}\n\n, associated with a feature map \n\n\n\n\nL\n\nx\n\n\n:\n\n\nF\n\n\n\u21a6\n\nR\n\n\n\n{\\displaystyle L_{x}:{\\mathcal {F}}\\mapsto \\mathbb {R} }\n\n defined for a fixed \n\n\n\nx\n\u2208\n\nR\n\n\n\n{\\displaystyle x\\in \\mathbb {R} }\n\n. The \n\n\n\n\n\nF\n\n\n\n\n{\\displaystyle {\\mathcal {F}}}\n\n-correlation between two random variables \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n is defined as\nwhere the functions \n\n\n\nf\n,\ng\n:\n\nR\n\n\u2192\n\nR\n\n\n\n{\\displaystyle f,g:\\mathbb {R} \\to \\mathbb {R} }\n\n range over \n\n\n\n\n\nF\n\n\n\n\n{\\displaystyle {\\mathcal {F}}}\n\n and\nfor fixed \n\n\n\nf\n,\ng\n\u2208\n\n\nF\n\n\n\n\n{\\displaystyle f,g\\in {\\mathcal {F}}}\n\n.[1] Note that the reproducing property implies that \n\n\n\nf\n(\nx\n)\n=\n\u27e8\n\nL\n\nx\n\n\n,\nf\n\u27e9\n\n\n{\\displaystyle f(x)=\\langle L_{x},f\\rangle }\n\n for fixed \n\n\n\nx\n\u2208\n\nR\n\n\n\n{\\displaystyle x\\in \\mathbb {R} }\n\n and \n\n\n\nf\n\u2208\n\n\nF\n\n\n\n\n{\\displaystyle f\\in {\\mathcal {F}}}\n\n.[3] It follows then that the \n\n\n\n\n\nF\n\n\n\n\n{\\displaystyle {\\mathcal {F}}}\n\n-correlation between two independent random variables is zero.\nThis notion of \n\n\n\n\n\nF\n\n\n\n\n{\\displaystyle {\\mathcal {F}}}\n\n-correlations is used for defining contrast functions that are optimized in the Kernel ICA algorithm. Specifically, if \n\n\n\n\nX\n\n:=\n(\n\nx\n\ni\nj\n\n\n)\n\u2208\n\n\nR\n\n\nn\n\u00d7\nm\n\n\n\n\n{\\displaystyle \\mathbf {X} :=(x_{ij})\\in \\mathbb {R} ^{n\\times m}}\n\n is a prewhitened data matrix, that is, the sample mean of each column is zero and the sample covariance of the rows is the \n\n\n\nm\n\u00d7\nm\n\n\n{\\displaystyle m\\times m}\n\n dimensional identity matrix, Kernel ICA estimates a \n\n\n\nm\n\u00d7\nm\n\n\n{\\displaystyle m\\times m}\n\n dimensional orthogonal matrix \n\n\n\n\nA\n\n\n\n{\\displaystyle \\mathbf {A} }\n\n so as to minimize finite-sample \n\n\n\n\n\nF\n\n\n\n\n{\\displaystyle {\\mathcal {F}}}\n\n-correlations between the columns of \n\n\n\n\nS\n\n:=\n\nX\n\n\n\nA\n\n\n\u2032\n\n\n\n\n{\\displaystyle \\mathbf {S} :=\\mathbf {X} \\mathbf {A} ^{\\prime }}\n\n.\n", 
    "name": "Kernel Independent Component Analysis"
}