{
    "about": "Estimation of distribution algorithms (EDAs), sometimes called probabilistic model-building genetic algorithms (PMBGAs), are stochastic optimization methods that guide the search for the optimum by building and sampling explicit probabilistic models of promising candidate solutions. Optimization is viewed as a series of incremental updates of a probabilistic model, starting with the model encoding the uniform distribution over admissible solutions and ending with the model that generates only the global optima.[1][2][3]", 
    "name": "Estimation Of Distribution Algorithm", 
    "classification": "Stochastic Algorithms", 
    "full_text": "Estimation of distribution algorithms (EDAs), sometimes called probabilistic model-building genetic algorithms (PMBGAs), are stochastic optimization methods that guide the search for the optimum by building and sampling explicit probabilistic models of promising candidate solutions. Optimization is viewed as a series of incremental updates of a probabilistic model, starting with the model encoding the uniform distribution over admissible solutions and ending with the model that generates only the global optima.[1][2][3]\nEDAs belong to the class of evolutionary algorithms. The main difference between EDAs and most conventional evolutionary algorithms is that evolutionary algorithms generate new candidate solutions using an implicit distribution defined by one or more variation operators, whereas EDAs use an explicit probability distribution encoded by a Bayesian network, a multivariate normal distribution, or another model class. Similarly as other evolutionary algorithms, EDAs can be used to solve optimization problems defined over a number of representations from vectors to LISP style S expressions, and the quality of candidate solutions is often evaluated using one or more objective functions.\nThe general procedure of an EDA is outlined in the following:\nUsing explicit probabilistic models in optimization allowed EDAs to feasibly solve optimization problems that were notoriously difficult for most conventional evolutionary algorithms and traditional optimization techniques, such as problems with high levels of epistasis. Nonetheless, the advantage of EDAs is also that these algorithms provide an optimization practitioner with a series of probabilistic models that reveal a lot of information about the problem being solved. This information can in turn be used to design problem-speci\ufb01c neighborhood operators for local search, to bias future runs of EDAs on a similar problem, or to create an efficient computational model of the problem.\nFor example, if the population is represented by bit strings of length 4, the EDA can represent the population of promising solution using a single vector of four probabilities (p1, p2, p3, p4) where each component of p defines the probability of that position being a 1. Using this probability vector it is possible to create an arbitrary number of candidate solutions.\n\n\nThis section describes the models built by some well known EDAs of different levels of complexity. It is always assumed a population \n\n\n\nP\n(\nt\n)\n\n\n{\\displaystyle P(t)}\n\n at the generation \n\n\n\nt\n\n\n{\\displaystyle t}\n\n, a selection operator \n\n\n\nS\n\n\n{\\displaystyle S}\n\n, a model-building operator \n\n\n\n\u03b1\n\n\n{\\displaystyle \\alpha }\n\n and a sampling operator \n\n\n\n\u03b2\n\n\n{\\displaystyle \\beta }\n\n.\nThe most simple EDAs assume that decision variables are independent, i.e. \n\n\n\np\n(\n\nX\n\n1\n\n\n,\n\nX\n\n2\n\n\n)\n=\np\n(\n\nX\n\n1\n\n\n)\n\u22c5\np\n(\n\nX\n\n2\n\n\n)\n\n\n{\\displaystyle p(X_{1},X_{2})=p(X_{1})\\cdot p(X_{2})}\n\n. Therefore, univariate EDAs rely only on univariate statistics and multivariate distributions must be factorized as the product of \n\n\n\nN\n\n\n{\\displaystyle N}\n\n univariate probability distributions,\n\n\n\n\n\nD\n\nUnivariate\n\n\n:=\np\n(\n\nX\n\n1\n\n\n,\n\u2026\n,\n\nX\n\nN\n\n\n)\n=\n\n\u220f\n\ni\n=\n1\n\n\nN\n\n\np\n(\n\nX\n\ni\n\n\n)\n.\n\n\n{\\displaystyle D_{\\text{Univariate}}:=p(X_{1},\\dots ,X_{N})=\\prod _{i=1}^{N}p(X_{i}).}\n\n\nSuch factorizations are used in many different EDAs, next we describe some of them.\nThe UMDA[4] is a simple EDA that uses an operator \n\n\n\n\n\u03b1\n\nU\nM\nD\nA\n\n\n\n\n{\\displaystyle \\alpha _{UMDA}}\n\n to estimate marginal probabilities from a selected population \n\n\n\nS\n(\nP\n(\nt\n)\n)\n\n\n{\\displaystyle S(P(t))}\n\n. By assuming \n\n\n\nS\n(\nP\n(\nt\n)\n)\n\n\n{\\displaystyle S(P(t))}\n\n contain \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n elements, \n\n\n\n\n\u03b1\n\nU\nM\nD\nA\n\n\n\n\n{\\displaystyle \\alpha _{UMDA}}\n\n produces probabilities:\n\n\n\n\n\np\n\nt\n+\n1\n\n\n(\n\nX\n\ni\n\n\n)\n=\n\n\n\n1\n\u03bb\n\n\n\n\n\u2211\n\nx\n\u2208\nS\n(\nP\n(\nt\n)\n)\n\n\n\nx\n\ni\n\n\n,\n\u00a0\n\u2200\ni\n\u2208\n1\n,\n2\n,\n\u2026\n,\nN\n.\n\n\n{\\displaystyle p_{t+1}(X_{i})={\\dfrac {1}{\\lambda }}\\sum _{x\\in S(P(t))}x_{i},~\\forall i\\in 1,2,\\dots ,N.}\n\n\nEvery UMDA step can be described as follows\n\n\n\n\nD\n(\nt\n+\n1\n)\n=\n\n\u03b1\n\nUMDA\n\n\n\u2218\nS\n\u2218\n\n\u03b2\n\n\u03bb\n\n\n(\nD\n(\nt\n)\n)\n.\n\n\n{\\displaystyle D(t+1)=\\alpha _{\\text{UMDA}}\\circ S\\circ \\beta _{\\lambda }(D(t)).}\n\n\nThe PBIL,[5] represents the population implicitly by its model, from which it samples new solutions and updates the model. At each generation, \n\n\n\n\u03bc\n\n\n{\\displaystyle \\mu }\n\n individuals are sampled and \n\n\n\n\u03bb\n\u2264\n\u03bc\n\n\n{\\displaystyle \\lambda \\leq \\mu }\n\n are selected. Such individuals are then used to update the model as follows\n\n\n\n\n\np\n\nt\n+\n1\n\n\n(\n\nX\n\ni\n\n\n)\n=\n(\n1\n\u2212\n\u03b3\n)\n\np\n\nt\n\n\n(\n\nX\n\ni\n\n\n)\n+\n(\n\u03b3\n\n/\n\n\u03bb\n)\n\n\u2211\n\nx\n\u2208\nS\n(\nP\n(\nt\n)\n)\n\n\n\nx\n\ni\n\n\n,\n\u00a0\n\u2200\ni\n\u2208\n1\n,\n2\n,\n\u2026\n,\nN\n,\n\n\n{\\displaystyle p_{t+1}(X_{i})=(1-\\gamma )p_{t}(X_{i})+(\\gamma /\\lambda )\\sum _{x\\in S(P(t))}x_{i},~\\forall i\\in 1,2,\\dots ,N,}\n\n\nwhere \n\n\n\n\u03b3\n\u2208\n(\n0\n,\n1\n]\n\n\n{\\displaystyle \\gamma \\in (0,1]}\n\n is a parameter defining the learning rate, a small value determines that the previous model \n\n\n\n\np\n\nt\n\n\n(\n\nX\n\ni\n\n\n)\n\n\n{\\displaystyle p_{t}(X_{i})}\n\n should be only slightly modified by the new solutions sampled. PBIL can be described as\n\n\n\n\nD\n(\nt\n+\n1\n)\n=\n\n\u03b1\n\nPIBIL\n\n\n\u2218\nS\n\u2218\n\n\u03b2\n\n\u03bc\n\n\n(\nD\n(\nt\n)\n)\n\n\n{\\displaystyle D(t+1)=\\alpha _{\\text{PIBIL}}\\circ S\\circ \\beta _{\\mu }(D(t))}\n\n\nThe CGA,[6] also relies on the implicit populations defined by univariate distributions. At each generation \n\n\n\nt\n\n\n{\\displaystyle t}\n\n, two individuals \n\n\n\nx\n,\ny\n\n\n{\\displaystyle x,y}\n\n are sampled, \n\n\n\nP\n(\nt\n)\n=\n\n\u03b2\n\n2\n\n\n(\nD\n(\nt\n)\n)\n\n\n{\\displaystyle P(t)=\\beta _{2}(D(t))}\n\n. The population \n\n\n\nP\n(\nt\n)\n\n\n{\\displaystyle P(t)}\n\n is then sort in decreasing order of fitness, \n\n\n\n\nS\n\n\nSort\n\n(\nf\n)\n\n\n(\nP\n(\nt\n)\n)\n\n\n{\\displaystyle S_{{\\text{Sort}}(f)}(P(t))}\n\n, with \n\n\n\nu\n\n\n{\\displaystyle u}\n\n being the best and \n\n\n\nv\n\n\n{\\displaystyle v}\n\n being the worst solution. The CGA estimates univariate probabilities as follows\n\n\n\n\n\np\n\nt\n+\n1\n\n\n(\n\nX\n\ni\n\n\n)\n=\n\np\n\nt\n\n\n(\n\nX\n\ni\n\n\n)\n+\n\u03b3\n(\n\nu\n\ni\n\n\n\u2212\n\nv\n\ni\n\n\n)\n,\n\n\u2200\ni\n\u2208\n1\n,\n2\n,\n\u2026\n,\nN\n,\n\n\n{\\displaystyle p_{t+1}(X_{i})=p_{t}(X_{i})+\\gamma (u_{i}-v_{i}),\\quad \\forall i\\in 1,2,\\dots ,N,}\n\n\nwhere, \n\n\n\n\u03b3\n\u2208\n(\n0\n,\n1\n]\n\n\n{\\displaystyle \\gamma \\in (0,1]}\n\n is a constant defining the learning rate, usually set to \n\n\n\n\u03b3\n=\n1\n\n/\n\nN\n\n\n{\\displaystyle \\gamma =1/N}\n\n. The CGA can be defined as\n\n\n\n\nD\n(\nt\n+\n1\n)\n=\n\n\u03b1\n\nCGA\n\n\n\u2218\n\nS\n\n\nSort\n\n(\nf\n)\n\n\n\u2218\n\n\u03b2\n\n2\n\n\n(\nD\n(\nt\n)\n)\n\n\n{\\displaystyle D(t+1)=\\alpha _{\\text{CGA}}\\circ S_{{\\text{Sort}}(f)}\\circ \\beta _{2}(D(t))}\n\n\nAlthough univariate models can be computed efficiently, in many cases they are not representative enough to provide better performance than GAs. In order to overcome such a drawback, the use of bivariate factorizations was proposed in the EDA community, in which dependencies between pairs of variables could be modeled. A bivariate factorization can be defined as follows, where \n\n\n\n\n\u03c0\n\ni\n\n\n\n\n{\\displaystyle \\pi _{i}}\n\n contains a possible variable dependent to \n\n\n\n\nX\n\ni\n\n\n\n\n{\\displaystyle X_{i}}\n\n, i.e. \n\n\n\n\n|\n\n\n\u03c0\n\ni\n\n\n\n|\n\n=\n1\n\n\n{\\displaystyle |\\pi _{i}|=1}\n\n.\n\n\n\n\n\nD\n\nBivariate\n\n\n:=\np\n(\n\nX\n\n1\n\n\n,\n\u2026\n,\n\nX\n\nN\n\n\n)\n=\n\n\u220f\n\ni\n=\n1\n\n\nN\n\n\np\n(\n\nX\n\ni\n\n\n\n|\n\n\n\u03c0\n\ni\n\n\n)\n.\n\n\n{\\displaystyle D_{\\text{Bivariate}}:=p(X_{1},\\dots ,X_{N})=\\prod _{i=1}^{N}p(X_{i}|\\pi _{i}).}\n\n\nBivariate and multivariate distributions are usually represented as Probabilistic Graphical Models (graphs), in which edges denote statistical dependencies (or conditional probabilities) and vertices denote variables. To learn the structure of a PGM from data linkage-learning is employed.\nThe MIMIC[7] factorizes the joint probability distribution in a chain-like model representing successive dependencies between variables. It finds a permutation of the decision variables, \n\n\n\nr\n:\ni\n\u21a6\nj\n\n\n{\\displaystyle r:i\\mapsto j}\n\n, such that \n\n\n\n\nx\n\nr\n(\n1\n)\n\n\n\nx\n\nr\n(\n2\n)\n\n\n,\n\u2026\n,\n\nx\n\nr\n(\nN\n)\n\n\n\n\n{\\displaystyle x_{r(1)}x_{r(2)},\\dots ,x_{r(N)}}\n\n minimizes the Kullback-Leibler divergence in relation to the true probability distribution, i.e. \n\n\n\n\n\u03c0\n\nr\n(\ni\n+\n1\n)\n\n\n=\n{\n\nX\n\nr\n(\ni\n)\n\n\n}\n\n\n{\\displaystyle \\pi _{r(i+1)}=\\{X_{r(i)}\\}}\n\n. MIMIC models a distribution\n\n\n\n\n\np\n\nt\n+\n1\n\n\n(\n\nX\n\n1\n\n\n,\n\u2026\n,\n\nX\n\nN\n\n\n)\n=\n\np\n\nt\n\n\n(\n\nX\n\nr\n(\nN\n)\n\n\n)\n\n\u220f\n\ni\n=\n1\n\n\nN\n\u2212\n1\n\n\n\np\n\nt\n\n\n(\n\nX\n\nr\n(\ni\n)\n\n\n\n|\n\n\nX\n\nr\n(\ni\n+\n1\n)\n\n\n)\n.\n\n\n{\\displaystyle p_{t+1}(X_{1},\\dots ,X_{N})=p_{t}(X_{r(N)})\\prod _{i=1}^{N-1}p_{t}(X_{r(i)}|X_{r(i+1)}).}\n\n\nNew solutions are sampled from the leftmost to the rightmost variable, the first is generated independently and the others according to conditional probabilities. Since the estimated distribution must be recomputed each generation, MIMIC uses concrete populations in the following way\n\n\n\n\nP\n(\nt\n+\n1\n)\n=\n\n\u03b2\n\n\u03bc\n\n\n\u2218\n\n\u03b1\n\nMIMIC\n\n\n\u2218\nS\n(\nP\n(\nt\n)\n)\n.\n\n\n{\\displaystyle P(t+1)=\\beta _{\\mu }\\circ \\alpha _{\\text{MIMIC}}\\circ S(P(t)).}\n\n\nThe BMDA[8] factorizes the joint probability distribution in bivariate distributions. First, a randomly chosen variable is added as a node in a graph, the most dependent variable to one of those in the graph is chosen among those not yet in the graph, this procedure is repeated until no remaining variable depends on any variable in the graph (verified according to a threshold value).\nThe resulting model is a forest with multiple trees rooted at nodes \n\n\n\n\n\u03a5\n\nt\n\n\n\n\n{\\displaystyle \\Upsilon _{t}}\n\n. Considering \n\n\n\n\nI\n\nt\n\n\n\n\n{\\displaystyle I_{t}}\n\n the non-root variables, BMDA estimates a factorized distribution in which the root variables can be sampled independently, whereas all the others must be conditioned to the parent variable \n\n\n\n\n\u03c0\n\ni\n\n\n\n\n{\\displaystyle \\pi _{i}}\n\n.\n\n\n\n\n\np\n\nt\n+\n1\n\n\n(\n\nX\n\n1\n\n\n,\n\u2026\n,\n\nX\n\nN\n\n\n)\n=\n\n\u220f\n\n\nX\n\ni\n\n\n\u2208\n\n\u03a5\n\nt\n\n\n\n\n\np\n\nt\n\n\n(\n\nX\n\ni\n\n\n)\n\u22c5\n\n\u220f\n\n\nX\n\ni\n\n\n\u2208\n\nI\n\nt\n\n\n\n\n\np\n\nt\n\n\n(\n\nX\n\ni\n\n\n\n|\n\n\n\u03c0\n\ni\n\n\n)\n.\n\n\n{\\displaystyle p_{t+1}(X_{1},\\dots ,X_{N})=\\prod _{X_{i}\\in \\Upsilon _{t}}p_{t}(X_{i})\\cdot \\prod _{X_{i}\\in I_{t}}p_{t}(X_{i}|\\pi _{i}).}\n\n\nEach step of BMDA is defined as follows\n\n\n\n\nP\n(\nt\n+\n1\n)\n=\n\n\u03b2\n\n\u03bc\n\n\n\u2218\n\n\u03b1\n\nBMDA\n\n\n\u2218\nS\n(\nP\n(\nt\n)\n)\n.\n\n\n{\\displaystyle P(t+1)=\\beta _{\\mu }\\circ \\alpha _{\\text{BMDA}}\\circ S(P(t)).}\n\n\nThe next stage of EDAs development was the use of multivariate factorizations. In this case, the joint probability distribution is usually factorized in a number of components of limited size \n\n\n\n\n|\n\n\n\u03c0\n\ni\n\n\n\n|\n\n\u2264\nK\n,\n\u00a0\n\u2200\ni\n\u2208\n1\n,\n2\n,\n\u2026\n,\nN\n\n\n{\\displaystyle |\\pi _{i}|\\leq K,~\\forall i\\in 1,2,\\dots ,N}\n\n.\n\n\n\n\np\n(\n\nX\n\n1\n\n\n,\n\u2026\n,\n\nX\n\nN\n\n\n)\n=\n\n\u220f\n\ni\n=\n1\n\n\nN\n\n\np\n(\n\nX\n\ni\n\n\n\n|\n\n\n\u03c0\n\ni\n\n\n)\n\n\n{\\displaystyle p(X_{1},\\dots ,X_{N})=\\prod _{i=1}^{N}p(X_{i}|\\pi _{i})}\n\n\nThe learning of PGMs encoding multivariate distributions is a computationally expensive task, therefore, it is usual for EDAs to estimate multivariate statistics from bivariate statistics. Such relaxation allows PGM to be built in polynomial time in \n\n\n\nN\n\n\n{\\displaystyle N}\n\n; however, it also limits the generality of such EDAs.\nThe ECGA[9] was one of the first EDA to employ multivariate factorizations, in which high-order dependencies among decision variables can be modeled. Its approach factorizes the joint probability distribution in the product of multivariate marginal distributions. Assume \n\n\n\n\nT\n\neCGA\n\n\n=\n{\n\n\u03c4\n\n1\n\n\n,\n\u2026\n,\n\n\u03c4\n\n\u03a8\n\n\n}\n\n\n{\\displaystyle T_{\\text{eCGA}}=\\{\\tau _{1},\\dots ,\\tau _{\\Psi }\\}}\n\n is a set of subsets, in which every \n\n\n\n\u03c4\n\u2208\n\nT\n\neCGA\n\n\n\n\n{\\displaystyle \\tau \\in T_{\\text{eCGA}}}\n\n is a linkage set, containing \n\n\n\n\n|\n\n\u03c4\n\n|\n\n\u2264\nK\n\n\n{\\displaystyle |\\tau |\\leq K}\n\n variables. The factorized joint probability distribution is represented as follows\n\n\n\n\np\n(\n\nX\n\n1\n\n\n,\n\u2026\n,\n\nX\n\nN\n\n\n)\n=\n\n\u220f\n\n\u03c4\n\u2208\n\nT\n\neCGA\n\n\n\n\np\n(\n\u03c4\n)\n.\n\n\n{\\displaystyle p(X_{1},\\dots ,X_{N})=\\prod _{\\tau \\in T_{\\text{eCGA}}}p(\\tau ).}\n\n\nThe ECGA popularized the term linkage-learning as denoting procedures that identify linkage sets. Its linkage-learning procedure relies on two measures: (1) the Model Complexity (MC) and (2) the Compressed Population Complexity (CPC). The MC quantifies the model representation size in terms of number of bits required to store all the marginal probabilities\n\n\n\n\nM\nC\n=\n\nlog\n\n2\n\n\n\u2061\n(\n\u03bb\n+\n1\n)\n\n\u2211\n\n\u03c4\n\u2208\n\nT\n\neCGA\n\n\n\n\n(\n\n2\n\n\n|\n\n\u03c4\n\n|\n\n\u2212\n1\n\n\n)\n,\n\n\n{\\displaystyle MC=\\log _{2}(\\lambda +1)\\sum _{\\tau \\in T_{\\text{eCGA}}}(2^{|\\tau |-1}),}\n\n\nThe CPC, on the other hand, quantifies the data compression in terms of entropy of the marginal distribution over all partitions, where \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n is the selected population size, \n\n\n\n\n|\n\n\u03c4\n\n|\n\n\n\n{\\displaystyle |\\tau |}\n\n is the number of decision variables in the linkage set \n\n\n\n\u03c4\n\n\n{\\displaystyle \\tau }\n\n and \n\n\n\nH\n(\n\u03c4\n)\n\n\n{\\displaystyle H(\\tau )}\n\n is the joint entropy of the variables in \n\n\n\n\u03c4\n\n\n{\\displaystyle \\tau }\n\n\n\n\n\n\nC\nP\nC\n=\n\u03bb\n\n\u2211\n\n\u03c4\n\u2208\n\nT\n\neCGA\n\n\n\n\nH\n(\n\u03c4\n)\n.\n\n\n{\\displaystyle CPC=\\lambda \\sum _{\\tau \\in T_{\\text{eCGA}}}H(\\tau ).}\n\n\nThe linkage-learning in ECGA works as follows: (1) Insert each variable in a cluster, (2) compute CCC = MC + CPC of the current linkage sets, (3) verify the increase on CCC provided by joining pairs of clusters, (4) effectively joins those clusters with highest CCC improvement. This procedure is repeated until no CCC improvements are possible and produces a linkage model \n\n\n\n\nT\n\neCGA\n\n\n\n\n{\\displaystyle T_{\\text{eCGA}}}\n\n. The ECGA works with concrete populations, therefore, using the factorized distribution modeled by ECGA, it can be described as\n\n\n\n\nP\n(\nt\n+\n1\n)\n=\n\n\u03b2\n\n\u03bc\n\n\n\u2218\n\n\u03b1\n\neCGA\n\n\n\u2218\nS\n(\nP\n(\nt\n)\n)\n\n\n{\\displaystyle P(t+1)=\\beta _{\\mu }\\circ \\alpha _{\\text{eCGA}}\\circ S(P(t))}\n\n\nThe BOA[10][11][12] uses Bayesian networks to model and sample promising solutions. Bayesian networks are directed acyclic graphs, with nodes representing variables and edges representing conditional probabilities between pair of variables. The value of a variable \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n can be conditioned on a maximum of \n\n\n\nK\n\n\n{\\displaystyle K}\n\n other variables, defined in \n\n\n\n\n\u03c0\n\ni\n\n\n\n\n{\\displaystyle \\pi _{i}}\n\n. BOA builds a PGM encoding a factorized joint distribution, in which the parameters of the network, i.e. the conditional probabilities, are estimated from the selected population using the maximum likelihood estimator.\n\n\n\n\np\n(\n\nX\n\n1\n\n\n,\n\nX\n\n2\n\n\n,\n\u2026\n,\n\nX\n\nN\n\n\n)\n=\n\n\u220f\n\ni\n=\n1\n\n\nN\n\n\np\n(\n\nX\n\ni\n\n\n\n|\n\n\n\u03c0\n\ni\n\n\n)\n.\n\n\n{\\displaystyle p(X_{1},X_{2},\\dots ,X_{N})=\\prod _{i=1}^{N}p(X_{i}|\\pi _{i}).}\n\n\nThe Bayesian network structure, on the other hand, must be built iteratively (linkage-learning). It starts with a network without edges and, at each step, adds the edge which better improves some scoring metric (e.g. Bayesian information criterion (BIC) or Bayesian-Dirichlet metric with likelihood equivalence (BDe)).[13] The scoring metric evaluates the network structure according to its accuracy in modeling the selected population. From the built network, BOA samples new promising solutions as follows: (1) it computes the ancestral ordering for each variable, each node being preceded by its parents; (2) each variable is sampled conditionally to its parents. Given such scenario, every BOA step can be defined as\n\n\n\n\nP\n(\nt\n+\n1\n)\n=\n\n\u03b2\n\n\u03bc\n\n\n\u2218\n\n\u03b1\n\nBOA\n\n\n\u2218\nS\n(\nP\n(\nt\n)\n)\n\n\n{\\displaystyle P(t+1)=\\beta _{\\mu }\\circ \\alpha _{\\text{BOA}}\\circ S(P(t))}\n\n\nThe LTGA[14] differs from most EDA in the sense it does not explicitly model a probabilisty distribution but only a linkage model, called linkage-tree. A linkage \n\n\n\nT\n\n\n{\\displaystyle T}\n\n is a set of linkage sets with no probability distribution associated, therefore, there is no way to sample new solutions directly from \n\n\n\nT\n\n\n{\\displaystyle T}\n\n. The linkage model is a linkage-tree produced stored as a Family of sets (FOS).\n\n\n\n\n\nT\n\nLT\n\n\n=\n{\n{\n\nx\n\n1\n\n\n}\n,\n{\n\nx\n\n2\n\n\n}\n,\n{\n\nx\n\n3\n\n\n}\n,\n{\n\nx\n\n4\n\n\n}\n,\n{\n\nx\n\n1\n\n\n,\n\nx\n\n2\n\n\n}\n,\n{\n\nx\n\n3\n\n\n,\n\nx\n\n4\n\n\n}\n}\n.\n\n\n{\\displaystyle T_{\\text{LT}}=\\{\\{x_{1}\\},\\{x_{2}\\},\\{x_{3}\\},\\{x_{4}\\},\\{x_{1},x_{2}\\},\\{x_{3},x_{4}\\}\\}.}\n\n\nThe linkage-tree learning procedure is a hierarchical clustering algorithm, which work as follows. At each step the two closest clusters \n\n\n\ni\n\n\n{\\displaystyle i}\n\n and \n\n\n\nj\n\n\n{\\displaystyle j}\n\n are merged, this procedure repeats until only one cluster remains, each subtree is stored as a subset \n\n\n\n\u03c4\n\u2208\n\nT\n\nLT\n\n\n\n\n{\\displaystyle \\tau \\in T_{\\text{LT}}}\n\n.\nThe LTGA uses \n\n\n\n\nT\n\nLT\n\n\n\n\n{\\displaystyle T_{\\text{LT}}}\n\n to guide an \"optimal mixing\" procedure which resembles a recombination operator but only accepts improving moves. We denote it as \n\n\n\n\nR\n\nLTGA\n\n\n\n\n{\\displaystyle R_{\\text{LTGA}}}\n\n, where the notation \n\n\n\nx\n[\n\u03c4\n]\n\u2190\ny\n[\n\u03c4\n]\n\n\n{\\displaystyle x[\\tau ]\\gets y[\\tau ]}\n\n indicates the transfer of the genetic material indexed by \n\n\n\n\u03c4\n\n\n{\\displaystyle \\tau }\n\n from \n\n\n\ny\n\n\n{\\displaystyle y}\n\n to \n\n\n\nx\n\n\n{\\displaystyle x}\n\n.\nThe LTGA does not implement typical selection operators, instead, selection is performed during recombination. Similar ideas have been usually applied into local-search heuristics and, in this sense, the LTGA can be seen as an hybrid method. In summary, one step of the LTGA is defined as\n\n\n\n\nP\n(\nt\n+\n1\n)\n=\n\nR\n\nLTGA\n\n\n(\nP\n(\nt\n)\n)\n\u2218\n\n\u03b1\n\nLTGA\n\n\n(\nP\n(\nt\n)\n)\n\n\n{\\displaystyle P(t+1)=R_{\\text{LTGA}}(P(t))\\circ \\alpha _{\\text{LTGA}}(P(t))}\n\n", 
    "dbpedia_url": "http://dbpedia.org/resource/Estimation_of_distribution_algorithm", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Estimation_of_distribution_algorithm\n"
}