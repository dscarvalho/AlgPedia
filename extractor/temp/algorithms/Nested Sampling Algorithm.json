{
    "about": "The nested sampling algorithm is a computational approach to the problem of comparing models in Bayesian statistics, developed in 2004 by physicist John Skilling.[1]", 
    "name": "Nested Sampling Algorithm", 
    "classification": "Statistical Algorithms", 
    "full_text": "The nested sampling algorithm is a computational approach to the problem of comparing models in Bayesian statistics, developed in 2004 by physicist John Skilling.[1]\n\n\nBayes' theorem can be applied to a pair of competing models \n\n\n\nM\n1\n\n\n{\\displaystyle M1}\n\n and \n\n\n\nM\n2\n\n\n{\\displaystyle M2}\n\n for data \n\n\n\nD\n\n\n{\\displaystyle D}\n\n, one of which may be true (though which one is unknown) but which both cannot be true simultaneously. The posterior probability for \n\n\n\nM\n1\n\n\n{\\displaystyle M1}\n\n may be calculated as:\nGiven no a priori information in favor of \n\n\n\nM\n1\n\n\n{\\displaystyle M1}\n\n or \n\n\n\nM\n2\n\n\n{\\displaystyle M2}\n\n, it is reasonable to assign prior probabilities \n\n\n\nP\n(\nM\n1\n)\n=\nP\n(\nM\n2\n)\n=\n1\n\n/\n\n2\n\n\n{\\displaystyle P(M1)=P(M2)=1/2}\n\n, so that \n\n\n\nP\n(\nM\n2\n)\n\n/\n\nP\n(\nM\n1\n)\n=\n1\n\n\n{\\displaystyle P(M2)/P(M1)=1}\n\n. The remaining Bayes factor\n\n\n\nP\n(\nD\n\n|\n\nM\n2\n)\n\n/\n\nP\n(\nD\n\n|\n\nM\n1\n)\n\n\n{\\displaystyle P(D|M2)/P(D|M1)}\n\n is not so easy to evaluate, since in general it requires marginalizing nuisance parameters. Generally, \n\n\n\nM\n1\n\n\n{\\displaystyle M1}\n\n has a set of parameters that can be grouped together and called \n\n\n\n\u03b8\n\n\n{\\displaystyle \\theta }\n\n, and \n\n\n\nM\n2\n\n\n{\\displaystyle M2}\n\n has its own vector of parameters that may be of different dimensionality, but is still termed \n\n\n\n\u03b8\n\n\n{\\displaystyle \\theta }\n\n. The marginalization for \n\n\n\nM\n1\n\n\n{\\displaystyle M1}\n\n is\nand likewise for \n\n\n\nM\n2\n\n\n{\\displaystyle M2}\n\n. This integral is often analytically intractable, and in these cases it is necessary to employ a numerical algorithm to find an approximation. The nested sampling algorithm was developed by John Skilling specifically to approximate these marginalization integrals, and it has the added benefit of generating samples from the posterior distribution \n\n\n\nP\n(\n\u03b8\n\n|\n\nD\n,\nM\n1\n)\n\n\n{\\displaystyle P(\\theta |D,M1)}\n\n.[2] It is an alternative to methods from the Bayesian literature[3] such as bridge sampling and defensive importance sampling.\nHere is a simple version of the nested sampling algorithm, followed by a description of how it computes the marginal probability density \n\n\n\nZ\n=\nP\n(\nD\n\n|\n\nM\n)\n\n\n{\\displaystyle Z=P(D|M)}\n\n where \n\n\n\nM\n\n\n{\\displaystyle M}\n\n is \n\n\n\nM\n1\n\n\n{\\displaystyle M1}\n\n or \n\n\n\nM\n2\n\n\n{\\displaystyle M2}\n\n:\nAt each iteration, \n\n\n\n\nX\n\ni\n\n\n\n\n{\\displaystyle X_{i}}\n\n is an estimate of the amount of prior mass covered by the hypervolume in parameter space of all points with likelihood greater than \n\n\n\n\n\u03b8\n\ni\n\n\n\n\n{\\displaystyle \\theta _{i}}\n\n. The weight factor \n\n\n\n\nw\n\ni\n\n\n\n\n{\\displaystyle w_{i}}\n\n is an estimate of the amount of prior mass that lies between two nested hypersurfaces \n\n\n\n{\n\u03b8\n\n|\n\nP\n(\nD\n\n|\n\n\u03b8\n,\nM\n)\n=\nP\n(\nD\n\n|\n\n\n\u03b8\n\ni\n\u2212\n1\n\n\n,\nM\n)\n}\n\n\n{\\displaystyle \\{\\theta |P(D|\\theta ,M)=P(D|\\theta _{i-1},M)\\}}\n\n and \n\n\n\n{\n\u03b8\n\n|\n\nP\n(\nD\n\n|\n\n\u03b8\n,\nM\n)\n=\nP\n(\nD\n\n|\n\n\n\u03b8\n\ni\n\n\n,\nM\n)\n}\n\n\n{\\displaystyle \\{\\theta |P(D|\\theta ,M)=P(D|\\theta _{i},M)\\}}\n\n. The update step \n\n\n\nZ\n:=\nZ\n+\n\nL\n\ni\n\n\n\u2217\n\nw\n\ni\n\n\n\n\n{\\displaystyle Z:=Z+L_{i}*w_{i}}\n\n computes the sum over \n\n\n\ni\n\n\n{\\displaystyle i}\n\n of \n\n\n\n\nL\n\ni\n\n\n\u2217\n\nw\n\ni\n\n\n\n\n{\\displaystyle L_{i}*w_{i}}\n\n to numerically approximate the integral\nThe idea is to subdivide the range of \n\n\n\nf\n(\n\u03b8\n)\n=\nP\n(\nD\n\n|\n\n\u03b8\n,\nM\n)\n\n\n{\\displaystyle f(\\theta )=P(D|\\theta ,M)}\n\n and estimate, for each interval \n\n\n\n[\nf\n(\n\n\u03b8\n\ni\n\u2212\n1\n\n\n)\n,\nf\n(\n\n\u03b8\n\ni\n\n\n)\n]\n\n\n{\\displaystyle [f(\\theta _{i-1}),f(\\theta _{i})]}\n\n, how likely it is a priori that a randomly chosen \n\n\n\n\u03b8\n\n\n{\\displaystyle \\theta }\n\n would map to this interval. This can be thought of as a Bayesian's way to numerically implement Lebesgue integration.[4]\nExample implementations demonstrating the nested sampling algorithm are publicly available for download, written in several programming languages.\nSince nested sampling was proposed in 2004, it has been used in many aspects of the field of astronomy. One paper suggested using nested sampling for cosmological model selection and object detection, as it \"uniquely combines accuracy, general applicability and computational feasibility.\"[11] A refinement of the algorithm to handle multimodal posteriors has been suggested as a means to detect astronomical objects in extant datasets.[12] Other applications of nested sampling are in the field of finite element updating where the algorithm is used to choose an optimal finite element model, and this was applied to structural dynamics.[13]", 
    "dbpedia_url": "http://dbpedia.org/resource/Nested_sampling_algorithm", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Nested_sampling_algorithm\n"
}