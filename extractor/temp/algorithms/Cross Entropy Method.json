{
    "about": "The cross-entropy (CE) method attributed to Reuven Rubinstein is a general Monte Carlo approach to combinatorial and continuous multi-extremal optimization and importance sampling. The method originated from the field of rare event simulation, where very small probabilities need to be accurately estimated, for example in network reliability analysis, queueing models, or performance analysis of telecommunication systems. The CE method can be applied to static and noisy combinatorial optimization problems such as the traveling salesman problem, the quadratic assignment problem, DNA sequence alignment, the max-cut problem and the buffer allocation problem, as well as continuous global optimization problems with many local extrema.", 
    "classification": "Optimization Algorithms And Methods", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Cross-entropy_method\n", 
    "full_text": "The cross-entropy (CE) method attributed to Reuven Rubinstein is a general Monte Carlo approach to combinatorial and continuous multi-extremal optimization and importance sampling. The method originated from the field of rare event simulation, where very small probabilities need to be accurately estimated, for example in network reliability analysis, queueing models, or performance analysis of telecommunication systems. The CE method can be applied to static and noisy combinatorial optimization problems such as the traveling salesman problem, the quadratic assignment problem, DNA sequence alignment, the max-cut problem and the buffer allocation problem, as well as continuous global optimization problems with many local extrema.\nIn a nutshell the CE method consists of two phases:\n\n\nConsider the general problem of estimating the quantity \n\n\n\n\u2113\n=\n\n\nE\n\n\n\nu\n\n\n\n[\nH\n(\n\nX\n\n)\n]\n=\n\u222b\nH\n(\n\nx\n\n)\n\nf\n(\n\nx\n\n;\n\nu\n\n)\n\n\n\nd\n\n\n\nx\n\n\n\n{\\displaystyle \\ell =\\mathbb {E} _{\\mathbf {u} }[H(\\mathbf {X} )]=\\int H(\\mathbf {x} )\\,f(\\mathbf {x} ;\\mathbf {u} )\\,{\\textrm {d}}\\mathbf {x} }\n\n, where \n\n\n\nH\n\n\n{\\displaystyle H}\n\n is some performance function and \n\n\n\nf\n(\n\nx\n\n;\n\nu\n\n)\n\n\n{\\displaystyle f(\\mathbf {x} ;\\mathbf {u} )}\n\n is a member of some parametric family of distributions. Using importance sampling this quantity can be estimated as \n\n\n\n\n\n\n\u2113\n^\n\n\n\n=\n\n\n1\nN\n\n\n\n\u2211\n\ni\n=\n1\n\n\nN\n\n\nH\n(\n\n\nX\n\n\ni\n\n\n)\n\n\n\nf\n(\n\n\nX\n\n\ni\n\n\n;\n\nu\n\n)\n\n\ng\n(\n\n\nX\n\n\ni\n\n\n)\n\n\n\n\n\n{\\displaystyle {\\hat {\\ell }}={\\frac {1}{N}}\\sum _{i=1}^{N}H(\\mathbf {X} _{i}){\\frac {f(\\mathbf {X} _{i};\\mathbf {u} )}{g(\\mathbf {X} _{i})}}}\n\n, where \n\n\n\n\n\nX\n\n\n1\n\n\n,\n\u2026\n,\n\n\nX\n\n\nN\n\n\n\n\n{\\displaystyle \\mathbf {X} _{1},\\dots ,\\mathbf {X} _{N}}\n\n is a random sample from \n\n\n\ng\n\n\n\n{\\displaystyle g\\,}\n\n. For positive \n\n\n\nH\n\n\n{\\displaystyle H}\n\n, the theoretically optimal importance sampling density (pdf) is given by \n\n\n\n\ng\n\n\u2217\n\n\n(\n\nx\n\n)\n=\nH\n(\n\nx\n\n)\nf\n(\n\nx\n\n;\n\nu\n\n)\n\n/\n\n\u2113\n\n\n{\\displaystyle g^{*}(\\mathbf {x} )=H(\\mathbf {x} )f(\\mathbf {x} ;\\mathbf {u} )/\\ell }\n\n. This, however, depends on the unknown \n\n\n\n\u2113\n\n\n{\\displaystyle \\ell }\n\n. The CE method aims to approximate the optimal PDF by adaptively selecting members of the parametric family that are closest (in the Kullback\u2013Leibler sense) to the optimal PDF \n\n\n\n\ng\n\n\u2217\n\n\n\n\n{\\displaystyle g^{*}}\n\n.\nIn several cases, the solution to step 3 can be found analytically. Situations in which this occurs are\nThe same CE algorithm can be used for optimization, rather than estimation. Suppose the problem is to maximize some function \n\n\n\nS\n(\nx\n)\n\n\n{\\displaystyle S(x)}\n\n, for example, \n\n\n\nS\n(\nx\n)\n=\n\n\n\ne\n\n\n\n\u2212\n(\nx\n\u2212\n2\n\n)\n\n2\n\n\n\n\n+\n0.8\n\n\n\n\ne\n\n\n\n\u2212\n(\nx\n+\n2\n\n)\n\n2\n\n\n\n\n\n\n{\\displaystyle S(x)={\\textrm {e}}^{-(x-2)^{2}}+0.8\\,{\\textrm {e}}^{-(x+2)^{2}}}\n\n. To apply CE, one considers first the associated stochastic problem of estimating \n\n\n\n\n\nP\n\n\n\u03b8\n\n\n(\nS\n(\nX\n)\n\u2265\n\u03b3\n)\n\n\n{\\displaystyle \\mathbb {P} _{\\boldsymbol {\\theta }}(S(X)\\geq \\gamma )}\n\n for a given level \n\n\n\n\u03b3\n\n\n\n{\\displaystyle \\gamma \\,}\n\n, and parametric family \n\n\n\n\n{\nf\n(\n\u22c5\n;\n\n\u03b8\n\n)\n}\n\n\n\n{\\displaystyle \\left\\{f(\\cdot ;{\\boldsymbol {\\theta }})\\right\\}}\n\n, for example the 1-dimensional Gaussian distribution, parameterized by its mean \n\n\n\n\n\u03bc\n\nt\n\n\n\n\n\n{\\displaystyle \\mu _{t}\\,}\n\n and variance \n\n\n\n\n\u03c3\n\nt\n\n\n2\n\n\n\n\n{\\displaystyle \\sigma _{t}^{2}}\n\n (so \n\n\n\n\n\u03b8\n\n=\n(\n\u03bc\n,\n\n\u03c3\n\n2\n\n\n)\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}=(\\mu ,\\sigma ^{2})}\n\n here). Hence, for a given \n\n\n\n\u03b3\n\n\n\n{\\displaystyle \\gamma \\,}\n\n, the goal is to find \n\n\n\n\n\u03b8\n\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}}\n\n so that \n\n\n\n\nD\n\n\nK\nL\n\n\n\n(\n\n\n\nI\n\n\n\n{\nS\n(\nx\n)\n\u2265\n\u03b3\n}\n\n\n\u2225\n\nf\n\n\u03b8\n\n\n)\n\n\n{\\displaystyle D_{\\mathrm {KL} }({\\textrm {I}}_{\\{S(x)\\geq \\gamma \\}}\\|f_{\\boldsymbol {\\theta }})}\n\n is minimized. This is done by solving the sample version (stochastic counterpart) of the KL divergence minimization problem, as in step 3 above. It turns out that parameters that minimize the stochastic counterpart for this choice of target distribution and parametric family are the sample mean and sample variance corresponding to the elite samples, which are those samples that have objective function value \n\n\n\n\u2265\n\u03b3\n\n\n{\\displaystyle \\geq \\gamma }\n\n. The worst of the elite samples is then used as the level parameter for the next iteration. This yields the following randomized algorithm that happens to coincide with the so-called Estimation of Multivariate Normal Algorithm (EMNA), an estimation of distribution algorithm.", 
    "name": "Cross Entropy Method"
}