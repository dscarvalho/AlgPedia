{
    "about": "In probability and statistics, given two stochastic processes \n\n\n\nX\n=\n(\n\nX\n\nt\n\n\n)\n\n\n{\\displaystyle X=(X_{t})}\n\n and \n\n\n\nY\n=\n(\n\nY\n\nt\n\n\n)\n\n\n{\\displaystyle Y=(Y_{t})}\n\n, the cross-covariance is a function that gives the covariance of one process with the other at pairs of time points. With the usual notation E\u00a0 for the expectation operator, if the processes have the mean functions \n\n\n\n\n\u03bc\n\nt\n\n\n=\nE\n[\n\nX\n\nt\n\n\n]\n\n\n{\\displaystyle \\mu _{t}=E[X_{t}]}\n\n and \n\n\n\n\n\u03bd\n\nt\n\n\n=\nE\n[\n\nY\n\nt\n\n\n]\n\n\n{\\displaystyle \\nu _{t}=E[Y_{t}]}\n\n, then the cross-covariance is given by", 
    "classification": "Signal Processing", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Cross-covariance\n", 
    "full_text": "In probability and statistics, given two stochastic processes \n\n\n\nX\n=\n(\n\nX\n\nt\n\n\n)\n\n\n{\\displaystyle X=(X_{t})}\n\n and \n\n\n\nY\n=\n(\n\nY\n\nt\n\n\n)\n\n\n{\\displaystyle Y=(Y_{t})}\n\n, the cross-covariance is a function that gives the covariance of one process with the other at pairs of time points. With the usual notation E\u00a0 for the expectation operator, if the processes have the mean functions \n\n\n\n\n\u03bc\n\nt\n\n\n=\nE\n[\n\nX\n\nt\n\n\n]\n\n\n{\\displaystyle \\mu _{t}=E[X_{t}]}\n\n and \n\n\n\n\n\u03bd\n\nt\n\n\n=\nE\n[\n\nY\n\nt\n\n\n]\n\n\n{\\displaystyle \\nu _{t}=E[Y_{t}]}\n\n, then the cross-covariance is given by\nCross-covariance is related to the more commonly used cross-correlation of the processes in question.\nIn the case of two random vectors \n\n\n\nX\n=\n(\n\nX\n\n1\n\n\n,\n\nX\n\n2\n\n\n,\n.\n.\n.\n,\n\nX\n\np\n\n\n)\n\n\n{\\displaystyle X=(X_{1},X_{2},...,X_{p})}\n\n and \n\n\n\nY\n=\n(\n\nY\n\n1\n\n\n,\n\nY\n\n2\n\n\n,\n.\n.\n.\n,\n\nY\n\nq\n\n\n)\n\n\n{\\displaystyle Y=(Y_{1},Y_{2},...,Y_{q})}\n\n, the cross-covariance would be a p by q matrix \n\n\n\n\nC\n\nX\nY\n\n\n\n\n{\\displaystyle C_{XY}}\n\n (often denoted \n\n\n\ncov\n\u2061\n(\nX\n,\nY\n)\n\n\n{\\displaystyle \\operatorname {cov} (X,Y)}\n\n) with entries \n\n\n\n\nC\n\nX\nY\n\n\n(\nj\n,\nk\n)\n=\n\ncov\n\n(\n\nX\n\nj\n\n\n,\n\nY\n\nk\n\n\n)\n.\n\n\n\n{\\displaystyle C_{XY}(j,k)={\\text{cov}}(X_{j},Y_{k}).\\,}\n\n Thus the term cross-covariance is used in order to distinguish this concept from the covariance of a random vector X, which is understood to be the matrix of covariances between the scalar components of X itself.\nIn signal processing, the cross-covariance is often called cross-correlation and is a measure of similarity of two signals, commonly used to find features in an unknown signal by comparing it to a known one. It is a function of the relative time between the signals, is sometimes called the sliding dot product, and has applications in pattern recognition and cryptanalysis.\n\n\nFor random vectors X and Y, each containing random elements whose expected value and variance exist, the cross-covariance matrix of X and Y is defined by\nwhere \u03bcX and \u03bcY are vectors containing the expected values of X and Y. The vectors X and Y need not have the same dimension, and either might be a scalar value. Any element of the cross-covariance matrix is itself a \"cross-covariance\".\nFor example, if X=(X1, X2, X3) and Y=(Y1, Y2) are random vectors, then cov(X, Y) is a 3 x 2 matrix whose ij-th entry is cov(Xi, Yj).\nFor the cross-covariance matrix, the following basic properties apply:[1]\nwhere \n\n\n\nX\n,\n\nX\n\n1\n\n\n\n\n{\\displaystyle X,X_{1}}\n\n and \n\n\n\n\nX\n\n2\n\n\n\n\n{\\displaystyle X_{2}}\n\n are random p\u00d71 vectors, \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n is a random q\u00d71 vector, \n\n\n\na\n\n\n{\\displaystyle a}\n\n is a q\u00d71 vector, \n\n\n\nb\n\n\n{\\displaystyle b}\n\n is a p\u00d71 vector, \n\n\n\nA\n\n\n{\\displaystyle A}\n\n and \n\n\n\nB\n\n\n{\\displaystyle B}\n\n are q\u00d7p matrices of constants, and \n\n\n\n0\n\n\n{\\displaystyle 0}\n\n is a p\u00d7q matrix of zeroes.\nThe cross-covariance is also relevant in signal processing where the cross-covariance between two wide-sense stationary random processes can be estimated by averaging the product of samples measured from one process and samples measured from the other (and its time shifts). The samples included in the average can be an arbitrary subset of all the samples in the signal (e.g., samples within a finite time window or a sub-sampling of one of the signals). For a large number of samples, the average converges to the true covariance.\nCross-covariance may also refer to a \"deterministic\" cross-covariance between two signals. This consists of summing over all time indices. For example, for discrete signals fi and gi the cross-covariance is defined as\nwhere the asterisk indicates that the complex conjugate is taken when the signals are complex-valued.\nFor continuous functions f\u00a0(x) and g\u00a0(x) the (deterministic) cross-covariance is defined as\nThe cross-covariance of two signals is related to the convolution by:", 
    "name": "Cross Covariance"
}