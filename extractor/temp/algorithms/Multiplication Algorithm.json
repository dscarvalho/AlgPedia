{
    "about": "A multiplication algorithm is an algorithm (or method) to multiply two numbers. Depending on the size of the numbers, different algorithms are in use. Efficient multiplication algorithms have existed since the advent of the decimal system.", 
    "name": "Multiplication Algorithm", 
    "classification": "Computer Arithmetic Algorithms", 
    "full_text": "A multiplication algorithm is an algorithm (or method) to multiply two numbers. Depending on the size of the numbers, different algorithms are in use. Efficient multiplication algorithms have existed since the advent of the decimal system.\n\n\nThe grid method (or box method) is an introductory method for multiple-digit multiplication that is often taught to pupils at primary school or elementary school level. It has been a standard part of the national primary-school mathematics curriculum in England and Wales since the late 1990s.[1]\nBoth factors are broken up (\"partitioned\") into their hundreds, tens and units parts, and the products of the parts are then calculated explicitly in a relatively simple multiplication-only stage, before these contributions are then totalled to give the final answer in a separate addition stage.\nThe calculation 34 \u00d7 13, for example, could be computed using the grid:\nfollowed by addition to obtain 442, either in a single sum (see right), or through forming the row-by-row totals (300 + 40) + (90 + 12) = 340 + 102 = 442.\nThis calculation approach (though not necessarily with the explicit grid arrangement) is also known as the partial products algorithm. Its essence is the calculation of the simple multiplications separately, with all addition being left to the final gathering-up stage.\nThe grid method can in principle be applied to factors of any size, although the number of sub-products becomes cumbersome as the number of digits increases. Nevertheless, it is seen as a usefully explicit method to introduce the idea of multiple-digit multiplications; and, in an age when most multiplication calculations are done using a calculator or a spreadsheet, it may in practice be the only multiplication algorithm that some students will ever need.\nIf a positional numeral system is used, a natural way of multiplying numbers is taught in schools as long multiplication, sometimes called grade-school multiplication, sometimes called Standard Algorithm: multiply the multiplicand by each digit of the multiplier and then add up all the properly shifted results. It requires memorization of the multiplication table for single digits.\nThis is the usual algorithm for multiplying larger numbers by hand in base 10. Computers initially used a very similar shift and add algorithm in base 2, but modern processors have optimized circuitry for fast multiplications using more efficient algorithms, at the price of a more complex hardware realization. A person doing long multiplication on paper will write down all the products and then add them together; an abacus-user will sum the products as soon as each one is computed.\nThis example uses long multiplication to multiply 23,958,233 (multiplicand) by 5,830 (multiplier) and arrives at 139,676,498,390 for the result (product).\nBelow pseudocode describes the process of above multiplication. It keeps only one row to maintain sum which finally becomes the result. Note that the '+=' operator is used to denote sum to existing value and store operation (akin to languages such as Java and C) for compactness.\nLet n be the total number of digits in the two input numbers in base D. If the result must be kept in memory then the space complexity is trivially \u0398(n). However, in certain applications, the entire result need not be kept in memory and instead the digits of the result can be streamed out as they are computed (for example, to system console or file). In these scenarios, long multiplication has the advantage that it can easily be formulated as a log space algorithm; that is, an algorithm that only needs working space proportional to the logarithm of the number of digits in the input (\u0398(log\u00a0n)). This is the double logarithm of the numbers being multiplied themselves (log\u00a0log\u00a0N). Note that operands themselves still need to be kept in memory and their \u0398(n) space is not considered in this analysis.\nThe method is based on the observation that each digit of the result can be computed from right to left with only knowing the carry from the previous step. Let ai and bi be the i-th digit of the operand, ri be the i-th digit of the result and ci be the carry generated for ri (i=1 is the right most digit) then\nA simple inductive argument shows that the carry can never exceed n and the total sum for ri can never exceed D * n: the carry into the first column is zero, and for all other columns, there are at most n digits in the column, and a carry of at most n from the previous column (by the induction hypothesis). The sum is at most D * n, and the carry to the next column is at most D * n / D, or n. Thus both these values can be stored in O(log n) digits.\nIn pseudocode, the log-space algorithm is:\nSome chips implement this algorithm for various integer and floating-point sizes in computer hardware or in microcode. In arbitrary-precision arithmetic, it's common to use long multiplication with the base set to 2w, where w is the number of bits in a word, for multiplying relatively small numbers.\nTo multiply two numbers with n digits using this method, one needs about n2 operations. More formally: using a natural size metric of number of digits, the time complexity of multiplying two n-digit numbers using long multiplication is \u0398(n2).\nWhen implemented in software, long multiplication algorithms have to deal with overflow during additions, which can be expensive. For this reason, a typical approach is to represent the number in a small base b such that, for example, 8b is a representable machine integer (for example Richard Brent used this approach in his Fortran package MP[2]); we can then perform several additions before having to deal with overflow. When the number becomes too large, we add part of it to the result or carry and map the remaining part back to a number less than b; this process is called normalization.\nLattice, or sieve, multiplication is algorithmically equivalent to long multiplication. It requires the preparation of a lattice (a grid drawn on paper) which guides the calculation and separates all the multiplications from the additions. It was introduced to Europe in 1202 in Fibonacci's Liber Abaci. Leonardo described the operation as mental, using his right and left hands to carry the intermediate calculations. Matrak\u00e7\u0131 Nasuh presented 6 different variants of this method in this 16th-century book, Umdet-ul Hisab. It was widely used in Enderun schools across the Ottoman Empire.[3] Napier's bones, or Napier's rods also used this method, as published by Napier in 1617, the year of his death.\nAs shown in the example, the multiplicand and multiplier are written above and to the right of a lattice, or a sieve. It is found in Muhammad ibn Musa al-Khwarizmi's \"Arithmetic\", one of Leonardo's sources mentioned by Sigler, author of \"Fibonacci's Liber Abaci\", 2002.[citation needed]\nThe pictures on the right show how to calculate 345 \u00d7 12 using lattice multiplication. As a more complicated example, consider the picture below displaying the computation of 23,958,233 multiplied by 5,830 (multiplier); the result is 139,676,498,390. Notice 23,958,233 is along the top of the lattice and 5,830 is along the right side. The products fill the lattice and the sum of those products (on the diagonal) are along the left and bottom sides. Then those sums are totaled as shown.\nIn base 2, long multiplication reduces to a nearly trivial operation. For each '1' bit in the multiplier, shift the multiplicand an appropriate amount and then sum the shifted values. Depending on computer processor architecture and choice of multiplier, it may be faster to code this algorithm using hardware bit shifts and adds rather than depend on multiplication instructions, when the multiplier is fixed and the number of adds required is small.\nThis algorithm is also known as Peasant multiplication, because it has been widely used among those who are unschooled and thus have not memorized the multiplication tables required by long multiplication. The algorithm was also in use in ancient Egypt.\nOn paper, write down in one column the numbers you get when you repeatedly halve the multiplier, ignoring the remainder; in a column beside it repeatedly double the multiplicand. Cross out each row in which the last digit of the first number is even, and add the remaining numbers in the second column to obtain the product.\nThe main advantages of this method are that it can be taught quickly, no memorization is required, and it can be performed using tokens such as poker chips if paper and pencil are not available. It does however take more steps than long multiplication so it can be unwieldy when large numbers are involved.\nThis example uses peasant multiplication to multiply 11 by 3 to arrive at a result of 33.\nDescribing the steps explicitly:\nThe method works because multiplication is distributive, so:\nA more complicated example, using the figures from the earlier examples (23,958,233 and 5,830):\nHistorically, computers used a \"shift and add\" algorithm to multiply small integers. Both base 2 long multiplication and base 2 peasant multiplication reduce to this same algorithm. In base 2, multiplying by the single digit of the multiplier reduces to a simple series of logical AND operations. Each partial product is added to a running sum as soon as each partial product is computed. Most currently available microprocessors implement this or other similar algorithms (such as Booth encoding) for various integer and floating-point sizes in hardware multipliers or in microcode.\nOn currently available processors, a bit-wise shift instruction is faster than a multiply instruction and can be used to multiply (shift left) and divide (shift right) by powers of two. Multiplication by a constant and division by a constant can be implemented using a sequence of shifts and adds or subtracts. For example, there are several ways to multiply by 10 using only bit-shift and addition.\nIn some cases such sequences of shifts and adds or subtracts will outperform hardware multipliers and especially dividers. A division by a number of the form \n\n\n\n\n2\n\nn\n\n\n\n\n{\\displaystyle 2^{n}}\n\n or \n\n\n\n\n2\n\nn\n\n\n\u00b1\n1\n\n\n{\\displaystyle 2^{n}\\pm 1}\n\n often can be converted to such a short sequence.\nThese types of sequences have to always be used for computers that do not have a \"multiply\" instruction,[4] and can also be used by extension to floating point numbers if one replaces the shifts with computation of 2*x as x+x, as these are logically equivalent.\nTwo quantities can be multiplied using quarter squares by employing the following identity involving the floor function that some sources[5][6] attribute to Babylonian mathematics (2000\u20131600 BC).\nIf one of x+y and x-y is odd, the other is odd too; this means that the fractions, if any, will cancel out, and discarding the remainders does not introduce any error. Below is a lookup table of quarter squares with the remainder discarded for the digits 0 through 18; this allows for the multiplication of numbers up to 9\u00d79.\nIf, for example, you wanted to multiply 9 by 3, you observe that the sum and difference are 12 and 6 respectively. Looking both those values up on the table yields 36 and 9, the difference of which is 27, which is the product of 9 and 3.\nAntoine Voisin published a table of quarter squares from 1 to 1000 in 1817 as an aid in multiplication. A larger table of quarter squares from 1 to 100000 was published by Samuel Laundy in 1856,[7] and a table from 1 to 200000 by Joseph Blater in 1888.[8]\nQuarter square multipliers were used in analog computers to form an analog signal that was the product of two analog input signals. In this application, the sum and difference of two input voltages are formed using operational amplifiers. The square of each of these is approximated using piecewise linear circuits. Finally the difference of the two squares is formed and scaled by a factor of one fourth using yet another operational amplifier.\nIn 1980, Everett L. Johnson proposed using the quarter square method in a digital multiplier.[9] To form the product of two 8-bit integers, for example, the digital device forms the sum and difference, looks both quantities up in a table of squares, takes the difference of the results, and divides by four by shifting two bits to the right. For 8-bit integers the table of quarter squares will have 29-1=511 entries (one entry for the full range 0..510 of possible sums, the differences using only the first 256 entries in range 0..255) or 29-1=511 entries (using for negative differences the technique of 2-complements and 9-bit masking, which avoids testing the sign of differences), each entry being 16-bit wide (the entry values are from (0\u00b2/4)=0 to (510\u00b2/4)=65025).\nThe Quarter square multiplier technique has also benefitted 8-bit systems that do not have any support for a hardware multiplier. Steven Judd implemented this for the 6502.[10]\nSuppose we want to multiply two numbers \n\n\n\nx\n\n\n{\\displaystyle x}\n\n and \n\n\n\ny\n\n\n{\\displaystyle y}\n\n that are close to a round number \n\n\n\nN\n\n\n{\\displaystyle N}\n\n. Writing \n\n\n\nx\n=\nN\n+\n\nx\n\u2032\n\n\n\n{\\displaystyle x=N+x'}\n\n and \n\n\n\ny\n=\nN\n+\n\ny\n\u2032\n\n\n\n{\\displaystyle y=N+y'}\n\n, allows one to express the product as:\nExample. Suppose we want to multiply 92 by 87. We can then take \n\n\n\nN\n=\n100\n\n\n{\\displaystyle N=100}\n\n and implement the above formula as follows. We write the numbers below each other and next to them the amounts we have to add to get to 100:\nSince the numbers on the right are \n\n\n\n\u2212\n\nx\n\u2032\n\n\n\n{\\displaystyle -x'}\n\n and \n\n\n\n\u2212\n\ny\n\u2032\n\n\n\n{\\displaystyle -y'}\n\n, the product is obtained by subtracting from the top left number the bottom right number (or subtract from the bottom left number the top right number), multiply that by 100 and add to that the product of the two numbers on the right. We have 87 - 8 = 79; 79*100 = 7900; 8*13 = 104; 7900+104 = 8004.\nThe multiplication of 8 by 13 could also have been done using the same method, by taking \n\n\n\nN\n=\n10\n\n\n{\\displaystyle N=10}\n\n. The above table can then be extended to:\nThe product is then computed by evaluating the differences 87-8=79; 13-2 = 11, and the product 2*(-3) = -6. We then have 92*87 = 79*100 + 11*10 - 6 = 7900 + 104 = 8004.\n\nComplex multiplication normally involves four multiplications and two additions.\nOr\nBy 1805 Gauss had discovered a way of reducing the number of multiplications to three.[11]\nThe product (a\u00a0+\u00a0bi) \u00b7 (c\u00a0+\u00a0di) can be calculated in the following way.\nThis algorithm uses only three multiplications, rather than four, and five additions or subtractions rather than two. If a multiply is more expensive than three adds or subtracts, as when calculating by hand, then there is a gain in speed. On modern computers a multiply and an add can take about the same time so there may be no speed gain. There is a trade-off in that there may be some loss of precision when using floating point.\nFor fast Fourier transforms (FFTs) (or any linear transformation) the complex multiplies are by constant coefficients c\u00a0+\u00a0di (called twiddle factors in FFTs), in which case two of the additions (d\u2212c and c+d) can be precomputed. Hence, only three multiplies and three adds are required.[12] However, trading off a multiplication for an addition in this way may no longer be beneficial with modern floating-point units.[13]\nFor systems that need to multiply numbers in the range of several thousand digits, such as computer algebra systems and bignum libraries, long multiplication is too slow. These systems may employ Karatsuba multiplication, which was discovered in 1960 (published in 1962). The heart of Karatsuba's method lies in the observation that two-digit multiplication can be done with only three rather than the four multiplications classically required. This is an example of what is now called a divide and conquer algorithm. Suppose we want to multiply two 2-digit numbers: x1x2\u00b7 y1y2:\nBigger numbers x1x2 can be split into two parts x1 and x2. Then the method works analogously. To compute these three products of m-digit numbers, we can employ the same trick again, effectively using recursion. Once the numbers are computed, we need to add them together (step 5.), which takes about n operations.\nKaratsuba multiplication has a time complexity of O(nlog23) \u2248 O(n1.585), making this method significantly faster than long multiplication. Because of the overhead of recursion, Karatsuba's multiplication is slower than long multiplication for small values of n; typical implementations therefore switch to long multiplication if n is below some threshold.\nKaratsuba's algorithm is the first known algorithm for multiplication that is asymptotically faster than long multiplication,[14] and can thus be viewed as the starting point for the theory of fast multiplications.\nAnother method of multiplication is called Toom\u2013Cook or Toom-3. The Toom\u2013Cook method splits each number to be multiplied into multiple parts. The Toom\u2013Cook method is one of the generalizations of the Karatsuba method. A three-way Toom\u2013Cook can do a size-3N multiplication for the cost of five size-N multiplications, improvement by a factor of 9/5 compared to the Karatsuba method's improvement by a factor of 4/3.\nAlthough using more and more parts can reduce the time spent on recursive multiplications further, the overhead from additions and digit management also grows. For this reason, the method of Fourier transforms is typically faster for numbers with several thousand digits, and asymptotically faster for even larger numbers.\nThe basic idea due to Strassen (1968) is to use fast polynomial multiplication to perform fast integer multiplication. The algorithm was made practical and theoretical guarantees were provided in 1971 by Sch\u00f6nhage and Strassen resulting in the Sch\u00f6nhage\u2013Strassen algorithm.[15] The details are the following: We choose the largest integer w that will not cause overflow during the process outlined below. Then we split the two numbers into m groups of w bits as follows\nWe look at these numbers as polynomials in x, where x = 2w, to get,\nThen we can then say that,\nClearly the above setting is realized by polynomial multiplication, of two polynomials a and b. The crucial step now is to use Fast Fourier multiplication of polynomials to realize the multiplications above faster than in naive O(m2) time.\nTo remain in the modular setting of Fourier transforms, we look for a ring with a 2mth root of unity. hence we do multiplication modulo N (and thus in the Z/NZ ring). Further, N must be chosen so that there is no 'wrap around', essentially, no reductions modulo N occur. Thus, the choice of N is crucial. For example, it could be done as,\nThe ring Z/NZ would thus have a 2mth root of unity, namely 8. Also, it can be checked that ck < N, and thus no wrap around will occur.\nThe algorithm has a time complexity of \u0398(n\u00a0log(n)\u00a0log(log(n))) and is used in practice for numbers with more than 10,000 to 40,000 decimal digits. In 2007 this was improved by Martin F\u00fcrer (F\u00fcrer's algorithm) [16] to give a time complexity of n\u00a0log(n)\u00a02\u0398(log*(n)) using Fourier transforms over complex numbers. Anindya De, Chandan Saha, Piyush Kurur and Ramprasad Saptharishi[17] gave a similar algorithm using modular arithmetic in 2008 achieving the same running time. In context of the above material, what these latter authors have achieved is to find N much less than 23k + 1, so that Z/NZ has a 2mth root of unity. This speeds up computation and reduces the time complexity. However, these latter algorithms are only faster than Sch\u00f6nhage\u2013Strassen for impractically large inputs.\nUsing number-theoretic transforms instead of discrete Fourier transforms avoids rounding error problems by using modular arithmetic instead of floating-point arithmetic. In order to apply the factoring which enables the FFT to work, the length of the transform must be factorable to small primes and must be a factor of N-1, where N is the field size. In particular, calculation using a Galois Field GF(k2), where k is a Mersenne Prime, allows the use of a transform sized to a power of 2; e.g. k = 231-1 supports transform sizes up to 232.\nThere is a trivial lower bound of \u03a9(n) for multiplying two n-bit numbers on a single processor; no matching algorithm (on conventional Turing machines) nor any better lower bound is known. Multiplication lies outside of AC0[p] for any prime p, meaning there is no family of constant-depth, polynomial (or even subexponential) size circuits using AND, OR, NOT, and MODp gates that can compute a product. This follows from a constant-depth reduction of MODq to multiplication.[18] Lower bounds for multiplication are also known for some classes of branching programs.[19]\nAll the above multiplication algorithms can also be expanded to multiply polynomials. For instance the Strassen algorithm may be used for polynomial multiplication[20] Alternatively the Kronecker substitution technique may be used to convert the problem of multiplying polynomials into a single binary multiplication.[21]\nLong multiplication methods can be generalised to allow the multiplication of algebraic formulae:\nAs a further example of column based multiplication, consider multiplying 23 long tons (t), 12 hundredweight (cwt) and 2 quarters (qtr) by 47. This example uses avoirdupois measures: 1 t = 20 cwt, 1 cwt = 4 qtr.\nFirst multiply the quarters by 2, the result 94 is written into the first workspace. Next, multiply 12 x 47 but don't add up the partial results (48, 1080) yet. Likewise multiply 23 by 47. The quarters column is totalled and the result placed in the second workspace (a trivial move in this case). 94 quarters is 48 cwt and 2 qtr, so place the 2 in the answer and put the 23 in the next column left. Now add up the three entries in the cwt column giving 1131. This is 56 t 11 cwt, so write the 11 into the answer and the 56 in the column to the left. Now add up the tons column. There is no adjustment to make, so the result is just copied down.\nThe same layout and methods can be used for any traditional measurements and non-decimal currencies such as the old British \u00a3sd system.", 
    "dbpedia_url": "http://dbpedia.org/resource/Multiplication_algorithm", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Multiplication_algorithm\n"
}