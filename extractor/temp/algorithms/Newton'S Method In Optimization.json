{
    "about": "In calculus, Newton's method is an iterative method for finding the roots of a differentiable function f (i.e. solutions to the equation f(x)=0). In optimization, Newton's method is applied to the derivative f \u2032 of a twice-differentiable function f to find the roots of the derivative (solutions to f \u2032(x)=0), also known as the stationary points of f.", 
    "name": "Newton'S Method In Optimization", 
    "classification": "Optimization Algorithms And Methods", 
    "full_text": "In calculus, Newton's method is an iterative method for finding the roots of a differentiable function f (i.e. solutions to the equation f(x)=0). In optimization, Newton's method is applied to the derivative f \u2032 of a twice-differentiable function f to find the roots of the derivative (solutions to f \u2032(x)=0), also known as the stationary points of f.\n\n\nIn the one-dimensional problem, Newton's method attempts to construct a sequence xn from an initial guess x0 that converges towards some value x* satisfying f \u2032(x*)=0. This x* is a stationary point of f.\nThe second order Taylor expansion fT(x) of f around xn is:\nWe want to find \u0394x such that xn + \u0394x is a stationary point. We seek to solve the equation that sets the derivative of this last expression with respect to \u0394x equal to zero:\nFor the value of \u0394x = \u2212f \u2032(xn) / f \u2033(xn), which is the solution of this equation, it can be hoped that xn+1 = xn + \u0394x = xn \u2212 f \u2032(xn) / f \u2033(xn) will be closer to a stationary point x*. Provided that f is a twice-differentiable function and other technical conditions are satisfied, the sequence x1, x2, \u2026 will converge to a point x* satisfying f \u2032(x*) = 0.\nThe geometric interpretation of Newton's method is that at each iteration one approximates f(x) by a quadratic function around xn, and then takes a step towards the maximum/minimum of that quadratic function (in higher dimensions, this may also be a saddle point). Note that if f(x) happens to be a quadratic function, then the exact extremum is found in one step.\nThe above iterative scheme can be generalized to several dimensions by replacing the derivative with the gradient, \u2207f(x), and the reciprocal of the second derivative with the inverse of the Hessian matrix, H f(x). One obtains the iterative scheme\nOften Newton's method is modified to include a small step size \u03b3 \u2208 (0,1) instead of \u03b3 = 1\nThis is often done to ensure that the Wolfe conditions are satisfied at each step xn \u2192 xn+1 of the iteration. For step sizes other than 1, the method is often referred to as the relaxed Newton's method.\nWhere applicable, Newton's method converges much faster towards a local maximum or minimum than gradient descent. In fact, every local minimum has a neighborhood N such that, if we start with x0 \u2208 N, Newton's method with step size \u03b3 = 1 converges quadratically (if the Hessian is invertible and a Lipschitz continuous function of x in that neighborhood).\nFinding the inverse of the Hessian in high dimensions can be an expensive operation. In such cases, instead of directly inverting the Hessian it's better to calculate the vector \u0394x = xn + 1 - xn as the solution to the system of linear equations\nwhich may be solved by various factorizations or approximately (but to great accuracy) using iterative methods. Many of these methods are only applicable to certain types of equations, for example the Cholesky factorization and conjugate gradient will only work if [H f(xn)] is a positive definite matrix. While this may seem like a limitation, it's often useful indicator of something gone wrong, for example if a minimization problem is being approached and [H f(xn)] is not positive definite, then the iterations are converging to a saddle point and not a minimum.\nOn the other hand, if a constrained optimization is done (for example, with Lagrange multipliers), the problem may become one of saddle point finding, in which case the Hessian will be symmetric indefinite and the solution of xn+1 will need to be done with a method that will work for such, such as the LDLT variant of Cholesky factorization or the conjugate residual method.\nThere also exist various quasi-Newton methods, where an approximation for the Hessian (or its inverse directly) is built up from changes in the gradient.\nIf the Hessian is close to a non-invertible matrix, the inverted Hessian can be numerically unstable and the solution may diverge. In this case, certain workarounds have been tried in the past, which have varied success with certain problems. One can, for example, modify the Hessian by adding a correction matrix Bn so as to make Hf(xn) + Bn positive definite. One approach is to diagonalize H f(xn) and choose Bn so that H f(xn) + Bn has the same eigenvectors as H f(xn), but with each negative eigenvalue replaced by \u03f5 > 0.\nAn approach exploited in the Levenberg\u2013Marquardt algorithm (which uses an approximate Hessian) is to add a scaled identity matrix to the Hessian, \u03bcI, with the scale adjusted at every iteration as needed. For large \u03bc and small Hessian, the iterations will behave like gradient descent with step size 1 / \u03bc. This results in slower but more reliable convergence where the Hessian doesn't provide useful information.\n", 
    "dbpedia_url": "http://dbpedia.org/resource/Newton's_method_in_optimization", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Newton's_method_in_optimization\n"
}