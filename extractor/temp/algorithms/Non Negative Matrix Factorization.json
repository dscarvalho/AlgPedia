{
    "about": "Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.", 
    "classification": "Machine Learning Algorithms", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Non-negative_matrix_factorization\n", 
    "full_text": "Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.\nNMF finds applications in such fields as computer vision, document clustering,[1] chemometrics, audio signal processing[3] and recommender systems.[4][5]\n\n\nIn chemometrics non-negative matrix factorization has a long history under the name \"self modeling curve resolution\".[6] In this framework the vectors in the right matrix are continuous curves rather than discrete vectors. Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the middle of the 1990s under the name positive matrix factorization.[7][8] It became more widely known as non-negative matrix factorization after Lee and Seung investigated the properties of the algorithm and published some simple and useful algorithms for two types of factorizations.[9][10]\nLet matrix V be the product of the matrices W and H,\nMatrix multiplication can be implemented as computing the column vectors of V as linear combinations of the column vectors in W using coefficients supplied by columns of H. That is, each column of V can be computed as follows:\nwhere vi is the i-th column vector of the product matrix V and hi is the i-th column vector of the matrix H.\nWhen multiplying matrices, the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of NMF. NMF generates factors with significantly reduced dimensions compared to the original matrix. For example, if V is an m \u00d7 n matrix, W is an m \u00d7 p matrix, and H is a p \u00d7 n matrix then p can be significantly less than both m and n.\nHere's an example based on a text-mining application:\nThis last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.\nIt's useful to think of each feature (column vector) in the features matrix W as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix H represents an original document with a cell value defining the document's rank for a feature. This follows because each row in H represents a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in W) where each feature is weighted by the feature's cell value from the document's column in H.\nNMF has an inherent clustering property,[11] i.e., it automatically clusters the columns of input data \n\n\n\n\nV\n\n=\n(\n\nv\n\n1\n\n\n,\n\u22ef\n,\n\nv\n\nn\n\n\n)\n\n\n{\\displaystyle \\mathbf {V} =(v_{1},\\cdots ,v_{n})}\n\n. It is this property that drives most applications of NMF.\nMore specifically, the approximation of \n\n\n\n\nV\n\n\n\n{\\displaystyle \\mathbf {V} }\n\n by \n\n\n\n\nV\n\n\u2243\n\nW\n\n\nH\n\n\n\n{\\displaystyle \\mathbf {V} \\simeq \\mathbf {W} \\mathbf {H} }\n\n is achieved by minimizing the error function\n\n\n\n\n\nmin\n\nW\n,\nH\n\n\n\n|\n\n\n|\n\nV\n\u2212\nW\nH\n\n|\n\n\n\n|\n\n\nF\n\n\n,\n\n\n{\\displaystyle \\min _{W,H}||V-WH||_{F},}\n\n subject to \n\n\n\nW\n\u2265\n0\n,\nH\n\u2265\n0.\n\n\n{\\displaystyle W\\geq 0,H\\geq 0.}\n\n\nIf we add additional orthogonality constraint on \n\n\n\nH\n\n\n{\\displaystyle H}\n\n, i.e., \n\n\n\nH\n\nH\n\nT\n\n\n=\nI\n\n\n{\\displaystyle HH^{T}=I}\n\n, then the above minimization is mathematically equivalent to the minimization of K-means clustering ).\nFurthermore, the computed \n\n\n\nH\n\n\n{\\displaystyle H}\n\n gives the cluster indicator, i.e., if \n\n\n\n\n\nH\n\n\nk\nj\n\n\n>\n0\n\n\n{\\displaystyle \\mathbf {H} _{kj}>0}\n\n, that fact indicates input data \n\n\n\n\nv\n\nj\n\n\n\n\n{\\displaystyle v_{j}}\n\n belongs to \n\n\n\n\nk\n\nt\nh\n\n\n\n\n{\\displaystyle k^{th}}\n\n cluster. And the computed \n\n\n\nW\n\n\n{\\displaystyle W}\n\n gives the cluster centroids, i.e., the \n\n\n\n\nk\n\nt\nh\n\n\n\n\n{\\displaystyle k^{th}}\n\n column gives the cluster centroid of \n\n\n\n\nk\n\nt\nh\n\n\n\n\n{\\displaystyle k^{th}}\n\n cluster. This centroids representation can be significantly enhanced by convex NMF.\nWhen the orthogonality \n\n\n\nH\n\nH\n\nT\n\n\n=\nI\n\n\n{\\displaystyle HH^{T}=I}\n\n is not explicitly imposed, the orthogonality holds to a large extent, and the clustering property holds too. Clustering is the main objective of most data mining applications of NMF.[citation needed]\nWhen the error function to be used is Kullback\u2013Leibler divergence, NMF is identical to the Probabilistic latent semantic analysis, a popular document clustering method.[12]\nUsually the number of columns of W and the number of rows of H in NMF are selected so the product WH will become an approximation to V. The full decomposition of V then amounts to the two non-negative matrices W and H as well as a residual U, such that: V = WH + U. The elements of the residual matrix can either be negative or positive.\nWhen W and H are smaller than V they become easier to store and manipulate. Another reason for factorizing V into smaller matrices W and H, is that if one is able to approximately represent the elements of V by significantly less data, then one has to infer some latent structure in the data.\nIn standard NMF, matrix factor W \u2208 \u211d+m \u00d7 k\uff0c i.e., W can be anything in that space. Convex NMF[13] restricts the columns of W to convex combinations of the input data vectors \n\n\n\n(\n\nv\n\n1\n\n\n,\n\u22ef\n,\n\nv\n\nn\n\n\n)\n\n\n{\\displaystyle (v_{1},\\cdots ,v_{n})}\n\n. This greatly improves the quality of data representation of W. Furthermore, the resulting matrix factor H becomes more sparse and orthogonal.\nIn case the nonnegative rank of V is equal to its actual rank, V = WH is called a nonnegative rank factorization.[14][15][16] The problem of finding the NRF of V, if it exists, is known to be NP-hard.[17]\nThere are different types of non-negative matrix factorizations. The different types arise from using different cost functions for measuring the divergence between V and WH and possibly by regularization of the W and/or H matrices.[1]\nTwo simple divergence functions studied by Lee and Seung are the squared error (or Frobenius norm) and an extension of the Kullback\u2013Leibler divergence to positive matrices (the original Kullback\u2013Leibler divergence is defined on probability distributions). Each divergence leads to a different NMF algorithm, usually minimizing the divergence using iterative update rules.\nThe factorization problem in the squared error version of NMF may be stated as: Given a matrix \n\n\n\n\nV\n\n\n\n{\\displaystyle \\mathbf {V} }\n\n find nonnegative matrices W and H that minimize the function\nAnother type of NMF for images is based on the total variation norm.[18]\nWhen L1 regularization (akin to Lasso) is added to NMF with the mean squared error cost function, the resulting problem may be called non-negative sparse coding due to the similarity to the sparse coding problem,[19][20] although it may also still be referred to as NMF.[21]\nMany standard NMF algorithms analyze all the data together; i.e., the whole matrix is available from the start. This may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion. One such use is for collaborative filtering in recommendation systems, where there may be many users and many items to recommend, and it would be inefficient to recalculate everything when one user or one item is added to the system. The cost function for optimization in these cases may or may not be the same as for standard NMF, but the algorithms need to be rather different.[22][23][24]\nThere are several ways in which the W and H may be found: Lee and Seung's multiplicative update rule[10] has been a popular method due to the simplicity of implementation. Since then, a few other algorithmic approaches have been developed.\nSome successful algorithms are based on alternating non-negative least squares: in each step of such an algorithm, first H is fixed and W found by a non-negative least squares solver, then W is fixed and H is found analogously. The procedures used to solve for W and H may be the same[25] or different, as some NMF variants regularize one of W and H.[19] Specific approaches include the projected gradient descent methods,[25][26] the active set method,[4][27] the optimal gradient method,[28] and the block principal pivoting method[29] among several others.\nThe currently available algorithms are sub-optimal as they can only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the k-means clustering problem which is known to be NP-complete.[30] However, as in many other data mining applications, a local minimum may still prove to be useful.\nExact solutions for the variants of NMF can be expected (in polynomial time) when additional constraints hold for matrix V. A polynomial time algorithm for solving nonnegative rank factorization if V contains a monomial sub matrix of rank equal to its rank was given by Campbell and Poole in 1981.[31] Kalofolias and Gallopoulos (2012)[32] solved the symmetric counterpart of this problem, where V is symmetric and contains a diagonal principal sub matrix of rank r. Their algorithm runs in O(rm^2) time in the dense case. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) give a polynomial time algorithm for exact NMF that works for the case where one of the factors W satisfies the separability condition.[33]\nIn Learning the parts of objects by non-negative matrix factorization Lee and Seung proposed NMF mainly for parts-based decomposition of images. It compares NMF to vector quantization and principal component analysis, and shows that although the three techniques may be written as factorizations, they implement different constraints and therefore produce different results.\nIt was later shown that some types of NMF are an instance of a more general probabilistic model called \"multinomial PCA\".[34] When NMF is obtained by minimizing the Kullback\u2013Leibler divergence, it is in fact equivalent to another instance of multinomial PCA, probabilistic latent semantic analysis,[35] trained by maximum likelihood estimation. That method is commonly used for analyzing and clustering textual data and is also related to the latent class model.\nNMF with the least-squares objective is equivalent to a relaxed form of K-means clustering: the matrix factor W contains cluster centroids and H contains cluster membership indicators.[11][36] This provides a theoretical foundation for using NMF for data clustering. However, k-means does not enforce non-negativity on its centroids, so the closest analogy is in fact with \"semi-NMF\".[13]\nNMF can be seen as a two-layer directed graphical model with one layer of observed random variables and one layer of hidden random variables.[37]\nNMF extends beyond matrices to tensors of arbitrary order.[38][39][40] This extension may be viewed as a non-negative counterpart to, e.g., the PARAFAC model.\nOther extensions of NMF include joint factorisation of several data matrices and tensors where some factors are shared. Such models are useful for sensor fusion and relational learning.[41]\nNMF is an instance of nonnegative quadratic programming (NQP), just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains.[42]\nThe factorization is not unique: A matrix and its inverse can be used to transform the two factorization matrices by, e.g.,[43]\nIf the two new matrices \n\n\n\n\n\n\n\nW\n~\n\n\n\n=\nW\nB\n\n\n\n{\\displaystyle \\mathbf {{\\tilde {W}}=WB} }\n\n and \n\n\n\n\n\n\nH\n~\n\n\n\n=\n\n\nB\n\n\n\u2212\n1\n\n\n\nH\n\n\n\n{\\displaystyle \\mathbf {\\tilde {H}} =\\mathbf {B} ^{-1}\\mathbf {H} }\n\n are non-negative they form another parametrization of the factorization.\nThe non-negativity of \n\n\n\n\n\n\nW\n~\n\n\n\n\n\n{\\displaystyle \\mathbf {\\tilde {W}} }\n\n and \n\n\n\n\n\n\nH\n~\n\n\n\n\n\n{\\displaystyle \\mathbf {\\tilde {H}} }\n\n applies at least if B is a non-negative monomial matrix. In this simple case it will just correspond to a scaling and a permutation.\nMore control over the non-uniqueness of NMF is obtained with sparsity constraints.[44]\nNMF can be used for text mining applications. In this process, a document-term matrix is constructed with the weights of various terms (typically weighted word frequency information) from a set of documents. This matrix is factored into a term-feature and a feature-document matrix. The features are derived from the contents of the documents, and the feature-document matrix describes data clusters of related documents.\nOne specific application used hierarchical NMF on a small subset of scientific abstracts from PubMed.[45] Another research group clustered parts of the Enron email dataset[46] with 65,033 messages and 91,133 terms into 50 clusters.[47] NMF has also been applied to citations data, with one example clustering English Wikipedia articles and scientific journals based on the outbound scientific citations in English Wikipedia.[48]\nArora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) have given polynomial-time algorithms to learn topic models using NMF. The algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings.[33]\nNMF is also used to analyze spectral data; one such use is in the classification of space objects and debris.[49]\nNMF is applied in scalable Internet distance (round-trip time) prediction. For a network with \n\n\n\nN\n\n\n{\\displaystyle N}\n\n hosts, with the help of NMF, the distances of all the \n\n\n\n\nN\n\n2\n\n\n\n\n{\\displaystyle N^{2}}\n\n end-to-end links can be predicted after conducting only \n\n\n\nO\n(\nN\n)\n\n\n{\\displaystyle O(N)}\n\n measurements. This kind of method was firstly introduced in Internet Distance Estimation Service (IDES).[50] Afterwards, as a fully decentralized approach, Phoenix network coordinate system[51] is proposed. It achieves better overall prediction accuracy by introducing the concept of weight.\nSpeech denoising has been a long lasting problem in audio signal processing. There are lots of algorithms for denoising if the noise is stationary. For example, the Wiener filter is suitable for additive Gaussian noise. However, if the noise is non-stationary, the classical denoising algorithms usually have poor performance because the statistical information of the non-stationary noise is difficult to estimate. Schmidt et al.[52] use NMF to do speech denoising under non-stationary noise, which is completely different from classical statistical approaches.The key idea is that clean speech signal can be sparsely represented by a speech dictionary, but non-stationary noise cannot. Similarly, non-stationary noise can also be sparsely represented by a noise dictionary, but speech cannot.\nThe algorithm for NMF denoising goes as follows. Two dictionaries, one for speech and one for noise, need to be trained offline. Once a noisy speech is given, we first calculate the magnitude of the Short-Time-Fourier-Transform. Second, separate it into two parts via NMF, one can be sparsely represented by the speech dictionary, and the other part can be sparsely represented by the noise dictionary. Third, the part that is represented by the speech dictionary will be the estimated clean speech.\nNMF has been successfully applied in bioinformatics for clustering gene expression and DNA methylation data and finding the genes most representative of the clusters.[20][53][54][55] In the analysis of cancer mutations it has been used to identify common patterns of mutations that occur in many cancers and that probably have distinct causes.[56]\nNMF, also referred in this field as factor analysis, has been used since the 80s[57] to analyze sequences of images in SPECT and PET dynamic medical imaging. Non-uniqueness of NMF was addressed using sparsity constraints.[58]\nCurrent research (since 2010) in nonnegative matrix factorization includes, but is not limited to,", 
    "name": "Non Negative Matrix Factorization"
}