{
    "about": "FastICA is an efficient and popular algorithm for independent component analysis invented by Aapo Hyv\u00e4rinen at Helsinki University of Technology.[1][2] Like most ICA algorithms, FastICA seeks an orthogonal rotation of prewhitened data, through a fixed-point iteration scheme, that maximizes a measure of non-Gaussianity of the rotated components. Non-gaussianity serves as a proxy for statistical independence, which is a very strong condition and requires infinite data to verify. FastICA can also be alternatively derived as an approximative Newton iteration.", 
    "name": "Fastica", 
    "classification": "Computational Statistics", 
    "full_text": "FastICA is an efficient and popular algorithm for independent component analysis invented by Aapo Hyv\u00e4rinen at Helsinki University of Technology.[1][2] Like most ICA algorithms, FastICA seeks an orthogonal rotation of prewhitened data, through a fixed-point iteration scheme, that maximizes a measure of non-Gaussianity of the rotated components. Non-gaussianity serves as a proxy for statistical independence, which is a very strong condition and requires infinite data to verify. FastICA can also be alternatively derived as an approximative Newton iteration.\n\n\nLet the \n\n\n\n\nX\n\n:=\n(\n\nx\n\ni\nj\n\n\n)\n\u2208\n\n\nR\n\n\nN\n\u00d7\nM\n\n\n\n\n{\\displaystyle \\mathbf {X} :=(x_{ij})\\in \\mathbb {R} ^{N\\times M}}\n\n denote the input data matrix, \n\n\n\nM\n\n\n{\\displaystyle M}\n\n the number of columns corresponding with the number of samples of mixed signals and \n\n\n\nN\n\n\n{\\displaystyle N}\n\n the number of rows corresponding with the number of independent source signals. The input data matrix \n\n\n\n\nX\n\n\n\n{\\displaystyle \\mathbf {X} }\n\n must be prewhitened, or centered and whitened, before applying the FastICA algorithm to it.\nThe iterative algorithm finds the direction for the weight vector \n\n\n\n\nw\n\n\u2208\n\n\nR\n\n\nN\n\n\n\n\n{\\displaystyle \\mathbf {w} \\in \\mathbb {R} ^{N}}\n\n that maximizes a measure of non-Gaussianity of the projection \n\n\n\n\n\nw\n\n\nT\n\n\n\nX\n\n\n\n{\\displaystyle \\mathbf {w} ^{T}\\mathbf {X} }\n\n, with \n\n\n\n\nX\n\n\u2208\n\n\nR\n\n\nN\n\u00d7\nM\n\n\n\n\n{\\displaystyle \\mathbf {X} \\in \\mathbb {R} ^{N\\times M}}\n\n denoting a prewhitened data matrix as described above. Note that \n\n\n\n\nw\n\n\n\n{\\displaystyle \\mathbf {w} }\n\n is a column vector. To measure non-Gaussianity, FastICA relies on a nonquadratic nonlinearity function \n\n\n\nf\n(\nu\n)\n\n\n{\\displaystyle f(u)}\n\n, its first derivative \n\n\n\ng\n(\nu\n)\n\n\n{\\displaystyle g(u)}\n\n, and its second derivative \n\n\n\n\ng\n\n\u2032\n\n\n(\nu\n)\n\n\n{\\displaystyle g^{\\prime }(u)}\n\n. Hyv\u00e4rinen states that the functions\nare useful for general purposes, while\nmay be highly robust.[1] The steps for extracting the weight vector \n\n\n\n\nw\n\n\n\n{\\displaystyle \\mathbf {w} }\n\n for single component in FastICA are the following:\nThe single unit iterative algorithm estimates only one weight vector which extracts a single component. Estimating additional components that are mutually \"independent\" requires repeating the algorithm to obtain linearly independent projection vectors - note that the notion of independence here refers to maximizing non-Gaussianity in the estimated components. Hyv\u00e4rinen provides several ways of extracting multiple components with the simplest being the following. Here, \n\n\n\n\n1\n\n\n\n{\\displaystyle \\mathbf {1} }\n\n is a column vector of 1's of dimension \n\n\n\nM\n\n\n{\\displaystyle M}\n\n.\nAlgorithm FastICA\n", 
    "dbpedia_url": "http://dbpedia.org/resource/FastICA", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/FastICA\n"
}