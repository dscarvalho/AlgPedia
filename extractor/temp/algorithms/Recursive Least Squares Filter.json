{
    "about": "The Recursive least squares (RLS) is an adaptive filter which recursively finds the coefficients that minimize a weighted linear least squares cost function relating to the input signals. This is in contrast to other algorithms such as the least mean squares (LMS) that aim to reduce the mean square error. In the derivation of the RLS, the input signals are considered deterministic, while for the LMS and similar algorithm they are considered stochastic. Compared to most of its competitors, the RLS exhibits extremely fast convergence. However, this benefit comes at the cost of high computational complexity.", 
    "name": "Recursive Least Squares Filter", 
    "classification": "Digital Signal Processing", 
    "full_text": "The Recursive least squares (RLS) is an adaptive filter which recursively finds the coefficients that minimize a weighted linear least squares cost function relating to the input signals. This is in contrast to other algorithms such as the least mean squares (LMS) that aim to reduce the mean square error. In the derivation of the RLS, the input signals are considered deterministic, while for the LMS and similar algorithm they are considered stochastic. Compared to most of its competitors, the RLS exhibits extremely fast convergence. However, this benefit comes at the cost of high computational complexity.\n\n\nRLS was discovered by Gauss but laid unused or ignored until 1950 when Plackett rediscovered the original work of Gauss from 1821. In general, the RLS can be used to solve any problem that can be solved by adaptive filters. For example, suppose that a signal d(n) is transmitted over an echoey, noisy channel that causes it to be received as\nwhere \n\n\n\nv\n(\nn\n)\n\n\n{\\displaystyle v(n)}\n\n represents additive noise. We will attempt to recover the desired signal \n\n\n\nd\n(\nn\n)\n\n\n{\\displaystyle d(n)}\n\n by use of a \n\n\n\np\n+\n1\n\n\n{\\displaystyle p+1}\n\n-tap FIR filter, \n\n\n\n\nw\n\n\n\n{\\displaystyle \\mathbf {w} }\n\n:\nwhere \n\n\n\n\n\nx\n\n\nn\n\n\n=\n[\nx\n(\nn\n)\n\nx\n(\nn\n\u2212\n1\n)\n\n\u2026\n\nx\n(\nn\n\u2212\np\n)\n\n]\n\nT\n\n\n\n\n{\\displaystyle \\mathbf {x} _{n}=[x(n)\\quad x(n-1)\\quad \\ldots \\quad x(n-p)]^{T}}\n\n is the vector containing the \n\n\n\np\n+\n1\n\n\n{\\displaystyle p+1}\n\n most recent samples of \n\n\n\nx\n(\nn\n)\n\n\n{\\displaystyle x(n)}\n\n. Our goal is to estimate the parameters of the filter \n\n\n\n\nw\n\n\n\n{\\displaystyle \\mathbf {w} }\n\n, and at each time n we refer to the new least squares estimate by \n\n\n\n\n\nw\n\n\nn\n\n\n\n\n{\\displaystyle \\mathbf {w} _{n}}\n\n. As time evolves, we would like to avoid completely redoing the least squares algorithm to find the new estimate for \n\n\n\n\n\nw\n\n\nn\n+\n1\n\n\n\n\n{\\displaystyle \\mathbf {w} _{n+1}}\n\n, in terms of \n\n\n\n\n\nw\n\n\nn\n\n\n\n\n{\\displaystyle \\mathbf {w} _{n}}\n\n.\nThe benefit of the RLS algorithm is that there is no need to invert matrices, thereby saving computational power. Another advantage is that it provides intuition behind such results as the Kalman filter.\nThe idea behind RLS filters is to minimize a cost function \n\n\n\nC\n\n\n{\\displaystyle C}\n\n by appropriately selecting the filter coefficients \n\n\n\n\n\nw\n\n\nn\n\n\n\n\n{\\displaystyle \\mathbf {w} _{n}}\n\n, updating the filter as new data arrives. The error signal \n\n\n\ne\n(\nn\n)\n\n\n{\\displaystyle e(n)}\n\n and desired signal \n\n\n\nd\n(\nn\n)\n\n\n{\\displaystyle d(n)}\n\n are defined in the negative feedback diagram below:\n\nThe error implicitly depends on the filter coefficients through the estimate \n\n\n\n\n\n\nd\n^\n\n\n\n(\nn\n)\n\n\n{\\displaystyle {\\hat {d}}(n)}\n\n:\nThe weighted least squares error function \n\n\n\nC\n\n\n{\\displaystyle C}\n\n\u2014the cost function we desire to minimize\u2014being a function of e(n) is therefore also dependent on the filter coefficients:\nwhere \n\n\n\n0\n<\n\u03bb\n\u2264\n1\n\n\n{\\displaystyle 0<\\lambda \\leq 1}\n\n is the \"forgetting factor\" which gives exponentially less weight to older error samples.\nThe cost function is minimized by taking the partial derivatives for all entries \n\n\n\nk\n\n\n{\\displaystyle k}\n\n of the coefficient vector \n\n\n\n\n\nw\n\n\nn\n\n\n\n\n{\\displaystyle \\mathbf {w} _{n}}\n\n and setting the results to zero\nNext, replace \n\n\n\ne\n(\nn\n)\n\n\n{\\displaystyle e(n)}\n\n with the definition of the error signal\nRearranging the equation yields\nThis form can be expressed in terms of matrices\nwhere \n\n\n\n\n\nR\n\n\nx\n\n\n(\nn\n)\n\n\n{\\displaystyle \\mathbf {R} _{x}(n)}\n\n is the weighted sample covariance matrix for \n\n\n\nx\n(\nn\n)\n\n\n{\\displaystyle x(n)}\n\n, and \n\n\n\n\n\nr\n\n\nd\nx\n\n\n(\nn\n)\n\n\n{\\displaystyle \\mathbf {r} _{dx}(n)}\n\n is the equivalent estimate for the cross-covariance between \n\n\n\nd\n(\nn\n)\n\n\n{\\displaystyle d(n)}\n\n and \n\n\n\nx\n(\nn\n)\n\n\n{\\displaystyle x(n)}\n\n. Based on this expression we find the coefficients which minimize the cost function as\nThis is the main result of the discussion.\nThe smaller \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n is, the smaller is the contribution of previous samples to the covariance matrix. This makes the filter more sensitive to recent samples, which means more fluctuations in the filter co-efficients. The \n\n\n\n\u03bb\n=\n1\n\n\n{\\displaystyle \\lambda =1}\n\n case is referred to as the growing window RLS algorithm. In practice, \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n is usually chosen between 0.98 and 1.[1] By using type-II maximum likelihood estimation the optimal \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n can be estimated from a set of data.[2]\nThe discussion resulted in a single equation to determine a coefficient vector which minimizes the cost function. In this section we want to derive a recursive solution of the form\nwhere \n\n\n\n\u0394\n\n\nw\n\n\nn\n\u2212\n1\n\n\n\n\n{\\displaystyle \\Delta \\mathbf {w} _{n-1}}\n\n is a correction factor at time \n\n\n\n\nn\n\u2212\n1\n\n\n\n{\\displaystyle {n-1}}\n\n. We start the derivation of the recursive algorithm by expressing the cross covariance \n\n\n\n\n\nr\n\n\nd\nx\n\n\n(\nn\n)\n\n\n{\\displaystyle \\mathbf {r} _{dx}(n)}\n\n in terms of \n\n\n\n\n\nr\n\n\nd\nx\n\n\n(\nn\n\u2212\n1\n)\n\n\n{\\displaystyle \\mathbf {r} _{dx}(n-1)}\n\n\nwhere \n\n\n\n\nx\n\n(\ni\n)\n\n\n{\\displaystyle \\mathbf {x} (i)}\n\n is the \n\n\n\n\np\n+\n1\n\n\n\n{\\displaystyle {p+1}}\n\n dimensional data vector\nSimilarly we express \n\n\n\n\n\nR\n\n\nx\n\n\n(\nn\n)\n\n\n{\\displaystyle \\mathbf {R} _{x}(n)}\n\n in terms of \n\n\n\n\n\nR\n\n\nx\n\n\n(\nn\n\u2212\n1\n)\n\n\n{\\displaystyle \\mathbf {R} _{x}(n-1)}\n\n by\nIn order to generate the coefficient vector we are interested in the inverse of the deterministic auto-covariance matrix. For that task the Woodbury matrix identity comes in handy. With\nThe Woodbury matrix identity follows\nTo come in line with the standard literature, we define\nwhere the gain vector \n\n\n\ng\n(\nn\n)\n\n\n{\\displaystyle g(n)}\n\n is\nBefore we move on, it is necessary to bring \n\n\n\n\ng\n\n(\nn\n)\n\n\n{\\displaystyle \\mathbf {g} (n)}\n\n into another form\nSubtracting the second term on the left side yields\nWith the recursive definition of \n\n\n\n\nP\n\n(\nn\n)\n\n\n{\\displaystyle \\mathbf {P} (n)}\n\n the desired form follows\nNow we are ready to complete the recursion. As discussed\nThe second step follows from the recursive definition of \n\n\n\n\n\nr\n\n\nd\nx\n\n\n(\nn\n)\n\n\n{\\displaystyle \\mathbf {r} _{dx}(n)}\n\n. Next we incorporate the recursive definition of \n\n\n\n\nP\n\n(\nn\n)\n\n\n{\\displaystyle \\mathbf {P} (n)}\n\n together with the alternate form of \n\n\n\n\ng\n\n(\nn\n)\n\n\n{\\displaystyle \\mathbf {g} (n)}\n\n and get\nWith \n\n\n\n\n\nw\n\n\nn\n\u2212\n1\n\n\n=\n\nP\n\n(\nn\n\u2212\n1\n)\n\n\nr\n\n\nd\nx\n\n\n(\nn\n\u2212\n1\n)\n\n\n{\\displaystyle \\mathbf {w} _{n-1}=\\mathbf {P} (n-1)\\mathbf {r} _{dx}(n-1)}\n\n we arrive at the update equation\nwhere \n\n\n\n\u03b1\n(\nn\n)\n=\nd\n(\nn\n)\n\u2212\n\n\nx\n\n\nT\n\n\n(\nn\n)\n\n\nw\n\n\nn\n\u2212\n1\n\n\n\n\n{\\displaystyle \\alpha (n)=d(n)-\\mathbf {x} ^{T}(n)\\mathbf {w} _{n-1}}\n\n is the a priori error. Compare this with the a posteriori error; the error calculated after the filter is updated:\nThat means we found the correction factor\nThis intuitively satisfying result indicates that the correction factor is directly proportional to both the error and the gain vector, which controls how much sensitivity is desired, through the weighting factor, \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n.\nThe RLS algorithm for a p-th order RLS filter can be summarized as\n\n\n\n\n\nx\n\n(\nn\n)\n=\n\n[\n\n\n\n\nx\n(\nn\n)\n\n\n\n\nx\n(\nn\n\u2212\n1\n)\n\n\n\n\n\u22ee\n\n\n\n\nx\n(\nn\n\u2212\np\n)\n\n\n\n\n]\n\n\n\n{\\displaystyle \\mathbf {x} (n)=\\left[{\\begin{matrix}x(n)\\\\x(n-1)\\\\\\vdots \\\\x(n-p)\\end{matrix}}\\right]}\n\n\nNote that the recursion for \n\n\n\nP\n\n\n{\\displaystyle P}\n\n follows an Algebraic Riccati equation and thus draws parallels to the Kalman filter.[3]\nThe Lattice Recursive Least Squares adaptive filter is related to the standard RLS except that it requires fewer arithmetic operations (order N). It offers additional advantages over conventional LMS algorithms such as faster convergence rates, modular structure, and insensitivity to variations in eigenvalue spread of the input correlation matrix. The LRLS algorithm described is based on a posteriori errors and includes the normalized form. The derivation is similar to the standard RLS algorithm and is based on the definition of \n\n\n\nd\n(\nk\n)\n\n\n\n\n{\\displaystyle d(k)\\,\\!}\n\n. In the forward prediction case, we have \n\n\n\nd\n(\nk\n)\n=\nx\n(\nk\n)\n\n\n\n\n{\\displaystyle d(k)=x(k)\\,\\!}\n\n with the input signal \n\n\n\nx\n(\nk\n\u2212\n1\n)\n\n\n\n\n{\\displaystyle x(k-1)\\,\\!}\n\n as the most up to date sample. The backward prediction case is \n\n\n\nd\n(\nk\n)\n=\nx\n(\nk\n\u2212\ni\n\u2212\n1\n)\n\n\n\n\n{\\displaystyle d(k)=x(k-i-1)\\,\\!}\n\n, where i is the index of the sample in the past we want to predict, and the input signal \n\n\n\nx\n(\nk\n)\n\n\n\n\n{\\displaystyle x(k)\\,\\!}\n\n is the most recent sample.[4]\nThe algorithm for a LRLS filter can be summarized as\nThe normalized form of the LRLS has fewer recursions and variables. It can be calculated by applying a normalization to the internal variables of the algorithm which will keep their magnitude bounded by one. This is generally not used in real-time applications because of the number of division and square-root operations which comes with a high computational load.\nThe algorithm for a NLRLS filter can be summarized as", 
    "dbpedia_url": "http://dbpedia.org/resource/Recursive_least_squares_filter", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Recursive_least_squares_filter\n"
}