{
    "about": "In computer science, Hirschberg's algorithm, named after its inventor, Dan Hirschberg, is a dynamic programming algorithm that finds the optimal sequence alignment between two strings. Optimality is measured with the Levenshtein distance, defined to be the sum of the costs of insertions, replacements, deletions, and null actions needed to change one string into the other. Hirschberg's algorithm is simply described as a divide and conquer version of the Needleman\u2013Wunsch algorithm.[1] Hirschberg's algorithm is commonly used in computational biology to find maximal global alignments of DNA and protein sequences.", 
    "name": "Hirschberg'S Algorithm", 
    "classification": "Bioinformatics Algorithms", 
    "full_text": "In computer science, Hirschberg's algorithm, named after its inventor, Dan Hirschberg, is a dynamic programming algorithm that finds the optimal sequence alignment between two strings. Optimality is measured with the Levenshtein distance, defined to be the sum of the costs of insertions, replacements, deletions, and null actions needed to change one string into the other. Hirschberg's algorithm is simply described as a divide and conquer version of the Needleman\u2013Wunsch algorithm.[1] Hirschberg's algorithm is commonly used in computational biology to find maximal global alignments of DNA and protein sequences.\n\n\nHirschberg's algorithm is a generally applicable algorithm for optimal sequence alignment. BLAST and FASTA are suboptimal heuristics. If x and y are strings, where length(x) = n and length(y) = m, the Needleman-Wunsch algorithm finds an optimal alignment in O(nm) time, using O(nm) space. Hirschberg's algorithm is a clever modification of the Needleman-Wunsch Algorithm which still takes O(nm) time, but needs only O(min{n,m}) space.[2] One application of the algorithm is finding sequence alignments of DNA or protein sequences. It is also a space-efficient way to calculate the longest common subsequence between two sets of data such as with the common diff tool.\nThe Hirschberg algorithm can be derived from the Needleman-Wunsch algorithm by observing that:[3]\n\n\n\n\n\nX\n\ni\n\n\n\n\n{\\displaystyle X_{i}}\n\n denotes the i-th character of \n\n\n\nX\n\n\n{\\displaystyle X}\n\n, where \n\n\n\n1\n<\ni\n\u2a7d\nlength\n\u2061\n(\nX\n)\n\n\n{\\displaystyle 1<i\\leqslant \\operatorname {length} (X)}\n\n. \n\n\n\n\nX\n\ni\n:\nj\n\n\n\n\n{\\displaystyle X_{i:j}}\n\n denotes a substring of size \n\n\n\nj\n\u2212\ni\n+\n1\n\n\n{\\displaystyle j-i+1}\n\n, ranging from i-th to the j-th character of \n\n\n\nX\n\n\n{\\displaystyle X}\n\n. \n\n\n\nrev\n\u2061\n(\nX\n)\n\n\n{\\displaystyle \\operatorname {rev} (X)}\n\n is the reversed version of \n\n\n\nX\n\n\n{\\displaystyle X}\n\n.\n\n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n are sequences to be aligned. Let \n\n\n\nx\n\n\n{\\displaystyle x}\n\n be a character from \n\n\n\nX\n\n\n{\\displaystyle X}\n\n, and \n\n\n\ny\n\n\n{\\displaystyle y}\n\n be a character from \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n. We assume that \n\n\n\nDel\n\u2061\n(\nx\n)\n\n\n{\\displaystyle \\operatorname {Del} (x)}\n\n, \n\n\n\nIns\n\u2061\n(\ny\n)\n\n\n{\\displaystyle \\operatorname {Ins} (y)}\n\n and \n\n\n\nSub\n\u2061\n(\nx\n,\ny\n)\n\n\n{\\displaystyle \\operatorname {Sub} (x,y)}\n\n are well defined integer-valued functions. These functions represent the cost of deleting \n\n\n\nx\n\n\n{\\displaystyle x}\n\n, inserting \n\n\n\ny\n\n\n{\\displaystyle y}\n\n, and replacing \n\n\n\nx\n\n\n{\\displaystyle x}\n\n with \n\n\n\ny\n\n\n{\\displaystyle y}\n\n, respectively.\nWe define \n\n\n\nNWScore\n\u2061\n(\nX\n,\nY\n)\n\n\n{\\displaystyle \\operatorname {NWScore} (X,Y)}\n\n, which returns the last line of the Needleman-Wunsch score matrix \n\n\n\n\nS\nc\no\nr\ne\n\n(\ni\n,\nj\n)\n\n\n{\\displaystyle \\mathrm {Score} (i,j)}\n\n:\nNote that at any point, \n\n\n\nNWScore\n\n\n{\\displaystyle \\operatorname {NWScore} }\n\n only requires the two most recent rows of the score matrix. Thus, \n\n\n\nNWScore\n\n\n{\\displaystyle \\operatorname {NWScore} }\n\n can be implemented in \n\n\n\nO\n(\nmin\n\u2061\n{\nlength\n\u2061\n(\nX\n)\n,\nlength\n\u2061\n(\nY\n)\n}\n)\n\n\n{\\displaystyle O(\\operatorname {min} \\{\\operatorname {length} (X),\\operatorname {length} (Y)\\})}\n\n space, although the algorithm above takes O(i*j) space.\nThe Hirschberg algorithm follows:\nIn the context of Observation (2), assume that \n\n\n\n\nX\n\nl\n\n\n+\n\nX\n\nr\n\n\n\n\n{\\displaystyle X^{l}+X^{r}}\n\n is a partition of \n\n\n\nX\n\n\n{\\displaystyle X}\n\n. Index \n\n\n\n\ny\nm\ni\nd\n\n\n\n{\\displaystyle \\mathrm {ymid} }\n\n is computed such that \n\n\n\n\nY\n\nl\n\n\n=\n\nY\n\n1\n:\n\ny\nm\ni\nd\n\n\n\n\n\n{\\displaystyle Y^{l}=Y_{1:\\mathrm {ymid} }}\n\n and \n\n\n\n\nY\n\nr\n\n\n=\n\nY\n\n\ny\nm\ni\nd\n\n+\n1\n:\nlength\n\u2061\n(\nY\n)\n\n\n\n\n{\\displaystyle Y^{r}=Y_{\\mathrm {ymid} +1:\\operatorname {length} (Y)}}\n\n.\nLet\n\n\n\n\n\n\n\n\nX\n\n\n\n=\n\nA\nG\nT\nA\nC\nG\nC\nA\n\n,\n\n\n\n\nY\n\n\n\n=\n\nT\nA\nT\nG\nC\n\n,\n\n\n\n\nDel\n\u2061\n(\nx\n)\n\n\n\n=\n\u2212\n2\n,\n\n\n\n\nIns\n\u2061\n(\ny\n)\n\n\n\n=\n\u2212\n2\n,\n\n\n\n\nSub\n\u2061\n(\nx\n,\ny\n)\n\n\n\n=\n\n\n{\n\n\n\n+\n2\n,\n\n\n\nif\u00a0\n\nx\n=\ny\n\n\n\n\n\u2212\n1\n,\n\n\n\nif\u00a0\n\nx\n\u2260\ny\n.\n\n\n\n\n\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}X&=\\mathrm {AGTACGCA} ,\\\\Y&=\\mathrm {TATGC} ,\\\\\\operatorname {Del} (x)&=-2,\\\\\\operatorname {Ins} (y)&=-2,\\\\\\operatorname {Sub} (x,y)&={\\begin{cases}+2,&{\\mbox{if }}x=y\\\\-1,&{\\mbox{if }}x\\neq y.\\end{cases}}\\end{aligned}}}\n\n\nThe optimal alignment is given by\nIndeed, this can be verified by backtracking its corresponding Needleman-Wunsch matrix:\nOne starts with the top level call to \n\n\n\nHirschberg\n\u2061\n(\n\nA\nG\nT\nA\nC\nG\nC\nA\n\n,\n\nT\nA\nT\nG\nC\n\n)\n\n\n{\\displaystyle \\operatorname {Hirschberg} (\\mathrm {AGTACGCA} ,\\mathrm {TATGC} )}\n\n, which splits the first argument in half: \n\n\n\nX\n=\n\nA\nG\nT\nA\n\n+\n\nC\nG\nC\nA\n\n\n\n{\\displaystyle X=\\mathrm {AGTA} +\\mathrm {CGCA} }\n\n. The call to \n\n\n\nNWScore\n\u2061\n(\n\nA\nG\nT\nA\n\n,\nY\n)\n\n\n{\\displaystyle \\operatorname {NWScore} (\\mathrm {AGTA} ,Y)}\n\n produces the following matrix:\nLikewise, \n\n\n\nNWScore\n\u2061\n(\nrev\n\u2061\n(\n\nC\nG\nC\nA\n\n)\n,\nrev\n\u2061\n(\nY\n)\n)\n\n\n{\\displaystyle \\operatorname {NWScore} (\\operatorname {rev} (\\mathrm {CGCA} ),\\operatorname {rev} (Y))}\n\n generates the following matrix:\nTheir last lines (after reversing the latter) and sum of those are respectively\nThe maximum (shown in bold) appears at ymid = 2, producing the partition \n\n\n\nY\n=\n\nT\nA\n\n+\n\nT\nG\nC\n\n\n\n{\\displaystyle Y=\\mathrm {TA} +\\mathrm {TGC} }\n\n.\nThe entire Hirschberg recursion (which we omit for brevity) produces the following tree:\nThe leaves of the tree contain the optimal alignment.", 
    "dbpedia_url": "http://dbpedia.org/resource/Hirschberg's_algorithm", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Hirschberg's_algorithm\n"
}