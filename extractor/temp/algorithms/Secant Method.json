{
    "about": "In numerical analysis, the secant method is a root-finding algorithm that uses a succession of roots of secant lines to better approximate a root of a function f. The secant method can be thought of as a finite difference approximation of Newton's method. However, the method was developed independently of Newton's method, and predates it by over 3,000 years.[1]", 
    "name": "Secant Method", 
    "classification": "Root-Finding Algorithms", 
    "full_text": "In numerical analysis, the secant method is a root-finding algorithm that uses a succession of roots of secant lines to better approximate a root of a function f. The secant method can be thought of as a finite difference approximation of Newton's method. However, the method was developed independently of Newton's method, and predates it by over 3,000 years.[1]\n\n\nThe secant method is defined by the recurrence relation\nAs can be seen from the recurrence relation, the secant method requires two initial values, x0 and x1, which should ideally be chosen to lie close to the root.\nStarting with initial values x0 and x1, we construct a line through the points (x0, f(x0)) and (x1, f(x1)), as demonstrated in the picture on the right. In slope-intercept form, this line has the equation\nWe find the root of this line \u2013 the value of x such that y = 0 \u2013 by solving the following equation for x:\nThe solution is\nWe then use this new value of x as x2 and repeat the process using x1 and x2 instead of x0 and x1. We continue this process, solving for x3, x4, etc., until we reach a sufficiently high level of precision (a sufficiently small difference between xn and xn - 1).\nThe iterates \n\n\n\n\nx\n\nn\n\n\n\n\n{\\displaystyle x_{n}}\n\n of the secant method converge to a root of \n\n\n\nf\n\n\n{\\displaystyle f}\n\n, if the initial values \n\n\n\n\nx\n\n0\n\n\n\n\n{\\displaystyle x_{0}}\n\n and \n\n\n\n\nx\n\n1\n\n\n\n\n{\\displaystyle x_{1}}\n\n are sufficiently close to the root. The order of convergence is \u03b1, where\nis the golden ratio. In particular, the convergence is superlinear, but not quite quadratic.\nThis result only holds under some technical conditions, namely that \n\n\n\nf\n\n\n{\\displaystyle f}\n\n be twice continuously differentiable and the root in question be simple (i.e., with multiplicity 1).\nIf the initial values are not close enough to the root, then there is no guarantee that the secant method converges. There is no general definition of \"close enough\", but the criterion has to do with how \"wiggly\" the function is on the interval \n\n\n\n[\n\u00a0\n\nx\n\n0\n\n\n,\n\u00a0\n\nx\n\n1\n\n\n\u00a0\n]\n\n\n{\\displaystyle [~x_{0},~x_{1}~]}\n\n. For example, if \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is differentiable on that interval and there is a point where \n\n\n\n\nf\n\n\u2032\n\n\n=\n0\n\n\n{\\displaystyle f^{\\prime }=0}\n\n on the interval, then the algorithm may not converge.\nThe secant method does not require that the root remain bracketed like the bisection method does, and hence it does not always converge. The false position method (or regula falsi) uses the same formula as the secant method. However, it does not apply the formula on \n\n\n\n\nx\n\nn\n\u2212\n1\n\n\n\n\n{\\displaystyle x_{n-1}}\n\n and \n\n\n\n\nx\n\nn\n\u2212\n2\n\n\n\n\n{\\displaystyle x_{n-2}}\n\n, like the secant method, but on \n\n\n\n\nx\n\nn\n\u2212\n1\n\n\n\n\n{\\displaystyle x_{n-1}}\n\n and on the last iterate \n\n\n\n\nx\n\nk\n\n\n\n\n{\\displaystyle x_{k}}\n\n such that \n\n\n\nf\n(\n\nx\n\nk\n\n\n)\n\n\n{\\displaystyle f(x_{k})}\n\n and \n\n\n\nf\n(\n\nx\n\nn\n\u2212\n1\n\n\n)\n\n\n{\\displaystyle f(x_{n-1})}\n\n have a different sign. This means that the false position method always converges.\nThe recurrence formula of the secant method can be derived from the formula for Newton's method\nby using the finite difference approximation\nThe secant method can be interpreted as a method in which the derivative is replaced by an approximation and is thus a Quasi-Newton method. If we compare Newton's method with the secant method, we see that Newton's method converges faster (order 2 against \u03b1 \u2248 1.6). However, Newton's method requires the evaluation of both \n\n\n\nf\n\n\n{\\displaystyle f}\n\n and its derivative \n\n\n\n\nf\n\n\u2032\n\n\n\n\n{\\displaystyle f^{\\prime }}\n\n at every step, while the secant method only requires the evaluation of \n\n\n\nf\n\n\n{\\displaystyle f}\n\n. Therefore, the secant method may occasionally be faster in practice. For instance, if we assume that evaluating \n\n\n\nf\n\n\n{\\displaystyle f}\n\n takes as much time as evaluating its derivative and we neglect all other costs, we can do two steps of the secant method (decreasing the logarithm of the error by a factor \u03b1\u00b2 \u2248 2.6) for the same cost as one step of Newton's method (decreasing the logarithm of the error by a factor 2), so the secant method is faster. If however we consider parallel processing for the evaluation of the derivative, Newton's method proves its worth, being faster in time, though still spending more steps.\nBroyden's method is a generalization of the secant method to more than one dimension.\nThe following graph shows the function f in red and the last secant line in bold blue. In the graph, the x-intercept of the secant line seems to be a good approximation of the root of f.\nThe Secant method is applied to find a root of the function f(x) = x2 \u2212 612. Here is an implementation in the Matlab language (from calculation, we expect that the iteration converges at x = 24.7386):", 
    "dbpedia_url": "http://dbpedia.org/resource/Secant_method", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Secant_method\n"
}