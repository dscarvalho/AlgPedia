{
    "about": "The Least mean squares filter solution converges to the Wiener filter solution, assuming that the unknown system is LTI and the noise is stationary. Both filters can be used to identify the impulse response of an unknown system, knowing only the original input signal and the output of the unknown system.By relaxing the error criterion to reduce current sample error instead of minimizing the total error over all of n, the LMS algorithm can be derived from the Wiener filter.", 
    "name": "Similarities Between Wiener And Lms", 
    "classification": "Digital Signal Processing", 
    "full_text": "The Least mean squares filter solution converges to the Wiener filter solution, assuming that the unknown system is LTI and the noise is stationary. Both filters can be used to identify the impulse response of an unknown system, knowing only the original input signal and the output of the unknown system.By relaxing the error criterion to reduce current sample error instead of minimizing the total error over all of n, the LMS algorithm can be derived from the Wiener filter.\n\n\nGiven a known input signal \n\n\n\ns\n[\nn\n]\n\n\n{\\displaystyle s[n]}\n\n, the output of an unknown LTI system \n\n\n\nx\n[\nn\n]\n\n\n{\\displaystyle x[n]}\n\n can be expressed as:\n\n\n\n\nx\n[\nn\n]\n=\n\n\u2211\n\nk\n=\n0\n\n\nN\n\u2212\n1\n\n\n\nh\n\nk\n\n\ns\n[\nn\n\u2212\nk\n]\n+\nw\n[\nn\n]\n\n\n{\\displaystyle x[n]=\\sum _{k=0}^{N-1}h_{k}s[n-k]+w[n]}\n\n\nwhere \n\n\n\n\nh\n\nk\n\n\n\n\n{\\displaystyle h_{k}}\n\n is an unknown filter tap coefficients and \n\n\n\nw\n[\nn\n]\n\n\n{\\displaystyle w[n]}\n\n is noise.\nThe model system \n\n\n\n\n\n\nx\n^\n\n\n\n[\nn\n]\n\n\n{\\displaystyle {\\hat {x}}[n]}\n\n, using a Wiener filter solution with an order N, can be expressed as:\n\n\n\n\n\n\n\nx\n^\n\n\n\n[\nn\n]\n=\n\n\u2211\n\nk\n=\n0\n\n\nN\n\u2212\n1\n\n\n\n\n\n\nh\n^\n\n\n\n\nk\n\n\ns\n[\nn\n\u2212\nk\n]\n\n\n{\\displaystyle {\\hat {x}}[n]=\\sum _{k=0}^{N-1}{\\hat {h}}_{k}s[n-k]}\n\n\nwhere \n\n\n\n\n\n\n\nh\n^\n\n\n\n\nk\n\n\n\n\n{\\displaystyle {\\hat {h}}_{k}}\n\n are the filter tap coefficients to be determined.\nThe error between the model and the unknown system can be expressed as:\n\n\n\n\ne\n[\nn\n]\n=\nx\n[\nn\n]\n\u2212\n\n\n\nx\n^\n\n\n\n[\nn\n]\n\n\n{\\displaystyle e[n]=x[n]-{\\hat {x}}[n]}\n\n\nThe total squared error \n\n\n\nE\n\n\n{\\displaystyle E}\n\n can be expressed as:\n\n\n\n\nE\n=\n\n\u2211\n\nn\n=\n\u2212\n\u221e\n\n\n\u221e\n\n\ne\n[\nn\n\n]\n\n2\n\n\n\n\n{\\displaystyle E=\\sum _{n=-\\infty }^{\\infty }e[n]^{2}}\n\n\n\n\n\n\nE\n=\n\n\u2211\n\nn\n=\n\u2212\n\u221e\n\n\n\u221e\n\n\n(\nx\n[\nn\n]\n\u2212\n\n\n\nx\n^\n\n\n\n[\nn\n]\n\n)\n\n2\n\n\n\n\n{\\displaystyle E=\\sum _{n=-\\infty }^{\\infty }(x[n]-{\\hat {x}}[n])^{2}}\n\n\n\n\n\n\nE\n=\n\n\u2211\n\nn\n=\n\u2212\n\u221e\n\n\n\u221e\n\n\n(\nx\n[\nn\n\n]\n\n2\n\n\n\u2212\n2\nx\n[\nn\n]\n\n\n\nx\n^\n\n\n\n[\nn\n]\n+\n\n\n\nx\n^\n\n\n\n[\nn\n\n]\n\n2\n\n\n)\n\n\n{\\displaystyle E=\\sum _{n=-\\infty }^{\\infty }(x[n]^{2}-2x[n]{\\hat {x}}[n]+{\\hat {x}}[n]^{2})}\n\n\nUse the Minimum mean-square error criterion over all of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n by setting its gradient to zero:\n\n\n\n\n\u2207\nE\n=\n0\n\n\n{\\displaystyle \\nabla E=0}\n\n which is \n\n\n\n\n\n\n\u2202\nE\n\n\n\u2202\n\n\n\n\nh\n^\n\n\n\n\ni\n\n\n\n\n\n=\n0\n\n\n{\\displaystyle {\\frac {\\partial E}{\\partial {\\hat {h}}_{i}}}=0}\n\n for all \n\n\n\ni\n=\n0\n,\n1\n,\n2\n,\n.\n.\n.\n,\nN\n\u2212\n1\n\n\n{\\displaystyle i=0,1,2,...,N-1}\n\n\n\n\n\n\n\n\n\n\u2202\nE\n\n\n\u2202\n\n\n\n\nh\n^\n\n\n\n\ni\n\n\n\n\n\n=\n\n\n\u2202\n\n\u2202\n\n\n\n\nh\n^\n\n\n\n\ni\n\n\n\n\n\n\n\u2211\n\nn\n=\n\u2212\n\u221e\n\n\n\u221e\n\n\n[\nx\n[\nn\n\n]\n\n2\n\n\n\u2212\n2\nx\n[\nn\n]\n\n\n\nx\n^\n\n\n\n[\nn\n]\n+\n\n\n\nx\n^\n\n\n\n[\nn\n\n]\n\n2\n\n\n]\n\n\n{\\displaystyle {\\frac {\\partial E}{\\partial {\\hat {h}}_{i}}}={\\frac {\\partial }{\\partial {\\hat {h}}_{i}}}\\sum _{n=-\\infty }^{\\infty }[x[n]^{2}-2x[n]{\\hat {x}}[n]+{\\hat {x}}[n]^{2}]}\n\n\nSubstitute the definition of \n\n\n\n\n\n\nx\n^\n\n\n\n[\nn\n]\n\n\n{\\displaystyle {\\hat {x}}[n]}\n\n:\n\n\n\n\n\n\n\n\u2202\nE\n\n\n\u2202\n\n\n\n\nh\n^\n\n\n\n\ni\n\n\n\n\n\n=\n\n\n\u2202\n\n\u2202\n\n\n\n\nh\n^\n\n\n\n\ni\n\n\n\n\n\n\n\u2211\n\nn\n=\n\u2212\n\u221e\n\n\n\u221e\n\n\n[\nx\n[\nn\n\n]\n\n2\n\n\n\u2212\n2\nx\n[\nn\n]\n\n\u2211\n\nk\n=\n0\n\n\nN\n\u2212\n1\n\n\n\n\n\n\nh\n^\n\n\n\n\nk\n\n\ns\n[\nn\n\u2212\nk\n]\n+\n(\n\n\u2211\n\nk\n=\n0\n\n\nN\n\u2212\n1\n\n\n\n\n\n\nh\n^\n\n\n\n\nk\n\n\ns\n[\nn\n\u2212\nk\n]\n\n)\n\n2\n\n\n]\n\n\n{\\displaystyle {\\frac {\\partial E}{\\partial {\\hat {h}}_{i}}}={\\frac {\\partial }{\\partial {\\hat {h}}_{i}}}\\sum _{n=-\\infty }^{\\infty }[x[n]^{2}-2x[n]\\sum _{k=0}^{N-1}{\\hat {h}}_{k}s[n-k]+(\\sum _{k=0}^{N-1}{\\hat {h}}_{k}s[n-k])^{2}]}\n\n\nDistribute the partial derivative:\n\n\n\n\n\n\n\n\u2202\nE\n\n\n\u2202\n\n\n\n\nh\n^\n\n\n\n\ni\n\n\n\n\n\n=\n\n\u2211\n\nn\n=\n\u2212\n\u221e\n\n\n\u221e\n\n\n[\n\u2212\n2\nx\n[\nn\n]\ns\n[\nn\n\u2212\ni\n]\n+\n2\n(\n\n\u2211\n\nk\n=\n0\n\n\nN\n\u2212\n1\n\n\n\n\n\n\nh\n^\n\n\n\n\nk\n\n\ns\n[\nn\n\u2212\nk\n]\n)\ns\n[\nn\n\u2212\ni\n]\n]\n\n\n{\\displaystyle {\\frac {\\partial E}{\\partial {\\hat {h}}_{i}}}=\\sum _{n=-\\infty }^{\\infty }[-2x[n]s[n-i]+2(\\sum _{k=0}^{N-1}{\\hat {h}}_{k}s[n-k])s[n-i]]}\n\n\nUsing the definition of discrete cross-correlation:\n\n\n\n\n\nR\n\nx\ny\n\n\n(\ni\n)\n=\n\n\u2211\n\nn\n=\n\u2212\n\u221e\n\n\n\u221e\n\n\nx\n[\nn\n]\ny\n[\nn\n\u2212\ni\n]\n\n\n{\\displaystyle R_{xy}(i)=\\sum _{n=-\\infty }^{\\infty }x[n]y[n-i]}\n\n\n\n\n\n\n\n\n\n\u2202\nE\n\n\n\u2202\n\n\n\n\nh\n^\n\n\n\n\ni\n\n\n\n\n\n=\n\u2212\n2\n\nR\n\nx\ns\n\n\n[\ni\n]\n+\n2\n\n\u2211\n\nk\n=\n0\n\n\nN\n\u2212\n1\n\n\n\n\n\n\nh\n^\n\n\n\n\nk\n\n\n\nR\n\ns\ns\n\n\n[\ni\n\u2212\nk\n]\n=\n0\n\n\n{\\displaystyle {\\frac {\\partial E}{\\partial {\\hat {h}}_{i}}}=-2R_{xs}[i]+2\\sum _{k=0}^{N-1}{\\hat {h}}_{k}R_{ss}[i-k]=0}\n\n\nRearrange the terms:\n\n\n\n\n\nR\n\nx\ns\n\n\n[\ni\n]\n=\n\n\u2211\n\nk\n=\n0\n\n\nN\n\u2212\n1\n\n\n\n\n\n\nh\n^\n\n\n\n\nk\n\n\n\nR\n\ns\ns\n\n\n[\ni\n\u2212\nk\n]\n\n\n{\\displaystyle R_{xs}[i]=\\sum _{k=0}^{N-1}{\\hat {h}}_{k}R_{ss}[i-k]}\n\n for all \n\n\n\ni\n=\n0\n,\n1\n,\n2\n,\n.\n.\n.\n,\nN\n\u2212\n1\n\n\n{\\displaystyle i=0,1,2,...,N-1}\n\n\nThis system of N equations with N unknowns can be determined.\nBy relaxing the infinite sum of the Wiener filter to just the error at time \n\n\n\nn\n\n\n{\\displaystyle n}\n\n, the LMS algorithm can be derived.\nThe squared error can be expressed as:\n\n\n\n\nE\n=\n(\nd\n[\nn\n]\n\u2212\ny\n[\nn\n]\n\n)\n\n2\n\n\n\n\n{\\displaystyle E=(d[n]-y[n])^{2}}\n\n\nUsing the Minimum mean-square error criterion, take the gradient:\n\n\n\n\n\n\n\n\u2202\nE\n\n\n\u2202\nw\n\n\n\n=\n\n\n\u2202\n\n\u2202\nw\n\n\n\n(\nd\n[\nn\n]\n\u2212\ny\n[\nn\n]\n\n)\n\n2\n\n\n\n\n{\\displaystyle {\\frac {\\partial E}{\\partial w}}={\\frac {\\partial }{\\partial w}}(d[n]-y[n])^{2}}\n\n\nApply chain rule and substitute definition of y[n]\n\n\n\n\n\n\n\n\u2202\nE\n\n\n\u2202\nw\n\n\n\n=\n2\n(\nd\n[\nn\n]\n\u2212\ny\n[\nn\n]\n)\n\n\n\u2202\n\n\u2202\nw\n\n\n\n(\nd\n[\nn\n]\n\u2212\n\n\u2211\n\nk\n=\n0\n\n\nN\n\u2212\n1\n\n\n\n\n\n\nw\n^\n\n\n\n\nk\n\n\nx\n[\nn\n\u2212\nk\n]\n)\n\n\n{\\displaystyle {\\frac {\\partial E}{\\partial w}}=2(d[n]-y[n]){\\frac {\\partial }{\\partial w}}(d[n]-\\sum _{k=0}^{N-1}{\\hat {w}}_{k}x[n-k])}\n\n\n\n\n\n\n\n\n\n\u2202\nE\n\n\n\u2202\nw\n\n\n\n=\n\u2212\n2\n(\ne\n[\nn\n]\n)\n(\nx\n[\nn\n\u2212\ni\n]\n)\n\n\n{\\displaystyle {\\frac {\\partial E}{\\partial w}}=-2(e[n])(x[n-i])}\n\n\nUsing gradient descent and a step size \n\n\n\n\u03bc\n\n\n{\\displaystyle \\mu }\n\n:\n\n\n\n\nw\n[\nn\n+\n1\n]\n=\nw\n[\nn\n]\n\u2212\n\u03bc\n\n\n\n\u2202\nE\n\n\n\u2202\nw\n\n\n\n\n\n{\\displaystyle w[n+1]=w[n]-\\mu {\\frac {\\partial E}{\\partial w}}}\n\n\nwhich becomes, for i = 0, 1, ..., N-1,\n\n\n\n\nw\n[\nn\n+\n1\n]\n=\nw\n[\nn\n]\n+\n2\n\u03bc\n(\ne\n[\nn\n]\n)\n(\nx\n[\nn\n\u2212\ni\n]\n)\n\n\n{\\displaystyle w[n+1]=w[n]+2\\mu (e[n])(x[n-i])}\n\n\nThis is the LMS update equation.", 
    "dbpedia_url": "http://dbpedia.org/resource/Similarities_between_Wiener_and_LMS", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Similarities_between_Wiener_and_LMS\n"
}