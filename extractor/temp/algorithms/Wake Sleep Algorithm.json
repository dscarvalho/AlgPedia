{
    "about": "The wake-sleep algorithm[1] is an unsupervised learning algorithm for a stochastic multilayer neural network. The algorithm adjusts the parameters so as to produce a good density estimator.[2] There are two learning phases, the \u201cwake\u201d phase and the \u201csleep\u201d phase, which are performed alternately.[3] It was first designed as a model for brain functioning using variational Bayesian learning. After that, the algorithm was adapted to machine learning. It can be viewed as a way to train a Helmholtz Machine[4][5]", 
    "classification": "Machine Learning Algorithms", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Wake-sleep_algorithm\n", 
    "full_text": "The wake-sleep algorithm[1] is an unsupervised learning algorithm for a stochastic multilayer neural network. The algorithm adjusts the parameters so as to produce a good density estimator.[2] There are two learning phases, the \u201cwake\u201d phase and the \u201csleep\u201d phase, which are performed alternately.[3] It was first designed as a model for brain functioning using variational Bayesian learning. After that, the algorithm was adapted to machine learning. It can be viewed as a way to train a Helmholtz Machine[4][5]\n\n\nThe wake-sleep algorithms is visualized as a stack of layers containing representations of data.[6] Layers above represent data from the layer below it. Actual data is placed below the bottom layer, causing layers on top of it to become gradually more abstract. Between each pair of layers there is a recognition weight and generative weight, which are trained to improve reliability during the algorithm runtime.[7]\nThe wake-sleep algorithm is convergent[8] and can be stochastic[9] if alternated appropriately.\nTraining consists of two phases \u2013 the \u201cwake\u201d phase and the \u201csleep\u201d phase.\nNeurons are fired by recognition connections (from what would be input to what would be output). Generative connections (leading from outputs to inputs) are then modified to increase probability that they would recreate the correct activity in the layer below \u2013 closer to actual data from sensory input.[10]\nThe process is reversed in the \u201csleep\u201d phase \u2013 neurons are fired by generative connections while recognition connections are being modified to increase probability that they would recreate the correct activity in the layer above \u2013 further to actual data from sensory input.[11]\nVariational Bayesian learning is based on probabilities. There is a chance that an approximation is performed with mistakes, damaging further data representations. Another downside pertains to complicated or corrupted data samples, making it difficult to infer a representational pattern.\nThe wake-sleep algorithm has been suggested not to be powerful enough for the layers of the inference network in order to recover a good estimator of the posterior distribution of latent variables.[12]", 
    "name": "Wake Sleep Algorithm"
}