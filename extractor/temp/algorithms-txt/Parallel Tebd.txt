ABOUT
The parallel-TEBD is a version of the TEBD algorithm adapted to run on multiple hosts. The task of parallelizing TEBD could be achieved in various ways.
FULL TEXT
The parallel-TEBD is a version of the TEBD algorithm adapted to run on multiple hosts. The task of parallelizing TEBD could be achieved in various ways.
This article introduces the conceptual basis of the implementation, using MPI-based pseudo-code for exemplification, while not restricting itself to MPI - the same basic schema could be implemented with the use of home-grown messaging routines.
The TEBD algorithm is a good candidate for parallel computing because the exponential operators used to calculate the time-evolution factorize under the Suzuki-Trotter expansion. A detailed presentation of the way TEBD works is given in the main article. Here we concern ourselves only with its parallel implementation.
For our purposes, we will use the canonical form of the MPS as introduced by Guifré Vidal in his original papers. Hence, we will write the function of state 




|

Ψ
⟩


{\displaystyle |\Psi \rangle }

 as:





|

Ψ
⟩
=

∑


i

1


,
.
.
,

i

N


=
1


M



∑


α

1


,
.
.
,

α

N
−
1


=
0


χ



Γ


α

1




[
1
]

i

1





λ


α

1




[
1
]



Γ


α

1



α

2




[
2
]

i

2





λ


α

2




[
2
]



Γ


α

2



α

3




[
3
]

i

3





λ


α

3




[
3
]


⋅
.
.
⋅

Γ


α

N
−
2



α

N
−
1




[

N
−
1

]

i

N
−
1





λ


α

N
−
1




[
N
−
1
]



Γ


α

N
−
1




[
N
]

i

N





|



i

1


,

i

2


,
.
.
,

i

N
−
1


,

i

N



⟩


{\displaystyle |\Psi \rangle =\sum \limits _{i_{1},..,i_{N}=1}^{M}\sum \limits _{\alpha _{1},..,\alpha _{N-1}=0}^{\chi }\Gamma _{\alpha _{1}}^{[1]i_{1}}\lambda _{\alpha _{1}}^{[1]}\Gamma _{\alpha _{1}\alpha _{2}}^{[2]i_{2}}\lambda _{\alpha _{2}}^{[2]}\Gamma _{\alpha _{2}\alpha _{3}}^{[3]i_{3}}\lambda _{\alpha _{3}}^{[3]}\cdot ..\cdot \Gamma _{\alpha _{N-2}\alpha _{N-1}}^{[{N-1}]i_{N-1}}\lambda _{\alpha _{N-1}}^{[N-1]}\Gamma _{\alpha _{N-1}}^{[N]i_{N}}|{i_{1},i_{2},..,i_{N-1},i_{N}}\rangle }


This function describes a N-point lattice which we would like to compute on P different compute nodes. Let us suppose, for the sake of simplicity, that N=2k*P, where k is an integer number. This means that if we distribute the lattice points evenly among the compute nodes (the easiest scenario), an even number of lattice points 2k is assigned to each compute node. Indexing the lattice points from 0 to N-1 (note that the usual indexing is 1,N) and the compute nodes from 0 to P-1, the lattice points would be distributed as follows among the nodes:
Using the canonical form of the MPS, we define 




λ


α

l




[
l
]




{\displaystyle \lambda _{\alpha _{l}}^{[l]}}

 as "belonging" to node m if m*2k ≤ l ≤ (m+1)*2k - 1. Similarly, we use the index l to assign the 





Γ

′

s


{\displaystyle {\Gamma }'s}

 to a certain lattice point. This means that 




Γ


α

0




[
0
]

i

0






{\displaystyle \Gamma _{\alpha _{0}}^{[0]i_{0}}}

 and 




Γ


α

l
−
1



α

l




[
l
]

i

l




,
l
=
1
,
2
k
−
1


{\displaystyle \Gamma _{\alpha _{l-1}\alpha _{l}}^{[l]i_{l}},l=1,2k-1}

, belong to NODE 0, as well as 




λ


α

l




[
l
]


,
l
=
0
,
2
k
−
2


{\displaystyle \lambda _{\alpha _{l}}^{[l]},l=0,2k-2}

. A parallel version of TEBD implies that the computing nodes need to exchange information among them. The information exchanged will be the MPS matrices and singular values lying at the border between neighbouring compute nodes. How this is done, it will be explained below.
The TEBD algorithm divides the exponential operator performing the time-evolution into a sequence of two-qubit gates of the form:





e




−
i
δ

ℏ



H

k
,
k
+
1




.


{\displaystyle e^{{\frac {-i\delta }{\hbar }}H_{k,k+1}}.}


Setting the Planck constant to 1, the time-evolution is expressed as:





|

Ψ
(
t
+
δ
)
⟩
=

e


−
i
δ



F
2





e


−
i
δ

G



e


−
i
δ



F
2





|

Ψ
(
t
)
⟩
,


{\displaystyle |\Psi (t+\delta )\rangle =e^{{-i\delta }{\frac {F}{2}}}e^{{-i\delta }G}e^{{-i\delta }{\frac {F}{2}}}|\Psi (t)\rangle ,}


where H = F + G,




F
≡

∑

k
=
0




N
2


−
1


(

H

2
k
,
2
k
+
1


)
=

∑

k
=
0




N
2


−
1


(

F

2
k


)
,


{\displaystyle F\equiv \sum _{k=0}^{{\frac {N}{2}}-1}(H_{2k,2k+1})=\sum _{k=0}^{{\frac {N}{2}}-1}(F_{2k}),}






G
≡

∑

k
=
0




N
2


−
2


(

H

2
k
+
1
,
2
k
+
2


)
=

∑

k
=
0




N
2


−
2


(

G

2
k
+
1


)
.


{\displaystyle G\equiv \sum _{k=0}^{{\frac {N}{2}}-2}(H_{2k+1,2k+2})=\sum _{k=0}^{{\frac {N}{2}}-2}(G_{2k+1}).}


What we can explicitly compute in parallel is the sequence of gates 




e


−
i



δ
2



F

2
k




,

e


−
i
δ



G

2
k
+
1





.


{\displaystyle e^{{-i}{\frac {\delta }{2}}F_{2k}},e^{{-i\delta }{G_{2k+1}}}.}

 Each of the compute node can apply most of the two-qubit gates without needing information from its neighbours. The compute nodes need to exchange information only at the borders, where two-qubit gates cross them, or just need information from the other side. We will now consider all three sweeps, two even and one odd and see what information has to be exchanged. Let us see what is happening on node m during the sweeps.
The sequence of gates that has to be applied in this sweep is:





e


−
i



δ
2



F

m
∗
2
k




,

e


−
i



δ
2



F

m
∗
2
k
+
2




,
.
.
.
,

e


−
i



δ
2



F

(
m
+
1
)
∗
2
k
−
2






{\displaystyle e^{{-i}{\frac {\delta }{2}}F_{m*2k}},e^{{-i}{\frac {\delta }{2}}F_{m*2k+2}},...,e^{{-i}{\frac {\delta }{2}}F_{(m+1)*2k-2}}}


Already for computing the first gate, process m needs information from its lowest neighbour, m-1. On the other side, m doesn't need anything from its "higher" neighbour, m+1, because it has all the information it needs to apply the last gate. So the best strategy for m is to send a request to m-1, postponing the calculation of the first gate for later, and continue with the calculation of the other gates. What m does is called non-blocking communication. Let's look at this in detail. The tensors involved in the calculation of the first gate are:[1]