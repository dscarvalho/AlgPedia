ABOUT
In mathematics, the resultant of two polynomials is a polynomial expression of their coefficients, which is equal to zero if and only if the polynomials have a common root (possibly in a field extension), or, equivalently, a common factor (over their field of coefficients). In some older texts, the resultant is also called eliminant.[1]
FULL TEXT
In mathematics, the resultant of two polynomials is a polynomial expression of their coefficients, which is equal to zero if and only if the polynomials have a common root (possibly in a field extension), or, equivalently, a common factor (over their field of coefficients). In some older texts, the resultant is also called eliminant.[1]
The resultant is widely used in number theory, either directly or through the discriminant, which is essentially the resultant of a polynomial and its derivative. The resultant of two polynomials with rational or polynomial coefficients may be computed efficiently on a computer. It is a basic tool of computer algebra, and is a built-in function of most computer algebra systems. It is used, among others, for cylindrical algebraic decomposition, integration of rational functions and drawing of curves defined by a bivariate polynomial equation.
The resultant of n homogeneous polynomials in n variables or multivariate resultant, sometimes called Macaulay's resultant, is a generalization, introduced by Macaulay, of the usual resultant.[2] It is, with Gröbner bases, one of the main tools of effective elimination theory (elimination theory on computers).


The resultant of two univariate polynomials A and B is commonly denoted 



res
⁡
(
A
,
B
)


{\displaystyle \operatorname {res} (A,B)}

 or 



Res
⁡
(
A
,
B
)
.


{\displaystyle \operatorname {Res} (A,B).}


In many applications of the resultant, the polynomials depend on several indeterminates and may be considered as univariate polynomials in one of their indeterminates, with polynomials in the other indeterminates as coefficients. In this case, the indeterminate that is selected for defining and computing the resultant is indicated as a subscript: 




res

x


⁡
(
A
,
B
)


{\displaystyle \operatorname {res} _{x}(A,B)}

 or 




Res

x


⁡
(
A
,
B
)
.


{\displaystyle \operatorname {Res} _{x}(A,B).}


The degree of the polynomials are used in the definition of the resultant. However, a polynomial of degree d may also be considered as a polynomial of higher degree such the leading coefficients are zero. If such a higher degree is used for the resultant, it is usually indicated as a subscript or a superscript, such as 




res

d
,
e


⁡
(
A
,
B
)


{\displaystyle \operatorname {res} _{d,e}(A,B)}

 or 




res

x


d
,
e


⁡
(
A
,
B
)
.


{\displaystyle \operatorname {res} _{x}^{d,e}(A,B).}


The resultant of two univariate polynomials over a field or over a commutative ring is commonly defined as the determinant of their Sylvester matrix. More precisely, let
and
be nonzero polynomials of respective degrees d and e. Let us denote by 






P



i




{\displaystyle {\mathcal {P}}_{i}}

 the vector space (or free module if the coefficients belong to a commutative ring) of dimension i whose elements are the polynomials of degree less than i. The map
such that
is a linear map between two spaces of the same dimension. Over the basis of the powers of x, this map is represented by a square matrix of dimension d + e, which called the Sylvester matrix of A and B (for many authors and in the article Sylvester matrix, the Sylvester matrix is defined as the transpose of this matrix; this convention is not used here, as it breaks the usual convention for writing the matrix of a linear map).
The resultant of A and B is thus the determinant
which has e columns of ai and d columns of bj (for simplification, d = e in the displayed determinant).
In the case of monic polynomials over an integral domain the resultant is equal to the product
where x and y run over the roots of the polynomials over an algebraically closed field containing the coefficients. For non-monic polynomials with leading coefficients a0 and b0 , respectively, the above product is multiplied by 




a

0


e



b

0


d


.


{\displaystyle a_{0}^{e}b_{0}^{d}.}


In this section and its subsections, A and B are two polynomials in x of respective degrees d and e, and their resultant is denoted 



res
⁡
(
A
,
B
)
.


{\displaystyle \operatorname {res} (A,B).}


Let A and B be two polynomials of respective degrees d and e with coefficients in a commutative ring R, and 



φ
:
R
→
S


{\displaystyle \varphi \colon R\to S}

 a ring homomorphism of R into another commutative ring S. Applying 



φ


{\displaystyle \varphi }

 to the coefficients of a polynomial extends 



φ


{\displaystyle \varphi }

 to a homomorphism of polynomial rings 



R
[
x
]
→
S
[
x
]


{\displaystyle R[x]\to S[x]}

, which is also denoted 



φ
.


{\displaystyle \varphi .}

 With this notation, we have:
These properties are easily deduced from the definition of the resultant as a determinant. They are mainly used in two situations. For computing a resultant of polynomials with integer coefficients, it is generally faster to compute it modulo several primes and to retrieve the desired resultant with Chinese remainder theorem. When R is a polynomial ring in other indeterminates, and S is the ring obtained by specializing to numerical values some or all indeterminates of R, these properties may be restated as if the degrees are preserved by the specialization, the resultant of the specialization of two polynomials is the specialization of the resultant. This property is fundamental, for example, for cylindrical algebraic decomposition.
This means that the property of the resultant being zero is invariant under linear and projective changes of the variable
These properties imply that in Euclidean algorithm for polynomials, the resultant of two successive remainders differs from the resultant of the initial polynomials by a factor, which is easy to compute. Moreover, the constant a in above second formula may be chosen in order that the successive remainders have their coefficients in the ring of coefficients of input polynomials. This is the starting idea of the subresultant-pseudo-remainder-sequence algorithm for computing the greatest common divisor and the resultant of two polynomials. This algorithms works for polynomials over the integers or, more generally, over an integral domain, without any other division than exact divisions (that is without involving fractions). It involves 



O
(
d
e
)


{\displaystyle O(de)}

 arithmetic operations, while the computation of the determinant of the Sylvester matrix with standard algorithms require 



O
(
(
d
+
e

)

3


)


{\displaystyle O((d+e)^{3})}

 arithmetic operations.
In this section, we consider two polynomials
and
whose d + e +2 coefficients are distinct indeterminates. Let
be the polynomial ring over the integers defined by these indeterminates. The resultant 



res
⁡
(
A
,
B
)


{\displaystyle \operatorname {res} (A,B)}

 is often called the generic resultant for the degrees d and e. It has the following properties.
The generic resultant for the degrees d and e is homogeneous in various ways. More precisely:
Let 



I
=
⟨
A
,
B
⟩


{\displaystyle I=\langle A,B\rangle }

 be the ideal generated by two polynomials A and B in a polynomial ring 



R
[
x
]
,


{\displaystyle R[x],}

 where R is itself a polynomial ring over a field. Then:
An example where k > 1 in the latter property is 



R
=


R


[
y
]
,


{\displaystyle R={\mathbb {R}}[y],}

 



A
=

x

2


+

y

2


−
1


{\displaystyle A=x^{2}+y^{2}-1}

 (the unit circle), and B = y − 2. In this case, 




res

x


⁡
(
A
,
B
)
=
(
y
−
2

)

2




{\displaystyle \operatorname {res} _{x}(A,B)=(y-2)^{2}}

 and 



⟨
A
,
B
⟩
∩
R
=
R
⋅
(
y
−
2
)
.


{\displaystyle \langle A,B\rangle \cap R=R\cdot (y-2).}

 This example has been chosen for having a prime ideal 



⟨
A
,
B
⟩
.


{\displaystyle \langle A,B\rangle .}

 For another field of coefficients and another constant term in B, one has also k > 1, but the ideal may be non-prime.
Theoretically, the resultant could be computed by using the formula expressing it as a product of roots differences. However, as the roots may generally not be computed exactly, such an algorithm would be inefficient and numerically unstable. As the resultant is a symmetric function of the roots of each polynomial, it could also be computed by using the fundamental theorem of symmetric polynomials, but this would be highly inefficient.
As the resultant is the determinant of the Sylvester matrix (and of the Bézout matrix), it may be computed by using any algorithm for computing determinants. This needs 



O
(

n

3


)


{\displaystyle O(n^{3})}

 arithmetic operations. As one knows algorithms with a better complexity (see below), this method is not used in practice.
It follows from § Invariance under change of polynomials that the computation of a resultant is strongly related with Euclidean algorithm for polynomials. This shows that the computation of the resultant of two polynomials of degrees d and e may be done in 



O
(
d
e
)


{\displaystyle O(de)}

 arithmetic operations in the field of coefficients.
However, when the coefficients are integers, rational numbers or polynomials, these arithmetic operations imply a number of GCD computations of coefficients which is of the same order and make the algorithm inefficient. The subresultant pseudo-remainder sequences were introduced to solve this problem and avoid any fraction and any GCD computation of coefficients. A more efficient algorithm is obtained by using the good behavior of the resultant under a ring homomorphism on the coefficients: to compute a resultant of two polynomials with integer coefficients, one computes their resultants modulo sufficiently many prime numbers and then reconstructs the result with the Chinese remainder theorem.
The use of fast multiplication of integers and polynomials allows algorithms for resultants and greatest common divisors that have a better time complexity, which is of the order of the complexity of the multiplication, multiplied by the logarithm of the size of the input (



log
⁡
(
s
(
d
+
e
)
)
,


{\displaystyle \log(s(d+e)),}

 where s is an upper bound of the number of digits of the input polynomials).
Resultants were introduced for solving systems of polynomial equations and provides the oldest proof that there exist algorithms for solving such systems. There are primarily intended for systems of two equations in two unknown, but allow also solving general systems.
Let us consider two polynomials the system of equations
where P and Q are polynomials of respective total degrees d and e. Then 



R
=

res

y


d
,
e


⁡
(
P
,
Q
)


{\displaystyle R=\operatorname {res} _{y}^{d,e}(P,Q)}

 is a polynomial in x, which is generically of degree de (by properties of § Homogeneity). A value 



α


{\displaystyle \alpha }

 of x is a root of R, if and only if, either there exist 



β


{\displaystyle \beta }

 in an algebraically closed field containing the coefficients, such that 



P
(
α
,
β
)
=
Q
(
α
,
β
)
=
0


{\displaystyle P(\alpha ,\beta )=Q(\alpha ,\beta )=0}

, or 



deg
⁡
(
P
(
α
,
y
)
)
<
d


{\displaystyle \deg(P(\alpha ,y))<d}

 and 



deg
⁡
(
Q
(
α
,
y
)
)
<
d


{\displaystyle \deg(Q(\alpha ,y))<d}

 (in this case, one says that P and Q have a common root at infinity for 



x
=
α


{\displaystyle x=\alpha }

).
Therefore, solving the system amounts computing the roots of R, and for each root 



α
,


{\displaystyle \alpha ,}

 computing the common root(s) of 



P
(
α
,
y
)
,


{\displaystyle P(\alpha ,y),}

 



Q
(
α
,
y
)
,


{\displaystyle Q(\alpha ,y),}

 and 




res

x


⁡
(
P
,
Q
)
.


{\displaystyle \operatorname {res} _{x}(P,Q).}


It is worth to remark that Bézout's theorem results of the value of 



deg
⁡

(

res

y


d
,
e


⁡
(
P
,
Q
)
)

.


{\displaystyle \deg \left(\operatorname {res} _{y}^{d,e}(P,Q)\right).}


At first glance, it seems that resultants may be applied to a general polynomial system of equations
by computing the resultants of every pair 



(

P

i


,

P

j


)


{\displaystyle (P_{i},P_{j})}

 with respect to 




x

n




{\displaystyle x_{n}}

 for eliminating one unknown, and repeating the process until getting univariate polynomials. Unfortunately, this introduce many spurious solutions, which are difficult to remove.
A method, introduced at the end of 19th century, works as follows: introduce k − 1 new indeterminates 




U

2


,
…
,

U

k




{\displaystyle U_{2},\ldots ,U_{k}}

 and compute
This is a polynomial in 




U

2


,
…
,

U

k




{\displaystyle U_{2},\ldots ,U_{k}}

 whose coefficients are polynomials in 




x

1


,
…
,

x

n
−
1


,


{\displaystyle x_{1},\ldots ,x_{n-1},}

 which have the property that 




α

1


,
…
,

α

n
−
1




{\displaystyle \alpha _{1},\ldots ,\alpha _{n-1}}

 is a common zero of these polynomial coefficients, if and only if the univariate polynomials 




P

i


(

α

1


,
…
,

α

n
−
1


,

x

n


)


{\displaystyle P_{i}(\alpha _{1},\ldots ,\alpha _{n-1},x_{n})}

 have a common zero, possibly at infinity. This process may be iterated until finding univariate polynomials.
For getting a correct algorithm two complements have to be added to the method. Firstly, at each step, a linear change of variable may be needed in order that the degrees of the polynomials in the last variable are the same as their total degree. Secondly, if, at any step, the resultant is zero, this means that the polynomials have a common factor and that the solutions split in two components. One, were the common factor is zero, and the other which is obtained by factoring out this common factor before continuing.
This algorithm is very complicated and has a huge time complexity. Therefore, its interest is mainly historical.
The discriminant of a polynomial, which is a fundamental tool in number theory is the quotient by its leading coefficient of the resultant of the polynomial and its derivative.
If x and y are algebraic numbers such that 



P
(
x
)
=
Q
(
y
)
=
0
,


{\displaystyle P(x)=Q(y)=0,}

 with degree Q of degree n), then 



z
=
x
+
y


{\displaystyle z=x+y}

 is a root of the resultant 




res

x


⁡
(
P
(
x
)
,
Q
(
z
−
x
)
)
,


{\displaystyle \operatorname {res} _{x}(P(x),Q(z-x)),}

 and 



t
=
x
y


{\displaystyle t=xy}

 is a root of 




res

x


⁡
(
P
(
x
)
,

x

n


Q
(
t

/

x
)
)
.


{\displaystyle \operatorname {res} _{x}(P(x),x^{n}Q(t/x)).}

 Combined with the fact that 



1

/

y


{\displaystyle 1/y}

 is a root of 




y

n


Q
(
1

/

y
)


{\displaystyle y^{n}Q(1/y)}

, this shows that the set of algebraic numbers is a field.
Let 



K
(
α
)


{\displaystyle K(\alpha )}

 be an algebraic field extension generated by an element 



α
,


{\displaystyle \alpha ,}

 which has 



P
(
x
)


{\displaystyle P(x)}

 as minimal polynomial. Every element of 



β
∈
K
(
α
)


{\displaystyle \beta \in K(\alpha )}

 may be written as 



β
=
Q
(
α
)
,


{\displaystyle \beta =Q(\alpha ),}

 where Q is a polynomial. Then 



β


{\displaystyle \beta }

 is a root of 




res

x


⁡
(
P
(
x
)
,
z
−
Q
(
x
)
)
,


{\displaystyle \operatorname {res} _{x}(P(x),z-Q(x)),}

 and this resultant is a power of the minimal polynomial of 



β
.


{\displaystyle \beta .}


Given two plane algebraic curves defined as the zeros of the polynomials P(x, y) and Q(x, y), the resultant allows the computation of their intersection. More precisely, the roots of 




res

y


⁡
(
P
,
Q
)


{\displaystyle \operatorname {res} _{y}(P,Q)}

 are the x-coordinates of the intersection points and of the common vertical asymptotes, and the roots of 




res

x


⁡
(
P
,
Q
)


{\displaystyle \operatorname {res} _{x}(P,Q)}

 are the y-coordinates of the intersection points and of the common horizontal asymptotes.
A rational plane curve may be defined by a parametric equation
where P, Q and R are polynomials. An implicit equation of the curve is given by
The degree of this curve is the highest degree of P, Q and R, which is equal to the total degree of the resultant.
In symbolic integration, for computing the antiderivative of a rational fraction, one uses partial fraction decomposition for decomposing the integral into a "rational part", which is a sum of rational fractions whose antiprimitives are rational fractions, and a "logarithmic part" which is a sum of rational fractions of the form
where Q is a square-free polynomial and P is a polynomial of lower degree than Q. The antiderivative of such a function involves necessarily logarithms, and generally algebraic numbers (the roots of Q). In fact, the antiderivative is
where the sum runs over all complex roots of Q.
The number of algebraic numbers involved by this expression is generally equal to the degree of Q, but it occurs frequently that an expression with less algebraic numbers may be computed. The Lazard–Rioboo–Trager method produced an expression, where the number of algebraic numbers is minimal, without any computation with algebraic numbers.
Let
be the square-free factorization of the resultant which appears on the right. Trager proved that the antiderivative is
where the internal sums run over the roots of the 




S

i




{\displaystyle S_{i}}

 (if 




S

i


=
1
)


{\displaystyle S_{i}=1)}

 the sum is zero, as being the empty sum), and 




T

i


(
r
,
x
)


{\displaystyle T_{i}(r,x)}

 is a polynomial of degree i in x. The Lazard-Rioboo contribution is the proof that 




T

i


(
r
,
x
)


{\displaystyle T_{i}(r,x)}

 is the subresultant of degree i of 



r

Q
′

(
x
)
−
P
(
x
)


{\displaystyle rQ'(x)-P(x)}

 and 



Q
(
x
)
.


{\displaystyle Q(x).}

 It is thus obtained for free if the resultant is computed by the subresultant pseudo-remainder sequence.
All preceding applications, and many others, show that the resultant is a fundamental tool in computer algebra. In fact most computer algebra systems include an efficient implementation of the computation of resultants.
The resultant is also defined for two homogeneous polynomial in two indeterminates. Given two homogeneous polynomials P(x, y) and Q(x, y) of respective total degrees p and q, their homogeneous resultant is the determinant of the matrix over the monomial basis of the linear map
where A runs over the bivariate homogeneous polynomials of degree q − 1, and B runs over the homogeneous polynomials of degree p − 1. In other words, the homogeneous resultant of P and Q is the resultant of P(x, 1) and Q(x, 1 when they are considered as polynomials of degree p and q (their degree in x may be lower than their total degree):
(The capitalization of "Res" is used here for distinguishing the two resultants, although there is no standard rule for the capitalization of the abbreviation).
The homogeneous resultant has essentially the same properties as the usual resultant, with essentially two differences: instead of polynomial roots, one considers zeros in the projective line, and the degree of a polynomial may not change under a ring homomorphism. That is:
Any property of the usual resultant may similarly extended to the homogeneous resultant, and the resulting property is either very similar or simpler than the corresponding property of the usual resultant.
Macaulay's resultant, named after Francis Sowerby Macaulay, also called the multivariate resultant, or the multipolynomial resultant,[3] is a generalizationof the homogeneous resultant to n homogeneous polynomials in n indeterminates. Macaulay's resultant is a polynomial in the coefficients of these n homogeneous polynomials that vanishes if and only if the polynomials have a common non-zero solution in an algebraically closed field containing containing the coefficients, or, equivalently, if the n hyper surfaces defined by the polynomials have a common zero in the n –1 dimensional projective space. The multivariate resultant is, with Gröbner bases, one of the main tools of effective elimination theory (elimination theory on computers).
Like the homogeneous resultant, Macaulay's may be defined with determinants, and thus behaves well under ring homomorphisms. However, it cannot be defined by a single determinant. It follows that it is easier to define it first on generic polynomials.
A homogeneous polynomial of degree d in n variables may have up to
coefficients; it is said to be generic, if these coefficients are distinct indeterminates.
Let 




P

1


,
…
,

P

n




{\displaystyle P_{1},\ldots ,P_{n}}

 be n generic homogeneous polynomials in n indeterminates, of respective degrees 




d

1


,
…
,

d

n


.


{\displaystyle d_{1},\dots ,d_{n}.}

 Together, they involve
indeterminate coefficients. Let C be the polynomial ring over the integers, in all these indeterminate coefficients. The polynomials 




P

1


,
…
,

P

n




{\displaystyle P_{1},\ldots ,P_{n}}

 belong thus to 



C
[

x

1


,
…
,

x

n


]
,


{\displaystyle C[x_{1},\ldots ,x_{n}],}

 and their resultant (still to be defined) belongs to C.
The Macaulay degree is the integer 



D
=

d

1


+
⋯
+

d

n


−
n
+
1
,


{\displaystyle D=d_{1}+\cdots +d_{n}-n+1,}

 which is fundamental in Macaulay's theory. For defining the resultant, one considers the Macaulay matrix, which is the matrix over the monomial basis of the C-linear map
in which each 




Q

i




{\displaystyle Q_{i}}

 runs over the homogeneous polynomials of degree 



D
−

d

i


,


{\displaystyle D-d_{i},}

 and the codomain is the C-module of the homogeneous polynomials of degree D.
If n = 2, the Macaulay matrix is the Sylvester matrix, and is a square matrix, but this is no longer true for n > 2. Thus, instead of considering the determinant, one considers all the maximal minors, that is the determinants of the square submatrices that have as many rows as the Macaulay matrix. Macaulay proved that the C-ideal generated by these principal minors is a principal ideal, which is generated by the greatest common divisor of these minors. As one is working with polynomials with integer coefficients, this greatest common divisor is defined up its sign. The generic Macaulay resultant is the greatest common divisor which becomes 1, when, for each i, zero is substituted for all coefficients of 




P

i


,


{\displaystyle P_{i},}

 except the coefficient of 




x

i



d

i




,


{\displaystyle x_{i}^{d_{i}},}

 for which one is substituted.
From now on, we consider that the homogeneous polynomials 




P

1


,
…
,

P

n


,


{\displaystyle P_{1},\ldots ,P_{n},}

 of degrees 




d

1


,
…
,

d

n




{\displaystyle d_{1},\ldots ,d_{n}}

 have their coefficients in a field k, that is that they belong to 



k
[

x

1


,
⋯
,

x

n


]
.


{\displaystyle k[x_{1},\cdots ,x_{n}].}

 Their resultant is defined as the element of k obtained by replacing in the generic resultant the indeterminate coefficients by the actual coefficients of the 




P

i


.


{\displaystyle P_{i}.}


The main property of the resultant is that it is zero if only if 




P

1


,
…
,

P

n


,


{\displaystyle P_{1},\ldots ,P_{n},}

 have a nonzero common zero in an algebraically closed extension of k.
The "only if" part of this theorem results on the last property of the preceding paragraph, and is an effective version of Projective Nullstellensatz: If the resultant is nonzerozero, then
where 



D
=

d

1


+
⋯
+

d

n


−
n
+
1


{\displaystyle D=d_{1}+\cdots +d_{n}-n+1}

 is the Macaulay degree, and 



⟨

x

1


,
…

x

n


⟩


{\displaystyle \langle x_{1},\ldots x_{n}\rangle }

 is the maximal homogeneous ideal. This implies that any common zero of 




P

1


,
…
,

P

n




{\displaystyle P_{1},\ldots ,P_{n}}

 is a common zero of 




x

1


,
…
,

x

n


.


{\displaystyle x_{1},\ldots ,x_{n}.}


As the computation of a resultant may reduced to computing determinants and polynomial greatest common divisors, there are algorithms for computing resultants in a finite number of steps.
However, the generic resultant is a polynomial of very high degree (exponential in n) depending on a huge number of indeterminates. It follows that, except for very small n and very small degrees of input polynomials, the generic resultant is, in practice, impossible to compute, even with modern computers. Moreover, the number of monomials of the generic resultant is so high, that, if it would be computable, the result could not be stored on available memory devices, even for rather small values of n and of the degrees of the input polynomials.
Therefore, computing the resultant makes sense only for polynomials whose coefficients belong to a field or are polynomials in few indeterminates over a field.
In the case of input polynomials with coefficients in a field, the exact value of the resultant is rarely important, only its equality (or not) to zero matters. As the resultant is zero if and only if the rank of the Macaulay matrix is lower than its number of its rows, this equality to zero may by tested by applying Gaussian elimination to the Macaulay matrix. This provides a computational complexity 




d

O
(
n
)


,


{\displaystyle d^{O(n)},}

 where d is the maximum degree of input polynomials.
Another case where the computation of the resultant may provide useful information is when the coefficients of the input polynomials are polynomials in a small number of indeterminates, often called parameters. In this case, the resultant, if not zero, defines a hypersurface in the parameter space. A point belongs to this hyper surface, if and only if there are values of 




x

1


,
…
,

x

n




{\displaystyle x_{1},\ldots ,x_{n}}

 which, together with the coordinates of the point are a zero of the input polynomials. In other words, the resultant is the result of the "elimination" of 




x

1


,
…
,

x

n




{\displaystyle x_{1},\ldots ,x_{n}}

 from the input polynomials.
Macaulay's resultant provides a method, called "U-resultant" by Macaulay, for solving systems of polynomial equations.
Given n − 1 homogeneous polynomials 




P

1


,
…
,

P

n
−
1


,


{\displaystyle P_{1},\ldots ,P_{n-1},}

 of degrees 




d

1


,
…
,

d

n
−
1


,


{\displaystyle d_{1},\ldots ,d_{n-1},}

 in n indeterminates 




x

1


,
…
,

x

n


,


{\displaystyle x_{1},\ldots ,x_{n},}

, over a field k, their U-resultant is the resultant of the n polynomials 




P

1


,
…
,

P

n
−
1


,

P

n


,


{\displaystyle P_{1},\ldots ,P_{n-1},P_{n},}

 where
is the generic linear form whose coefficients are new indeterminates 




u

1


,
…
,

u

n


.


{\displaystyle u_{1},\ldots ,u_{n}.}

 Notation 




u

i




{\displaystyle u_{i}}

 or 




U

i




{\displaystyle U_{i}}

 for these generic coefficients is traditional, and is the origin of the term U-resultant.
The U-resultant is a homogeneous polynomial in 



k
[

u

1


,
…
,

u

n


]
.


{\displaystyle k[u_{1},\ldots ,u_{n}].}

 It is zero if and only if the common zeros of 




P

1


,
…
,

P

n
−
1




{\displaystyle P_{1},\ldots ,P_{n-1}}

 form an projective algebraic set of positive dimension (that is, there are infinitely many projective zeros over an algebraically closed extension of k). If the U-resultant is not zero, its degree is the Bézout bound 




d

1


⋯

d

n
−
1


.


{\displaystyle d_{1}\cdots d_{n-1}.}

 The U-resultant factorizes over an algebraically closed extension of k into a product of linear forms. If 




α

1



u

1


+
…
+

α

n



u

n




{\displaystyle \alpha _{1}u_{1}+\ldots +\alpha _{n}u_{n}}

 is such a linear factor, then 




α

1


,
…
,

α

n




{\displaystyle \alpha _{1},\ldots ,\alpha _{n}}

 are the homogeneous coordinates of a common zero of 




P

1


,
…
,

P

n
−
1


.


{\displaystyle P_{1},\ldots ,P_{n-1}.}

 Moreover, every common zero may be obtained from one of these linear factors, and the multiplicity as a factor is equal to the intersection multiplicity of the 




P

i




{\displaystyle P_{i}}

 at this zero. In other words, the U-resultant provides a completely explicit version of Bézout's theorem.
The U-resultant, as defined by Macaulay was defined only for a number of homogeneous polynomials, which is one less than the number indeterminates. In 1981, Daniel Lazard provided the following generalization to any number of polynomials, which may be computed by a single Gaussian elimination.
Let 




P

1


,
…
,

P

k




{\displaystyle P_{1},\ldots ,P_{k}}

 be homogeneous polynomials in 




x

1


,
…
,

x

n


,


{\displaystyle x_{1},\ldots ,x_{n},}

 of degrees 




d

1


,
…
,

d

k


,


{\displaystyle d_{1},\ldots ,d_{k},}

 over a field k. Without generality, one may suppose that 




d

2


≥

d

3


≥
⋯
≥

d

k


≥

d

1


.


{\displaystyle d_{2}\geq d_{3}\geq \cdots \geq d_{k}\geq d_{1}.}

 Setting 




d

i


=
1


{\displaystyle d_{i}=1}

 for i > k, the Macaulay bound is 



D
=

d

1


+
⋯
,

d

n


−
n
+
1.


{\displaystyle D=d_{1}+\cdots ,d_{n}-n+1.}


Let 




u

1


,
…
,

u

n




{\displaystyle u_{1},\ldots ,u_{n}}

 be new indeterninates, and 




P

k
+
1


=

u

1



x

1


+
⋯
+

u

n



x

n


.


{\displaystyle P_{k+1}=u_{1}x_{1}+\cdots +u_{n}x_{n}.}

 In this case, the Macaulay matrix is the matrix, over the basis of the monomials in 




x

1


,
…
,

x

n


,


{\displaystyle x_{1},\ldots ,x_{n},}

 of the linear map
where, for each i, 




Q

i




{\displaystyle Q_{i}}

 runs over the homogeneous polynomials of degree 



D
−

d

i


.


{\displaystyle D-d_{i}.}


Gaussian elimination over k of the Macaulay matrix, provides, after removing zero rows and zero columns, either an empty matrix or a block diagonal square matrix. The result is the empty matrix if and only if 




P

1


,
…
,

P

k




{\displaystyle P_{1},\ldots ,P_{k}}

 have infinitely many common projective zeros over an algebraically closed extension of k. In the other case, the block diagonal matrix consists of two blocks. One is an identity matrix, and the other is a square matrix of linear forms in 




u

1


,
…
,

u

n


.


{\displaystyle u_{1},\ldots ,u_{n}.}

 The U-resultant is the determinant of the latter matrix, which is equal to the determinant of the block diagonal matrix.
As for the original U-resultant, this U-resultant factorizes into linear factors over an algebraically closed extension of k. The coefficients of these linear factors are the homogeneous coordinates of the common zeros of 




P

1


,
…
,

P

k


,


{\displaystyle P_{1},\ldots ,P_{k},}

 and the multiplicity of a common zero equals the multiplicity of the corresponding linear factor.
The number of rows of the Macaulay matrix is less than 



(
e
d

)

n


,


{\displaystyle (ed)^{n},}

 where e ~ 2.732 is the usual mathematical constant, and d is the arithmetic mean of the degrees of the 




P

i


.


{\displaystyle P_{i}.}

 It follows that, if k is not too large (say, if k ≤ 2n), a system of polynomial equations, that has a finite number of projective zeros, may be completely solved in time 




d

O
(
n
)


.


{\displaystyle d^{O(n)}.}


Although large, this bound is almost optimal. In fact, if all input degrees are equal, this implies that the time complexity is polynomial in the expected number of solutions (Bézout's theorem).