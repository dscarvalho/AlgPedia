ABOUT
The shifting nth root algorithm is an algorithm for extracting the nth root of a positive real number which proceeds iteratively by shifting in n digits of the radicand, starting with the most significant, and produces one digit of the root on each iteration, in a manner similar to long division.
FULL TEXT
The shifting nth root algorithm is an algorithm for extracting the nth root of a positive real number which proceeds iteratively by shifting in n digits of the radicand, starting with the most significant, and produces one digit of the root on each iteration, in a manner similar to long division.


Let B be the base of the number system you are using, and n be the degree of the root to be extracted. Let x be the radicand processed thus far, y be the root extracted thus far, and r be the remainder. Let α be the next n digits of the radicand, and β be the next digit of the root. Let x' be the new value of x for the next iteration, y' be the new value of y for the next iteration, and r' be the new value of r for the next iteration. These are all integers.
At each iteration, the invariant 




y

n


+
r
=
x


{\displaystyle y^{n}+r=x}

 will hold. The invariant 



(
y
+
1

)

n


>
x


{\displaystyle (y+1)^{n}>x}

 will hold. Thus y is the largest integer less than or equal to the nth root of x, and r is the remainder.
The initial values of x, y, and r should be 0. The value of α for the first iteration should be the most significant aligned block of n digits of the radicand. An aligned block of n digits means a block of digits aligned so that the decimal point falls between blocks. For example, in 123.4 the most significant aligned block of 2 digits is 01, the next most significant is 23, and the third most significant is 40.
On each iteration we shift in n digits of the radicand, so we have 




x
′

=

B

n


x
+
α


{\displaystyle x'=B^{n}x+\alpha }

 and we produce 1 digit of the root, so we have 




y
′

=
B
y
+
β


{\displaystyle y'=By+\beta }

. We want to choose β and r' so that the invariants described above hold. It turns out that there is always exactly one such choice, as will be proved below.
The first invariant says that:
or
So, pick the largest integer β such that
and let
Such a β always exists, since if 



β
=
0


{\displaystyle \beta =0}

 then the condition is 




B

n



y

n


≤

B

n


x
+
α


{\displaystyle B^{n}y^{n}\leq B^{n}x+\alpha }

, but 




y

n


≤
x


{\displaystyle y^{n}\leq x}

, so this is always true. Also, β must be less than B, since if 



β
=
B


{\displaystyle \beta =B}

 then we would have
but the second invariant implies that
and since 




B

n


x


{\displaystyle B^{n}x}

 and 




B

n


(
y
+
1

)

n




{\displaystyle B^{n}(y+1)^{n}}

 are both multiples of 




B

n




{\displaystyle B^{n}}

 the difference between them must be at least 




B

n




{\displaystyle B^{n}}

, and then we have
but 



0
≤
α
<

B

n




{\displaystyle 0\leq \alpha <B^{n}}

 by definition of α, so this can't be true, and 



(
B
y
+
β

)

n




{\displaystyle (By+\beta )^{n}}

 is a monotonically increasing function of β, so it can't be true for larger β either, so we conclude that there exists an integer γ with 



γ
<
B


{\displaystyle \gamma <B}

 such that an integer r' with 




r
′

≥
0


{\displaystyle r'\geq 0}

 exists such that the first invariant holds if and only if 



0
≤
β
≤
γ


{\displaystyle 0\leq \beta \leq \gamma }

.
Now consider the second invariant. It says:
or
Now, if β is not the largest admissible β for the first invariant as described above, then 



β
+
1


{\displaystyle \beta +1}

 is also admissible, and we have
This violates the second invariant, so to satisfy both invariants we must pick the largest β allowed by the first invariant. Thus we have proven the existence and uniqueness of β and r'.
To summarize, on each iteration:
Now, note that 



x
=

y

n


+
r


{\displaystyle x=y^{n}+r}

, so the condition
is equivalent to
and
is equivalent to
Thus, we don't actually need 



x


{\displaystyle x}

, and since 



r
=
x
−

y

n




{\displaystyle r=x-y^{n}}

 and 



x
<
(
y
+
1

)

n




{\displaystyle x<(y+1)^{n}}

, 



r
<
(
y
+
1

)

n


−

y

n




{\displaystyle r<(y+1)^{n}-y^{n}}

 or 



r
<
n

y

n
−
1


+
O
(

y

n
−
2


)


{\displaystyle r<ny^{n-1}+O(y^{n-2})}

, or 



r
<
n

x



n
−
1

n



+
O
(

x



n
−
2

n



)


{\displaystyle r<nx^{{n-1} \over n}+O(x^{{n-2} \over n})}

, so by using 



r


{\displaystyle r}

 instead of 



x


{\displaystyle x}

 we save time and space by a factor of 1/n. Also, the 




B

n



y

n




{\displaystyle B^{n}y^{n}}

 we subtract in the new test cancels the one in 



(
B
y
+
β

)

n




{\displaystyle (By+\beta )^{n}}

, so now the highest power of y we have to evaluate is 




y

n
−
1




{\displaystyle y^{n-1}}

 rather than 




y

n




{\displaystyle y^{n}}

.
As noted above, this algorithm is similar to long division, and it lends itself to the same notation:
Note that after the first iteration or two the leading term dominates the 



(
B
y
+
β

)

n


−

B

n



y

n




{\displaystyle (By+\beta )^{n}-B^{n}y^{n}}

, so we can get an often correct first guess at β by dividing 




B

n


r
+
α


{\displaystyle B^{n}r+\alpha }

 by 



n

B

n
−
1



y

n
−
1




{\displaystyle nB^{n-1}y^{n-1}}

.
On each iteration, the most time-consuming task is to select β. We know that there are B possible values, so we can find β using 



O
(
log
⁡
(
B
)
)


{\displaystyle O(\log(B))}

 comparisons. Each comparison will require evaluating 



(
B
y
+
β

)

n


−

B

n



y

n




{\displaystyle (By+\beta )^{n}-B^{n}y^{n}}

. In the kth iteration, y has k digits, and the polynomial can be evaluated with 



2
n
−
4


{\displaystyle 2n-4}

 multiplications of up to 



k
(
n
−
1
)


{\displaystyle k(n-1)}

 digits and 



n
−
2


{\displaystyle n-2}

 additions of up to 



k
(
n
−
1
)


{\displaystyle k(n-1)}

 digits, once we know the powers of y and β up through 



n
−
1


{\displaystyle n-1}

 for y and n for β. β has a restricted range, so we can get the powers of β in constant time. We can get the powers of y with 



n
−
2


{\displaystyle n-2}

 multiplications of up to 



k
(
n
−
1
)


{\displaystyle k(n-1)}

 digits. Assuming n-digit multiplication takes time 



O
(

n

2


)


{\displaystyle O(n^{2})}

 and addition takes time 



O
(
n
)


{\displaystyle O(n)}

, we take time 



O
(

k

2



n

2


)


{\displaystyle O(k^{2}n^{2})}

 for each comparison, or time 



O
(

k

2



n

2


log
⁡
(
B
)
)


{\displaystyle O(k^{2}n^{2}\log(B))}

 to pick β. The remainder of the algorithm is addition and subtraction that takes time 



O
(
k
)


{\displaystyle O(k)}

, so each iteration takes 



O
(

k

2



n

2


log
⁡
(
B
)
)


{\displaystyle O(k^{2}n^{2}\log(B))}

. For all k digits, we need time 



O
(

k

3



n

2


log
⁡
(
B
)
)


{\displaystyle O(k^{3}n^{2}\log(B))}

.
The only internal storage needed is r, which is 



O
(
k
)


{\displaystyle O(k)}

 digits on the kth iteration. That this algorithm doesn't have bounded memory usage puts an upper bound on the number of digits which can be computed mentally, unlike the more elementary algorithms of arithmetic. Unfortunately, any bounded memory state machine with periodic inputs can only produce periodic outputs, so there are no such algorithms which can compute irrational numbers from rational ones, and thus no bounded memory root extraction algorithms.
Note that increasing the base increases the time needed to pick β by a factor of 



O
(
log
⁡
(
B
)
)


{\displaystyle O(\log(B))}

, but decreases the number of digits needed to achieve a given precision by the same factor, and since the algorithm is cubic time in the number of digits, increasing the base gives an overall speedup of 



O
(

log

2


⁡
(
B
)
)


{\displaystyle O(\log ^{2}(B))}

. When the base is larger than the radicand, the algorithm degenerates to binary search, so it follows that this algorithm is not useful for computing roots with a computer, as it is always outperformed by much simpler binary search, and has the same memory complexity.