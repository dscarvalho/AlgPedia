ABOUT
Sequential decoding is a limited memory technique for decoding tree codes. Sequential decoding is mainly used is as an approximate decoding algorithm for long constraint-length convolutional codes. This approach may not be as accurate as the Viterbi algorithm but can save a substantial amount of computer memory.
FULL TEXT
Sequential decoding is a limited memory technique for decoding tree codes. Sequential decoding is mainly used is as an approximate decoding algorithm for long constraint-length convolutional codes. This approach may not be as accurate as the Viterbi algorithm but can save a substantial amount of computer memory.
Sequential decoding explores the tree code in such a way to try to minimise the computational cost and memory requirements to store the tree.
There is a range of sequential decoding approaches based on choice of metric and algorithm. Metrics include:
Algorithms include:


Given a partially explored tree (represented by a set of nodes which are limit of exploration), we would like to know the best node from which to explore further. The Fano metric (named after Robert Fano) allows one to calculate from which is the best node to explore further. This metric is optimal given no other constraints (e.g. memory).
For a binary symmetric channel (with error probability 



p


{\displaystyle p}

) the Fano metric can be derived via Bayes theorem. We are interested in following the most likely path 




P

i




{\displaystyle P_{i}}

 given an explored state of the tree 



X


{\displaystyle X}

 and a received sequence 





r




{\displaystyle {\mathbf {r} }}

. Using the language of probability and Bayes theorem we want to choose the maximum over 



i


{\displaystyle i}

 of:
We now introduce the following notation:
We express the likelihood 



Pr
(


r



|


P

i


,
X
)


{\displaystyle \Pr({\mathbf {r} }|P_{i},X)}

 as 




p


d

i




(
1
−
p

)


n

i


b
−

d

i





2

−
(
N
−

n

i


)
b




{\displaystyle p^{d_{i}}(1-p)^{n_{i}b-d_{i}}2^{-(N-n_{i})b}}

 (by using the binary symmetric channel likelihood for the first 




n

i


b


{\displaystyle n_{i}b}

 bits followed by a uniform prior over the remaining bits).
We express the prior 



Pr
(

P

i



|

X
)


{\displaystyle \Pr(P_{i}|X)}

 in terms of the number of branch choices one has made, 




n

i




{\displaystyle n_{i}}

, and the number of branches from each node, 




2

R
b




{\displaystyle 2^{Rb}}

.
Therefore:
We can equivalently maximise the log of this probability, i.e.
This last expression is the Fano metric. The important point to see is that we have two terms here: one based on the number of wrong bits and one based on the number of right bits. We can therefore update the Fano metric simply by adding 




log

2


⁡
p
+
1
−
R


{\displaystyle \log _{2}p+1-R}

 for each non-matching bit and 




log

2


⁡
(
1
−
p
)
+
1
−
R


{\displaystyle \log _{2}(1-p)+1-R}

 for each matching bit.
For sequential decoding to a good choice of decoding algorithm, the number of states explored wants to remain small (otherwise an algorithm which deliberately explores all states, e.g. the Viterbi algorithm, may be more suitable). For a particular noise level there is a maximum coding rate 




R

0




{\displaystyle R_{0}}

 called the computational cutoff rate where there is a finite backtracking limit. For the binary symmetric channel:
The simplest algorithm to describe is the "stack algorithm" in which the best 



N


{\displaystyle N}

 paths found so far are stored. Sequential decoding may introduce an additional error above Viterbi decoding when the correct path has 



N


{\displaystyle N}

 or more highly scoring paths above it; at this point the best path will drop off the stack and be no longer considered.
The famous Fano algorithm (named after Robert Fano) has a very low memory requirement and hence is suited to hardware implementations. This algorithm explores backwards and forward from a single point on the tree.