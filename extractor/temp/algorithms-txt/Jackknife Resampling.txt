ABOUT
In statistics, the jackknife is a resampling technique especially useful for variance and bias estimation. The jackknife predates other common resampling methods such as the bootstrap. The jackknife estimator of a parameter is found by systematically leaving out each observation from a dataset and calculating the estimate and then finding the average of these calculations. Given a sample of size 



N


{\displaystyle N}

, the jackknife estimate is found by aggregating the estimates of each 



N
−
1


{\displaystyle N-1}

 estimate in the sample.
FULL TEXT
In statistics, the jackknife is a resampling technique especially useful for variance and bias estimation. The jackknife predates other common resampling methods such as the bootstrap. The jackknife estimator of a parameter is found by systematically leaving out each observation from a dataset and calculating the estimate and then finding the average of these calculations. Given a sample of size 



N


{\displaystyle N}

, the jackknife estimate is found by aggregating the estimates of each 



N
−
1


{\displaystyle N-1}

 estimate in the sample.
The jackknife technique was developed by Maurice Quenouille (1949, 1956). John Tukey (1958) expanded on the technique and proposed the name "jackknife" since, like a physical jack-knife (a compact folding knife), it is a rough-and-ready tool that can improvise a solution for a variety of problems even though specific problems may be more efficiently solved with a purpose-designed tool.[1]
The jackknife is a linear approximation of the bootstrap.[1]


The jackknife estimate of a parameter can be found by estimating the parameter for each subsample omitting the ith observation to estimate the previously unknown value of a parameter (say 







x
¯




i




{\displaystyle {\bar {x}}_{i}}

).[2]
An estimate of the variance of an estimator can be calculated using the jackknife technique.
where 







x
¯




i




{\displaystyle {\bar {x}}_{i}}

 is the parameter estimate based on leaving out the ith observation, and 







x
¯





(
.
)



=


1
n



∑

i


n






x
¯




i




{\displaystyle {\bar {x}}_{\mathrm {(.)} }={\frac {1}{n}}\sum _{i}^{n}{\bar {x}}_{i}}

 is the estimator based on all of the subsamples.[3][4]
The jackknife technique can be used to estimate the bias of an estimator calculated over the entire sample. Say 






θ
^





{\displaystyle {\hat {\theta }}}

 is the calculated estimator of the parameter of interest based on all 




n



{\displaystyle {n}}

 observations. Let
where 







θ
^





(
i
)





{\displaystyle {\hat {\theta }}_{\mathrm {(i)} }}

 is the estimate of interest based on the sample with the ith observation removed, and 







θ
^





(
.
)





{\displaystyle {\hat {\theta }}_{\mathrm {(.)} }}

 is the average of these "leave-one-out" estimates. The jackknife estimate of the bias of 






θ
^





{\displaystyle {\hat {\theta }}}

 is given by:
and the resulting bias-corrected jackknife estimate of 



θ


{\displaystyle \theta }

 is given by:
This removes the bias in the special case that the bias is 



O
(

N

−
1


)


{\displaystyle O(N^{-1})}

 and to 



O
(

N

−
2


)


{\displaystyle O(N^{-2})}

 in other cases.[1]
This provides an estimated correction of bias due to the estimation method. The jackknife does not correct for a biased sample.