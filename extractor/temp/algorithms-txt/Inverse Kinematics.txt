ABOUT
Inverse kinematics refers to the use of the kinematics equations of a robot to determine the joint parameters that provide a desired position of the end-effector.[1] Specification of the movement of a robot so that its end-effector achieves a desired task is known as motion planning. Inverse kinematics transforms the motion plan into joint actuator trajectories for the robot.
FULL TEXT
Inverse kinematics refers to the use of the kinematics equations of a robot to determine the joint parameters that provide a desired position of the end-effector.[1] Specification of the movement of a robot so that its end-effector achieves a desired task is known as motion planning. Inverse kinematics transforms the motion plan into joint actuator trajectories for the robot.
The movement of a kinematic chain whether it is a robot or an animated character is modeled by the kinematics equations of the chain. These equations define the configuration of the chain in terms of its joint parameters. Forward kinematics uses the joint parameters to compute the configuration of the chain, and inverse kinematics reverses this calculation to determine the joint parameters that achieves a desired configuration.[2][3][4]
For example, inverse kinematics formulas allow calculation of the joint parameters that position a robot arm to pick up a part. Similar formulas determine the positions of the skeleton of an animated character that is to move in a particular way.


Kinematic analysis is one of the first steps in the design of most industrial robots. Kinematic analysis allows the designer to obtain information on the position of each component within the mechanical system. This information is necessary for subsequent dynamic analysis along with control paths.
Inverse kinematics is an example of the kinematic analysis of a constrained system of rigid bodies, or kinematic chain. The kinematic equations of a robot can be used to define the loop equations of a complex articulated system. These loop equations are non-linear constraints on the configuration parameters of the system. The independent parameters in these equations are known as the degrees of freedom of the system.
While analytical solutions to the inverse kinematics problem exist for a wide range of kinematic chains, computer modeling and animation tools often use Newton's method to solve the non-linear kinematics equations.
Other applications of inverse kinematic algorithms include interactive manipulation, animation control and collision avoidance.
Inverse kinematics is important to game programming and 3D animation, where it is used to connect game characters physically to the world, such as feet landing firmly on top of terrain.
An animated figure is modeled with a skeleton of rigid segments connected with joints, called a kinematic chain. The kinematics equations of the figure define the relationship between the joint angles of the figure and its pose or configuration. The forward kinematic animation problem uses the kinematics equations to determine the pose given the joint angles. The inverse kinematics problem computes the joint angles for a desired pose of the figure.
It is often easier for computer-based designers, artists and animators to define the spatial configuration of an assembly or figure by moving parts, or arms and legs, rather than directly manipulating joint angles. Therefore, inverse kinematics is used in computer-aided design systems to animate assemblies and by computer-based artists and animators to position figures and characters.
The assembly is modeled as rigid links connected by joints that are defined as mates, or geometric constraints. Movement of one element requires the computation of the joint angles for the other elements to maintain the joint constraints. For example, inverse kinematics allows an artist to move the hand of a 3D human model to a desired position and orientation and have an algorithm select the proper angles of the wrist, elbow, and shoulder joints. Successful implementation of computer animation usually also requires that the figure move within reasonable anthropomorphic limits.
An analytic solution to an inverse kinematics problem is a closed-form expression that takes the end-effector pose as input and gives joint positions as output, 



q
=
f
(
x
)


{\displaystyle q=f(x)}

. Analytical inverse kinematics solvers can be significantly faster than numerical solvers and provide more than one solution for a given end-effector pose.
The IKFast open-source program can solve for the complete analytical solutions of most common robot manipulators and generate C++ code for them. The generated solvers cover most degenerate cases and can finish in microseconds on recent computers.
There are many methods of modelling and solving inverse kinematics problems. The most flexible of these methods typically rely on iterative optimization to seek out an approximate solution, due to the difficulty of inverting the forward kinematics equation and the possibility of an empty solution space. The core idea behind several of these methods is to model the forward kinematics equation using a Taylor series expansion, which can be simpler to invert and solve than the original system.
The Jacobian inverse technique is a simple yet effective way of implementing inverse kinematics. Let there be 



m


{\displaystyle m}

 variables that govern the forward-kinematics equation, i.e. the position function. These variables may be joint angles, lengths, or other arbitrary real values. If the IK system lives in a 3-dimensional space, the position function can be viewed as a mapping 



p
(
x
)
:

ℜ

m


→

ℜ

3




{\displaystyle p(x):\Re ^{m}\rightarrow \Re ^{3}}

. Let 




p

0


=
p
(

x

0


)


{\displaystyle p_{0}=p(x_{0})}

 give the initial position of the system, and





p

1


=
p
(

x

0


+
Δ
x
)


{\displaystyle p_{1}=p(x_{0}+\Delta x)}


be the goal position of the system. The Jacobian inverse technique iteratively computes an estimate of 



Δ
x


{\displaystyle \Delta x}

 that minimizes the error given by 




|


|

p
(

x

0


+
Δ
x
)
−

p

0



|


|



{\displaystyle ||p(x_{0}+\Delta x)-p_{0}||}

.
For small 



Δ
x


{\displaystyle \Delta x}

-vectors, the series expansion of the position function gives:




p
(

x

1


)
≈
p
(

x

0


)
+

J

p


(

x

0


)
Δ
x


{\displaystyle p(x_{1})\approx p(x_{0})+J_{p}(x_{0})\Delta x}


Where 




J

p


(

x

0


)


{\displaystyle J_{p}(x_{0})}

 is the (3 x m) Jacobian matrix of the position function at 




x

0




{\displaystyle x_{0}}

.
Note that the (i, k)-th entry of the Jacobian matrix can be determined numerically:







∂

p

i




∂

x

k





≈




p

i


(

x

0
,
k


+
h
)
−

p

i


(

x

0


)

h




{\displaystyle {\frac {\partial p_{i}}{\partial x_{k}}}\approx {\frac {p_{i}(x_{0,k}+h)-p_{i}(x_{0})}{h}}}


Where 




p

i


(
x
)


{\displaystyle p_{i}(x)}

 gives the i-th component of the position function, 




x

0
,
k


+
h


{\displaystyle x_{0,k}+h}

 is simply 




x

0




{\displaystyle x_{0}}

 with a small delta added to its k-th component, and 



h


{\displaystyle h}

 is a reasonably small positive value.
Taking the Moore-Penrose pseudoinverse of the Jacobian (computable using a singular value decomposition) and re-arranging terms results in:




Δ
x
≈

J

p


+


(

x

0


)
Δ
p


{\displaystyle \Delta x\approx J_{p}^{+}(x_{0})\Delta p}


Where 



Δ
p
=
p
(

x

0


+
Δ
x
)
−
p
(

x

0


)


{\displaystyle \Delta p=p(x_{0}+\Delta x)-p(x_{0})}

.
Applying the inverse Jacobian method once will result in a very rough estimate of the desired 



Δ
x


{\displaystyle \Delta x}

-vector. A line search should be used to scale this 



Δ
x


{\displaystyle \Delta x}

 to an acceptable value. The estimate for 



Δ
x


{\displaystyle \Delta x}

 can be improved via the following algorithm (known as the Newton-Raphson method):




Δ

x

k
+
1


=

J

p


+


(

x

k


)
Δ

p

k




{\displaystyle \Delta x_{k+1}=J_{p}^{+}(x_{k})\Delta p_{k}}


Once some 



Δ
x


{\displaystyle \Delta x}

-vector has caused the error to drop close to zero, the algorithm should terminate. Existing methods based on the Hessian matrix of the system have been reported to converge to desired 



Δ
x


{\displaystyle \Delta x}

 values using fewer iterations, though, in some cases more computational resources.