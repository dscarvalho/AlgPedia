ABOUT
The Goertzel algorithm is a technique in digital signal processing (DSP) that provides a means for efficient evaluation of individual terms of the discrete Fourier transform (DFT), thus making it useful in certain practical applications, such as recognition of DTMF tones produced by the buttons pushed on a telephone keypad. The algorithm was first described by Gerald Goertzel in 1958.[1]
FULL TEXT
The Goertzel algorithm is a technique in digital signal processing (DSP) that provides a means for efficient evaluation of individual terms of the discrete Fourier transform (DFT), thus making it useful in certain practical applications, such as recognition of DTMF tones produced by the buttons pushed on a telephone keypad. The algorithm was first described by Gerald Goertzel in 1958.[1]
Like the DFT, the Goertzel algorithm analyses one selectable frequency component from a discrete signal.[2][3][4] Unlike direct DFT calculations, the Goertzel algorithm applies a single real-valued coefficient at each iteration, using real-valued arithmetic for real-valued input sequences. For covering a full spectrum, the Goertzel algorithm has a higher order of complexity than fast Fourier transform (FFT) algorithms, but for computing a small number of selected frequency components, it is more numerically efficient. The simple structure of the Goertzel algorithm makes it well suited to small processors and embedded applications, though not limited to these.
The Goertzel algorithm can also be used "in reverse" as a sinusoid synthesis function, which requires only 1 multiplication and 1 subtraction per generated sample.[5]


The main calculation in the Goertzel algorithm has the form of a digital filter, and for this reason the algorithm is often called a Goertzel filter. The filter operates on an input sequence 



x
[
n
]


{\displaystyle x[n]}

 in a cascade of two stages with a parameter 




ω

0




{\displaystyle \omega _{0}}

, giving the frequency to be analysed, normalised to radians per sample.
The first stage calculates an intermediate sequence, 



s
[
n
]


{\displaystyle s[n]}

:




s
[
n
]
=
x
[
n
]
+
2
cos
⁡
(

ω

0


)
s
[
n
−
1
]
−
s
[
n
−
2
]
.


{\displaystyle s[n]=x[n]+2\cos(\omega _{0})s[n-1]-s[n-2].}



 
 
 
 
(1)
The second stage applies the following filter to 



s
[
n
]


{\displaystyle s[n]}

, producing output sequence 



y
[
n
]


{\displaystyle y[n]}

:




y
[
n
]
=
s
[
n
]
−

e

−
j

ω

0




s
[
n
−
1
]
.


{\displaystyle y[n]=s[n]-e^{-j\omega _{0}}s[n-1].}



 
 
 
 
(2)
The first filter stage can be observed to be a second-order IIR filter with a direct-form structure. This particular structure has the property that its internal state variables equal the past output values from that stage. Input values 



x
[
n
]


{\displaystyle x[n]}

 for 



n
<
0


{\displaystyle n<0}

 are presumed all equal to 0. To establish the initial filter state so that evaluation can begin at sample 



x
[
0
]


{\displaystyle x[0]}

, the filter states are assigned initial values 



s
[
−
2
]
=
s
[
−
1
]
=
0


{\displaystyle s[-2]=s[-1]=0}

. To avoid aliasing hazards, frequency 




ω

0




{\displaystyle \omega _{0}}

 is often restricted to the range 0 to π (see Nyquist–Shannon sampling theorem); using a value outside this range is not meaningless, but is equivalent to using an aliased frequency inside this range, since the exponential function is periodic with a period of 2π in 




ω

0




{\displaystyle \omega _{0}}

.
The second-stage filter can be observed to be a FIR filter, since its calculations do not use any of its past outputs.
Z-transform methods can be applied to study the properties of the filter cascade. The Z transform of the first filter stage given in equation (1) is











S
(
z
)


X
(
z
)






=


1

1
−
2
cos
⁡
(

ω

0


)

z

−
1


+

z

−
2











=


1

(
1
−

e

+
j

ω

0





z

−
1


)
(
1
−

e

−
j

ω

0





z

−
1


)



.






{\displaystyle {\begin{aligned}{\frac {S(z)}{X(z)}}&={\frac {1}{1-2\cos(\omega _{0})z^{-1}+z^{-2}}}\\&={\frac {1}{(1-e^{+j\omega _{0}}z^{-1})(1-e^{-j\omega _{0}}z^{-1})}}.\end{aligned}}}



 
 
 
 
(3)
The Z transform of the second filter stage given in equation (2) is







Y
(
z
)


S
(
z
)



=
1
−

e

−
j

ω

0





z

−
1


.


{\displaystyle {\frac {Y(z)}{S(z)}}=1-e^{-j\omega _{0}}z^{-1}.}



 
 
 
 
(4)
The combined transfer function of the cascade of the two filter stages is then











S
(
z
)


X
(
z
)






Y
(
z
)


S
(
z
)



=



Y
(
z
)


X
(
z
)






=



(
1
−

e

−
j

ω

0





z

−
1


)


(
1
−

e

+
j

ω

0





z

−
1


)
(
1
−

e

−
j

ω

0





z

−
1


)









=


1

1
−

e

+
j

ω

0





z

−
1





.






{\displaystyle {\begin{aligned}{\frac {S(z)}{X(z)}}{\frac {Y(z)}{S(z)}}={\frac {Y(z)}{X(z)}}&={\frac {(1-e^{-j\omega _{0}}z^{-1})}{(1-e^{+j\omega _{0}}z^{-1})(1-e^{-j\omega _{0}}z^{-1})}}\\&={\frac {1}{1-e^{+j\omega _{0}}z^{-1}}}.\end{aligned}}}



 
 
 
 
(5)
This can be transformed back to an equivalent time-domain sequence, and the terms unrolled back to the first input term at index 



n
=
0


{\displaystyle n=0}

:[citation needed]








y
[
n
]



=
x
[
n
]
+

e

+
j

ω

0




y
[
n
−
1
]






=

∑

k
=
−
∞


n


x
[
k
]

e

+
j

ω

0


(
n
−
k
)








=

e

j

ω

0


n



∑

k
=
0


n


x
[
k
]

e

−
j

ω

0


k




since 

∀
k
<
0
,
x
[
k
]
=
0.






{\displaystyle {\begin{aligned}y[n]&=x[n]+e^{+j\omega _{0}}y[n-1]\\&=\sum _{k=-\infty }^{n}x[k]e^{+j\omega _{0}(n-k)}\\&=e^{j\omega _{0}n}\sum _{k=0}^{n}x[k]e^{-j\omega _{0}k}\qquad {\text{since }}\forall k<0,x[k]=0.\end{aligned}}}



 
 
 
 
(6)
It can be observed that the poles of the filter's Z transform are located at 




e

+
j

ω

0






{\displaystyle e^{+j\omega _{0}}}

 and 




e

−
j

ω

0






{\displaystyle e^{-j\omega _{0}}}

, on a circle of unit radius centered on the origin of the complex Z-transform plane. This property indicates that the filter process is marginally stable and vulnerable to numerical-error accumulation when computed using low-precision arithmetic and long input sequences.[6] A numerically stable version was proposed by Christian Reinsch.[7]
For the important case of computing a DFT term, the following special restrictions are applied.





ω

0


=
2
π


k
N


.


{\displaystyle \omega _{0}=2\pi {\frac {k}{N}}.}



 
 
 
 
(7)




k
∈
{
0
,
1
,
2
,
.
.
.
,
N
−
1
}
.


{\displaystyle k\in \{0,1,2,...,N-1\}.}



 
 
 
 
(8)
Making these substitutions into equation (6) and observing that the term 




e

+
j
2
π
k


=
1


{\displaystyle e^{+j2\pi k}=1}

, equation (6) then takes the following form:




y
[
N
]
=

∑

n
=
0


N


x
[
n
]

e

−
j
2
π



n
k

N




.


{\displaystyle y[N]=\sum _{n=0}^{N}x[n]e^{-j2\pi {\frac {nk}{N}}}.}



 
 
 
 
(9)
We can observe that the right side of equation (9) is extremely similar to the defining formula for DFT term 



X
[
k
]


{\displaystyle X[k]}

, the DFT term for index number 



k


{\displaystyle k}

, but not exactly the same. The summation shown in equation (9) requires 



N
+
1


{\displaystyle N+1}

 input terms, but only 



N


{\displaystyle N}

 input terms are available when evaluating a DFT. A simple but inelegant expedient is to extend the input sequence 



x
[
n
]


{\displaystyle x[n]}

 with one more artificial value 



x
[
N
]
=
0


{\displaystyle x[N]=0}

.[8] We can see from equation (9) that the mathematical effect on the final result is the same as removing term 



x
[
N
]


{\displaystyle x[N]}

 from the summation, thus delivering the intended DFT value.
However, there is a more elegant approach that avoids the extra filter pass. From equation (1), we can note that when the extended input term 



x
[
N
]
=
0


{\displaystyle x[N]=0}

 is used in the final step,




s
[
N
]
=
2
cos
⁡
(

ω

0


)
s
[
N
−
1
]
−
s
[
N
−
2
]
.


{\displaystyle s[N]=2\cos(\omega _{0})s[N-1]-s[N-2].}



 
 
 
 
(10)
Thus, the algorithm can be completed as follows:
The last two mathematical operations are simplified by combining them algebraically:








y
[
N
]



=
s
[
N
]
−

e

−
j
2
π


k
N




s
[
N
−
1
]






=
(
2
cos
⁡
(

ω

0


)
s
[
N
−
1
]
−
s
[
N
−
2
]
)
−

e

−
j
2
π


k
N




s
[
N
−
1
]






=

e

j
2
π


k
N




s
[
N
−
1
]
−
s
[
N
−
2
]
.






{\displaystyle {\begin{aligned}y[N]&=s[N]-e^{-j2\pi {\frac {k}{N}}}s[N-1]\\&=(2\cos(\omega _{0})s[N-1]-s[N-2])-e^{-j2\pi {\frac {k}{N}}}s[N-1]\\&=e^{j2\pi {\frac {k}{N}}}s[N-1]-s[N-2].\end{aligned}}}



 
 
 
 
(11)
Note that stopping the filter updates at term 



N
−
1


{\displaystyle N-1}

 and immediately applying equation (2) rather than equation (11) misses the final filter state updates, yielding a result with incorrect phase.[9]
The particular filtering structure chosen for the Goertzel algorithm is the key to its efficient DFT calculations. We can observe that only one output value 



y
[
N
]


{\displaystyle y[N]}

 is used for calculating the DFT, so calculations for all the other output terms are omitted. Since the FIR filter is not calculated, the IIR stage calculations 



s
[
0
]
,
s
[
1
]


{\displaystyle s[0],s[1]}

, etc. can be discarded immediately after updating the first stage's internal state.
This seems to leave a paradox: to complete the algorithm, the FIR filter stage must be evaluated once using the final two outputs from the IIR filter stage, while for computational efficiency the IIR filter iteration discards its output values. This is where the properties of the direct-form filter structure are applied. The two internal state variables of the IIR filter provide the last two values of the IIR filter output, which are the terms required to evaluate the FIR filter stage.
Examining equation (6), a final IIR filter pass to calculate term 



y
[
N
]


{\displaystyle y[N]}

 using a supplemental input value 



x
[
N
]
=
0


{\displaystyle x[N]=0}

 applies a complex multiplier of magnitude 1 to the previous term 



y
[
N
−
1
]


{\displaystyle y[N-1]}

. Consequently, 



y
[
N
]


{\displaystyle y[N]}

 and 



y
[
N
−
1
]


{\displaystyle y[N-1]}

 represent equivalent signal power. It is equally valid to apply equation (11) and calculate the signal power from term 



y
[
N
]


{\displaystyle y[N]}

 or to apply equation (2) and calculate the signal power from term 



y
[
N
−
1
]


{\displaystyle y[N-1]}

. Both cases lead to the following expression for the signal power represented by DFT term 



X
[
k
]


{\displaystyle X[k]}

:








X
[
k
]


X
′

[
k
]



=
y
[
N
]


y
′

[
N
]
=
y
[
N
−
1
]


y
′

[
N
−
1
]






=

s

2


[
N
−
1
]
+

s

2


[
N
−
2
]
−
2
cos
⁡

(
2
π


k
N


)


s
[
N
−
1
]

s
[
N
−
2
]
.






{\displaystyle {\begin{aligned}X[k]\,X'[k]&=y[N]\,y'[N]=y[N-1]\,y'[N-1]\\&=s^{2}[N-1]+s^{2}[N-2]-2\cos \left(2\pi {\frac {k}{N}}\right)\,s[N-1]\,s[N-2].\end{aligned}}}



 
 
 
 
(12)
In the pseudocode below, the variables sprev and sprev2 temporarily store output history from the IIR filter, while x[n] is an indexed element of the array x, which stores the input.
It is possible[10] to organise the computations so that incoming samples are delivered singly to a software object that maintains the filter state between updates, with the final power result accessed after the other processing is done.
The case of real-valued input data arises frequently, especially in embedded systems where the input streams result from direct measurements of physical processes. Comparing to the illustration in the previous section, when the input data are real-valued, the filter internal state variables sprev and sprev2 can be observed also to be real-valued, consequently, no complex arithmetic is required in the first IIR stage. Optimizing for real-valued arithmetic typically is as simple as applying appropriate real-valued data types for the variables.
After the calculations using input term 



x
[
N
−
1
]


{\displaystyle x[N-1]}

, and filter iterations are terminated, equation (11) must be applied to evaluate the DFT term. The final calculation uses complex-valued arithmetic, but this can be converted into real-valued arithmetic by separating real and imaginary terms:









c

r





=
cos
⁡
(
2
π



k
N



)
,





c

i





=
sin
⁡
(
2
π



k
N



)
,




y
[
N
]



=

c

r


s
[
N
−
1
]
−
s
[
N
−
2
]
+
j

c

i


s
[
N
−
1
]
.






{\displaystyle {\begin{aligned}c_{r}&=\cos(2\pi {\tfrac {k}{N}}),\\c_{i}&=\sin(2\pi {\tfrac {k}{N}}),\\y[N]&=c_{r}s[N-1]-s[N-2]+jc_{i}s[N-1].\end{aligned}}}



 
 
 
 
(13)
Comparing to the power-spectrum application, the only difference are the calculation used to finish:
This application requires the same evaluation of DFT term 



X
[
k
]


{\displaystyle X[k]}

, as discussed in the previous section, using a real-valued or complex-valued input stream. Then the signal phase can be evaluated as




ϕ
=

tan

−
1


⁡



ℑ
(
X
[
k
]
)


ℜ
(
X
[
k
]
)



,


{\displaystyle \phi =\tan ^{-1}{\frac {\Im (X[k])}{\Re (X[k])}},}



 
 
 
 
(14)
taking appropriate precautions for singularities, quadrant, and so forth when computing the inverse tangent function.
Since complex signals decompose linearly into real and imaginary parts, the Goertzel algorithm can be computed in real arithmetic separately over the sequence of real parts, yielding 




y

r


[
n
]


{\displaystyle y_{\text{r}}[n]}

, and over the sequence of imaginary parts, yielding 




y

i


[
n
]


{\displaystyle y_{\text{i}}[n]}

. After that, the two complex-valued partial results can be recombined:




y
[
n
]
=

y

r


[
n
]
+
j

y

i


[
n
]
.


{\displaystyle y[n]=y_{\text{r}}[n]+jy_{\text{i}}[n].}



 
 
 
 
(15)
In the complexity order expressions, when the number of calculated terms 



M


{\displaystyle M}

 is smaller than 



log
⁡
N


{\displaystyle \log N}

, the advantage of the Goertzel algorithm is clear. But because FFT code is comparatively complex, the "cost per unit of work" factor 



K


{\displaystyle K}

 is often larger for an FFT, and the practical advantage favours the Goertzel algorithm even for 



M


{\displaystyle M}

 several times larger than 




log

2


⁡
(
N
)


{\displaystyle \log _{2}(N)}

.
As a rule-of-thumb for determining whether a radix-2 FFT or a Goertzel algorithm is more efficient, adjust the number of terms 



N


{\displaystyle N}

 in the data set upward to the nearest exact power of 2, calling this 




N

2




{\displaystyle N_{2}}

, and the Goertzel algorithm is likely to be faster if
FFT implementations and processing platforms have a significant impact on the relative performance. Some FFT implementations[11] perform internal complex-number calculations to generate coefficients on-the-fly, significantly increasing their "cost K per unit of work." FFT and DFT algorithms can use tables of pre-computed coefficient values for better numerical efficiency, but this requires more accesses to coefficient values buffered in external memory, which can lead to increased cache contention that counters some of the numerical advantage.
Both algorithms gain approximately a factor of 2 efficiency when using real-valued rather than complex-valued input data. However, these gains are natural for the Goertzel algorithm but will not be achieved for the FFT without using certain algorithm variants[which?] specialised for transforming real-valued data.