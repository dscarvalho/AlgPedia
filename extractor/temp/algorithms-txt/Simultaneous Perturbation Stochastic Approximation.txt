ABOUT
Simultaneous perturbation stochastic approximation (SPSA) is an algorithmic method for optimizing systems with multiple unknown parameters. It is a type of stochastic approximation algorithm. As an optimization method, it is appropriately suited to large-scale population models, adaptive modeling, simulation optimization, and atmospheric modeling. Many examples are presented at the SPSA website http://www.jhuapl.edu/SPSA. A comprehensive recent book on the subject is Bhatnagar et al. (2013). An early paper on the subject is Spall (1987) and the foundational paper providing the key theory and justification is Spall (1992).
FULL TEXT
Simultaneous perturbation stochastic approximation (SPSA) is an algorithmic method for optimizing systems with multiple unknown parameters. It is a type of stochastic approximation algorithm. As an optimization method, it is appropriately suited to large-scale population models, adaptive modeling, simulation optimization, and atmospheric modeling. Many examples are presented at the SPSA website http://www.jhuapl.edu/SPSA. A comprehensive recent book on the subject is Bhatnagar et al. (2013). An early paper on the subject is Spall (1987) and the foundational paper providing the key theory and justification is Spall (1992).
SPSA is a descent method capable of finding global minima, sharing this property with other methods as simulated annealing. Its main feature is the gradient approximation that requires only two measurements of the objective function, regardless of the dimension of the optimization problem. Recall that we want to find the optimal control 




u

∗




{\displaystyle u^{*}}

 with loss function 



J
(
u
)


{\displaystyle J(u)}

:
Both Finite Differences Stochastic Approximation (FDSA) and SPSA use the same iterative process:
where 




u

n


=
(
(

u

n



)

1


,
(

u

n



)

2


,
…
,
(

u

n



)

p



)

T




{\displaystyle u_{n}=((u_{n})_{1},(u_{n})_{2},\ldots ,(u_{n})_{p})^{T}}

 represents the 




n

t
h




{\displaystyle n^{th}}

 iterate, 







g
^




n


(

u

n


)


{\displaystyle {\hat {g}}_{n}(u_{n})}

 is the estimate of the gradient of the objective function 



g
(
u
)
=


∂

∂
u



J
(
u
)


{\displaystyle g(u)={\frac {\partial }{\partial u}}J(u)}

 evaluated at 





u

n





{\displaystyle {u_{n}}}

, and 



{

a

n


}


{\displaystyle \{a_{n}\}}

 is a positive number sequence converging to 0. If 




u

n




{\displaystyle u_{n}}

 is a p-dimensional vector, the 




i

t
h




{\displaystyle i^{th}}

 component of the symmetric finite difference gradient estimator is:
1 ≤i ≤p, where 




e

i




{\displaystyle e_{i}}

 is the unit vector with a 1 in the 




i

t
h




{\displaystyle i^{th}}

 place, and 




c

n




{\displaystyle c_{n}}

is a small positive number that decreases with n. With this method, 2p evaluations of J for each 




g

n




{\displaystyle g_{n}}

 are needed. Clearly, when p is large, this estimator loses efficiency.
Let now 




Δ

n




{\displaystyle \Delta _{n}}

 be a random perturbation vector. The 




i

t
h




{\displaystyle i^{th}}

 component of the stochastic perturbation gradient estimator is:
Remark that FD perturbs only one direction at a time, while the SP estimator disturbs all directions at the same time (the numerator is identical in all p components). The number of loss function measurements needed in the SPSA method for each 




g

n




{\displaystyle g_{n}}

 is always 2, independent of the dimension p. Thus, SPSA uses p times fewer function evaluations than FDSA, which makes it a lot more efficient.
Simple experiments with p=2 showed that SPSA converges in the same number of iterations as FDSA. The latter follows approximately the steepest descent direction, behaving like the gradient method. On the other hand, SPSA, with the random search direction, does not follow exactly the gradient path. In average though, it tracks it nearly because the gradient approximation is an almost unbiased estimator of the gradient, as shown in the following lemma.
Denote by
the bias in the estimator 







g
^




n




{\displaystyle {\hat {g}}_{n}}

. Assume that 



{
(

Δ

n



)

i


}


{\displaystyle \{(\Delta _{n})_{i}\}}

 are all mutually independent with zero-mean, bounded second moments, and 



E
(

|

(

Δ

n



)

i




|


−
1


)


{\displaystyle E(|(\Delta _{n})_{i}|^{-1})}

 uniformly bounded. Then 




b

n




{\displaystyle b_{n}}

→0 w.p. 1.
The main idea is to use conditioning on 




Δ

n




{\displaystyle \Delta _{n}}

 to express 



E
[
(




g
^




n



)

i


]


{\displaystyle E[({\hat {g}}_{n})_{i}]}

 and then to use a second order Taylor expansion of 



J
(

u

n


+

c

n



Δ

n



)

i




{\displaystyle J(u_{n}+c_{n}\Delta _{n})_{i}}

 and 



J
(

u

n


−

c

n



Δ

n



)

i




{\displaystyle J(u_{n}-c_{n}\Delta _{n})_{i}}

. After algebraic manipulations using the zero mean and the independence of 



{
(

Δ

n



)

i


}


{\displaystyle \{(\Delta _{n})_{i}\}}

, we get
The result follows from the hypothesis that 




c

n




{\displaystyle c_{n}}

→0.
Next we resume some of the hypotheses under which 




u

t




{\displaystyle u_{t}}

 converges in probability to the set of global minima of 



J
(
u
)


{\displaystyle J(u)}

. The efficiency of the method depends on the shape of 



J
(
u
)


{\displaystyle J(u)}

, the values of the parameters 




a

k




{\displaystyle a_{k}}

 and 




c

k




{\displaystyle c_{k}}

 and the distribution of the perturbation terms 




Δ

k
i




{\displaystyle \Delta _{ki}}

. First, the algorithm parameters must satisfy the following conditions:
A good choice for 




Δ

k
i




{\displaystyle \Delta _{ki}}

 is the Rademacher distribution, i.e. Bernoulli +-1 with probability 0.5. Other choices are possible too, but note that the uniform and normal distributions cannot be used because they do not satisfy the finite inverse moment conditions.
The loss function J(u) must be thrice continuously differentiable and the individual elements of the third derivative must be bounded: 




|


J

(
3
)


(
u
)

|

<

a

3


<
∞


{\displaystyle |J^{(3)}(u)|<a_{3}<\infty }

. Also, |J(u)|→∝ as u→∝.
In addition, 



∇
J


{\displaystyle \nabla J}

 must be Lipschitz continuous, bounded and the ODE 






u
˙



=
g
(
u
)


{\displaystyle {\dot {u}}=g(u)}

 must have a unique solution for each initial condition. Under these conditions and a few others, 




u

k




{\displaystyle u_{k}}

 converges in probability to the set of global minima of J(u) (see Maryak and Chin, 2008).
