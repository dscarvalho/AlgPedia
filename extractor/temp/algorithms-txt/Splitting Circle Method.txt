ABOUT
In mathematics, the splitting circle method is a numerical algorithm for the numerical factorization of a polynomial and, ultimately, for finding its complex roots. It was introduced by Arnold Schönhage in his 1982 paper The fundamental theorem of algebra in terms of computational complexity (Technical report, Mathematisches Institut der Universität Tübingen). A revised algorithm was presented by Victor Pan in 1998. An implementation was provided by Xavier Gourdon in 1996 for the Magma and PARI/GP computer algebra systems.
FULL TEXT
In mathematics, the splitting circle method is a numerical algorithm for the numerical factorization of a polynomial and, ultimately, for finding its complex roots. It was introduced by Arnold Schönhage in his 1982 paper The fundamental theorem of algebra in terms of computational complexity (Technical report, Mathematisches Institut der Universität Tübingen). A revised algorithm was presented by Victor Pan in 1998. An implementation was provided by Xavier Gourdon in 1996 for the Magma and PARI/GP computer algebra systems.


The fundamental idea of the splitting circle method is to use methods of complex analysis, more precisely the residue theorem, to construct factors of polynomials. With those methods it is possible to construct a factor of a given polynomial 



p
(
x
)
=

x

n


+

p

n
−
1



x

n
−
1


+
⋯
+

p

0




{\displaystyle p(x)=x^{n}+p_{n-1}x^{n-1}+\cdots +p_{0}}

 for any region of the complex plane with a piecewise smooth boundary. Most of those factors will be trivial, that is constant polynomials. Only regions that contain roots of p(x) result in nontrivial factors that have exactly those roots of p(x) as their own roots, preserving multiplicity.
In the numerical realization of this method one uses disks D(c,r) (center c, radius r) in the complex plane as regions. The boundary circle of a disk splits the set of roots of p(x) in two parts, hence the name of the method. To a given disk one computes approximate factors following the analytical theory and refines them using Newton's method. To avoid numerical instability one has to demand that all roots are well separated from the boundary circle of the disk. So to obtain a good splitting circle it should be embedded in a root free annulus A(c,r,R) (center c, inner radius r, outer radius R) with a large relative width R/r.
Repeating this process for the factors found, one finally arrives at an approximative factorization of the polynomial at a required precision. The factors are either linear polynomials representing well isolated zeros or higher order polynomials representing clusters of zeros.
Newton's identities are a bijective relation between the elementary symmetric polynomials of a tuple of complex numbers and its sums of powers. Therefore, it is possible to compute the coefficients of a polynomial
(or of a factor of it) from the sums of powers of its zeros
by solving the triangular system that is obtained by comparing the powers of u in the following identity of formal power series
If 



G
⊂

C



{\displaystyle G\subset \mathbb {C} }

 is a domain with piecewise smooth boundary C and if the zeros of p(x) are pairwise distinct and not on the boundary C, then from the residue theorem of residual calculus one gets
The identity of the left to the right side of this equation also holds for zeros with multiplicities. By using the Newton identities one is able to compute from those sums of powers the factor
of p(x) corresponding to the zeros of p(x) inside G. By polynomial division one also obtains the second factor g(x) in p(x) = f(x)g(x).
The commonly used regions are circles in the complex plane. Each circle gives raise to a split of the polynomial p(x) in factors f(x) and g(x). Repeating this procedure on the factors using different circles yields finer and finer factorizations. This recursion stops after a finite number of proper splits with all factors being nontrivial powers of linear polynomials.
The challenge now consists in the conversion of this analytical procedure into a numerical algorithm with good running time. The integration is approximated by a finite sum of a numerical integration method, making use of the fast Fourier transform for the evaluation of the polynomials p(x) and p'(x). The polynomial f(x) that results will only be an approximate factor. To ensure that its zeros are close to the zeros of p inside G and only to those, one must demand that all zeros of p are far away from the boundary C of the region G.
(Schönhage 1982) Let 



p
∈

C

[
X
]


{\displaystyle p\in \mathbb {C} [X]}

 be a polynomial of degree n has k zeros inside the circle of radius 1/2 and the remaining n-k zeros outside the circle of radius 2. With N=O(k) large enough, the approximation of the contour integrals using N points results in an approximation 




f

0




{\displaystyle f_{0}}

 of the factor f with error
where the norm of a polynomial is the sum of the moduli of its coefficients.
Since the zeros of a polynomial are continuous in its coefficients, one can make the zeros of 




f

0




{\displaystyle f_{0}}

 as close as wanted to the zeros of f by choosing N large enough. However, one can improve this approximation faster using a Newton method. Division of p with remainder yields an approximation 




g

0




{\displaystyle g_{0}}

 of the remaining factor g. Now
so discarding the last second order term one has to solve 



p
−

f

0



g

0


=

f

0


Δ
g
+

g

0


Δ
f


{\displaystyle p-f_{0}g_{0}=f_{0}\Delta g+g_{0}\Delta f}

 using any variant of the extended Euclidean algorithm to obtain the incremented approximations 




f

1


=

f

0


+
Δ
f


{\displaystyle f_{1}=f_{0}+\Delta f}

 and 




g

1


=

g

0


+
Δ
g


{\displaystyle g_{1}=g_{0}+\Delta g}

. This is repeated until the increments are zero relative to the chosen precision.
The crucial step in this method is to find an annulus of relative width 4 in the complex plane that contains no zeros of p and contains approximately as many zeros of p inside as outside of it. Any annulus of this characteristic can be transformed, by translation and scaling of the polynomial, into the annulus between the radii 1/2 and 2 around the origin. But, not every polynomial admits such a splitting annulus.
To remedy this situation, the Graeffe iteration is applied. It computes a sequence of polynomials
where the roots of 




p

j


(
x
)


{\displaystyle p_{j}(x)}

 are the 




2

j




{\displaystyle 2^{j}}

-th dyadic powers of the roots of the initial polynomial p. By splitting 




p

j


(
x
)
=
e
(
x
)
+
x

o
(
x
)


{\displaystyle p_{j}(x)=e(x)+x\,o(x)}

 into even and odd parts, the succeeding polynomial is obtained by purely arithmetic operations as 




p

j
+
1


(
x
)
=
(
−
1

)

deg
⁡
p


(
e
(
x

)

2


−
x

o
(
x

)

2


)


{\displaystyle p_{j+1}(x)=(-1)^{\deg p}(e(x)^{2}-x\,o(x)^{2})}

. The ratios of the absolute moduli of the roots increase by the same power 




2

j




{\displaystyle 2^{j}}

 and thus tend to infinity. Choosing j large enough one finally finds a splitting annulus of relative width 4 around the origin.
The approximate factorization of 




p

j


(
x
)
≈

f

j


(
x
)


g

j


(
x
)


{\displaystyle p_{j}(x)\approx f_{j}(x)\,g_{j}(x)}

 is now to be lifted back to the original polynomial. To this end an alternation of Newton steps and Padé approximations is used. It is easy to check that
holds. The polynomials on the left side are known in step j, the polynomials on the right side can be obtained as Padé approximants of the corresponding degrees for the power series expansion of the fraction on the left side.
Making use of the Graeffe iteration and any known estimate for the absolute value of the largest root one can find estimates R of this absolute value of any precision. Now one computes estimates for the largest and smallest distances 




R

j


>

r

j


>
0


{\displaystyle R_{j}>r_{j}>0}

 of any root of p(x) to any of the five center points 0, 2R, −2R, 2Ri, −2Ri and selects the one with the largest ratio 




R

j



/


r

j




{\displaystyle R_{j}/r_{j}}

 between the two. By this construction it can be guaranteed that 




R

j



/


r

j


>

e

0

.

3


≈
1.35


{\displaystyle R_{j}/r_{j}>e^{0{.}3}\approx 1.35}

 for at least one center. For such a center there has to be a root-free annulus of relative width 





e

0

.

3

/

n


≈
1
+



0

.

3

n





{\displaystyle \textstyle e^{0{.}3/n}\approx 1+{\frac {0{.}3}{n}}}

. After 




3
+

log

2


⁡
(
n
)



{\displaystyle \textstyle 3+\log _{2}(n)}

 Graeffe iterations, the corresponding annulus of the iterated polynomial has a relative width greater than 11 > 4, as required for the initial splitting described above (see Schönhage (1982)). After 




4
+

log

2


⁡
(
n
)
+

log

2


⁡
(
2
+

log

2


⁡
(
n
)
)



{\displaystyle \textstyle 4+\log _{2}(n)+\log _{2}(2+\log _{2}(n))}

 Graeffe iterations, the corresponding annulus has a relative width greater than 





2

13

.

8


⋅

n

6

.

9


>
(
64
⋅

n

3



)

2





{\displaystyle \textstyle 2^{13{.}8}\cdot n^{6{.}9}>(64\cdot n^{3})^{2}}

, allowing a much simplified initial splitting (see Malajovich/Zubelli (1997))
To locate the best root-free annulus one uses a consequence of the Rouché theorem: For k = 1, ..., n − 1 the polynomial equation
u > 0, has, by Descartes' rule of signs zero or two positive roots 




u

k


<

v

k




{\displaystyle u_{k}<v_{k}}

. In the latter case, there are exactly k roots inside the (closed) disk 



D
(
0
,

u

k


)


{\displaystyle D(0,u_{k})}

 and 



A
(
0
,

u

k


,

v

k


)


{\displaystyle A(0,u_{k},v_{k})}

 is a root-free (open) annulus.