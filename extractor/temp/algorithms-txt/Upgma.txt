ABOUT
UPGMA (Unweighted Pair Group Method with Arithmetic Mean) is a simple agglomerative (bottom-up) hierarchical clustering method. The method is generally attributed to Sokal and Michener.[1]
FULL TEXT
UPGMA (Unweighted Pair Group Method with Arithmetic Mean) is a simple agglomerative (bottom-up) hierarchical clustering method. The method is generally attributed to Sokal and Michener.[1]
The UPGMA method is similar to its weighted variant, the WPGMA method.


The UPGMA algorithm constructs a rooted tree (dendrogram) that reflects the structure present in a pairwise similarity matrix (or a dissimilarity matrix). At each step, the nearest two clusters are combined into a higher-level cluster. The distance between any two clusters A and B, each of size (i.e., cardinality) 





|



A



|




{\displaystyle {|{\mathcal {A}}|}}

 and 





|



B



|




{\displaystyle {|{\mathcal {B}}|}}

, is taken to be the average of all distances 



d
(
x
,
y
)


{\displaystyle d(x,y)}

 between pairs of objects 



x


{\displaystyle x}

 in 





A




{\displaystyle {\mathcal {A}}}

 and 



y


{\displaystyle y}

 in 





B




{\displaystyle {\mathcal {B}}}

, that is, the mean distance between elements of each cluster:
In other words, at each clustering step, the updated distance between the joined clusters 





A


∪


B




{\displaystyle {\mathcal {A}}\cup {\mathcal {B}}}

 and a new cluster 



X


{\displaystyle X}

 is given by the proportional averaging of the 




d



A


,
X




{\displaystyle d_{{\mathcal {A}},X}}

 and 




d



B


,
X




{\displaystyle d_{{\mathcal {B}},X}}

 distances:





d

(


A


∪


B


)
,
X


=




|



A



|

⋅

d



A


,
X


+

|



B



|

⋅

d



B


,
X





|



A



|

+

|



B



|






{\displaystyle d_{({\mathcal {A}}\cup {\mathcal {B}}),X}={\frac {|{\mathcal {A}}|\cdot d_{{\mathcal {A}},X}+|{\mathcal {B}}|\cdot d_{{\mathcal {B}},X}}{|{\mathcal {A}}|+|{\mathcal {B}}|}}}


The UPGMA algorithm produces rooted dendrograms and requires a constant-rate assumption - that is, it assumes an ultrametric tree in which the distances from the root to every branch tip are equal. When the tips are molecular data (i.e., DNA, RNA and protein), the ultrametricity assumption is called the molecular clock.
Let us assume that we have five elements 



(
a
,
b
,
c
,
d
,
e
)


{\displaystyle (a,b,c,d,e)}

 and the following matrix 




D

1




{\displaystyle D_{1}}

 of pairwise distances between them :
In this example, 




D

1


(
a
,
b
)
=
17


{\displaystyle D_{1}(a,b)=17}

 is the smallest value of 




D

1




{\displaystyle D_{1}}

, so we join elements 



a


{\displaystyle a}

 and 



b


{\displaystyle b}

.
Let 



u


{\displaystyle u}

 denote the node to which 



a


{\displaystyle a}

 and 



b


{\displaystyle b}

 are now connected. Setting 



δ
(
a
,
u
)
=
δ
(
b
,
u
)
=

D

1


(
a
,
b
)

/

2


{\displaystyle \delta (a,u)=\delta (b,u)=D_{1}(a,b)/2}

 ensures that elements 



a


{\displaystyle a}

 and 



b


{\displaystyle b}

 are equidistant from 



u


{\displaystyle u}

. This corresponds to the expectation of the ultrametricity hypothesis. The branches joining 



a


{\displaystyle a}

 and 



b


{\displaystyle b}

 to 



u


{\displaystyle u}

 then have lengths 



δ
(
a
,
u
)
=
δ
(
b
,
u
)
=
17

/

2
=
8.5


{\displaystyle \delta (a,u)=\delta (b,u)=17/2=8.5}

 (see the final dendrogram)
We then proceed to update the initial distance matrix 




D

1




{\displaystyle D_{1}}

 into a new distance matrix 




D

2




{\displaystyle D_{2}}

 (see below), reduced in size by one row and one column because of the clustering of 



a


{\displaystyle a}

 with 



b


{\displaystyle b}

. Bold values in 




D

2




{\displaystyle D_{2}}

 correspond to the new distances, calculated by averaging distances between the first cluster 



(
a
,
b
)


{\displaystyle (a,b)}

 and each of the remaining elements:





D

2


(
(
a
,
b
)
,
c
)
=
(

D

1


(
a
,
c
)
×
1
+

D

1


(
b
,
c
)
×
1
)

/

(
1
+
1
)
=
(
21
+
30
)

/

2
=
25.5


{\displaystyle D_{2}((a,b),c)=(D_{1}(a,c)\times 1+D_{1}(b,c)\times 1)/(1+1)=(21+30)/2=25.5}







D

2


(
(
a
,
b
)
,
d
)
=
(

D

1


(
a
,
d
)
+

D

1


(
b
,
d
)
)

/

2
=
(
31
+
34
)

/

2
=
32.5


{\displaystyle D_{2}((a,b),d)=(D_{1}(a,d)+D_{1}(b,d))/2=(31+34)/2=32.5}







D

2


(
(
a
,
b
)
,
e
)
=
(

D

1


(
a
,
e
)
+

D

1


(
b
,
e
)
)

/

2
=
(
23
+
21
)

/

2
=
22


{\displaystyle D_{2}((a,b),e)=(D_{1}(a,e)+D_{1}(b,e))/2=(23+21)/2=22}


Italicized values in 




D

2




{\displaystyle D_{2}}

 are not affected by the matrix update as they correspond to distances between elements not involved in the first cluster.
We now reiterate the three previous steps, starting from the new distance matrix 




D

2




{\displaystyle D_{2}}


Here, 




D

2


(
(
a
,
b
)
,
e
)
=
22


{\displaystyle D_{2}((a,b),e)=22}

 is the smallest value of 




D

2




{\displaystyle D_{2}}

, so we join cluster 



(
a
,
b
)


{\displaystyle (a,b)}

 and element 



e


{\displaystyle e}

.
Let 



v


{\displaystyle v}

 denote the node to which 



(
a
,
b
)


{\displaystyle (a,b)}

 and 



e


{\displaystyle e}

 are now connected. Because of the ultrametricity constraint, the branches joining 



a


{\displaystyle a}

 or 



b


{\displaystyle b}

 to 



v


{\displaystyle v}

, and 



e


{\displaystyle e}

 to 



v


{\displaystyle v}

 are equal and have the following length: 



δ
(
a
,
v
)
=
δ
(
b
,
v
)
=
δ
(
e
,
v
)
=
22

/

2
=
11


{\displaystyle \delta (a,v)=\delta (b,v)=\delta (e,v)=22/2=11}


We deduce the missing branch length: 



δ
(
u
,
v
)
=
δ
(
e
,
v
)
−
δ
(
a
,
u
)
=
δ
(
e
,
v
)
−
δ
(
b
,
u
)
=
11
−
8.5
=
2.5


{\displaystyle \delta (u,v)=\delta (e,v)-\delta (a,u)=\delta (e,v)-\delta (b,u)=11-8.5=2.5}

 (see the final dendrogram)
We then proceed to update 




D

2




{\displaystyle D_{2}}

 into a new distance matrix 




D

3




{\displaystyle D_{3}}

 (see below), reduced in size by one row and one column because of the clustering of 



(
a
,
b
)


{\displaystyle (a,b)}

 with 



e


{\displaystyle e}

. Bold values in 




D

3




{\displaystyle D_{3}}

 correspond to the new distances, calculated by proportional averaging:





D

3


(
(
(
a
,
b
)
,
e
)
,
c
)
=
(

D

2


(
(
a
,
b
)
,
c
)
×
2
+

D

2


(
e
,
c
)
×
1
)

/

(
2
+
1
)
=
(
25.5
×
2
+
39
×
1
)

/

3
=
30


{\displaystyle D_{3}(((a,b),e),c)=(D_{2}((a,b),c)\times 2+D_{2}(e,c)\times 1)/(2+1)=(25.5\times 2+39\times 1)/3=30}


Thanks to this proportional average, the calculation of this new distance accounts for the larger size of the 



(
a
,
b
)


{\displaystyle (a,b)}

 cluster (two elements) with respect to 



e


{\displaystyle e}

 (one element). Similarly:





D

3


(
(
(
a
,
b
)
,
e
)
,
d
)
=
(

D

2


(
(
a
,
b
)
,
d
)
×
2
+

D

2


(
e
,
d
)
×
1
)

/

(
2
+
1
)
=
(
32.5
×
2
+
43
×
1
)

/

3
=
36


{\displaystyle D_{3}(((a,b),e),d)=(D_{2}((a,b),d)\times 2+D_{2}(e,d)\times 1)/(2+1)=(32.5\times 2+43\times 1)/3=36}


Proportional averaging therefore gives equal weight to the initial distances of matrix 




D

1




{\displaystyle D_{1}}

. This is the reason why the method is unweighted, not with respect to the mathematical procedure but with respect to the initial distances.
We again reiterate the three previous steps, starting from the updated distance matrix 




D

3




{\displaystyle D_{3}}

.
Here, 




D

3


(
c
,
d
)
=
28


{\displaystyle D_{3}(c,d)=28}

 is the smallest value of 




D

3




{\displaystyle D_{3}}

, so we join elements 



c


{\displaystyle c}

 and 



d


{\displaystyle d}

.
Let 



w


{\displaystyle w}

 denote the node to which 



c


{\displaystyle c}

 and 



d


{\displaystyle d}

 are now connected. The branches joining 



c


{\displaystyle c}

 and 



d


{\displaystyle d}

 to 



w


{\displaystyle w}

 then have lengths 



δ
(
c
,
w
)
=
δ
(
d
,
w
)
=
28

/

2
=
14


{\displaystyle \delta (c,w)=\delta (d,w)=28/2=14}

 (see the final dendrogram)
There is a single entry to update, keeping in mind that the two elements 



c


{\displaystyle c}

 and 



d


{\displaystyle d}

 each have a contribution of 



1


{\displaystyle 1}

 in the average computation:





D

4


(
(
c
,
d
)
,
(
(
a
,
b
)
,
e
)
)
=
(

D

3


(
c
,
(
(
a
,
b
)
,
e
)
)
×
1
+

D

3


(
d
,
(
(
a
,
b
)
,
e
)
)
×
1
)

/

(
1
+
1
)
=
(
30
×
1
+
36
×
1
)

/

2
=
33


{\displaystyle D_{4}((c,d),((a,b),e))=(D_{3}(c,((a,b),e))\times 1+D_{3}(d,((a,b),e))\times 1)/(1+1)=(30\times 1+36\times 1)/2=33}


The final 




D

4




{\displaystyle D_{4}}

 matrix is:
So we join clusters 



(
(
a
,
b
)
,
e
)


{\displaystyle ((a,b),e)}

 and 



(
c
,
d
)


{\displaystyle (c,d)}

.
Let 



r


{\displaystyle r}

 denote the (root) node to which 



(
(
a
,
b
)
,
e
)


{\displaystyle ((a,b),e)}

 and 



(
c
,
d
)


{\displaystyle (c,d)}

 are now connected. The branches joining 



(
(
a
,
b
)
,
e
)


{\displaystyle ((a,b),e)}

 and 



(
c
,
d
)


{\displaystyle (c,d)}

 to 



r


{\displaystyle r}

 then have lengths:




δ
(
(
(
a
,
b
)
,
e
)
,
r
)
=
δ
(
(
c
,
d
)
,
r
)
=
33

/

2
=
16.5


{\displaystyle \delta (((a,b),e),r)=\delta ((c,d),r)=33/2=16.5}


We deduce the two remaining branch lengths:




δ
(
v
,
r
)
=
δ
(
(
(
a
,
b
)
,
e
)
,
r
)
−
δ
(
e
,
v
)
=
16.5
−
11
=
5.5


{\displaystyle \delta (v,r)=\delta (((a,b),e),r)-\delta (e,v)=16.5-11=5.5}






δ
(
w
,
r
)
=
δ
(
(
c
,
d
)
,
r
)
−
δ
(
c
,
w
)
=
16.5
−
14
=
2.5


{\displaystyle \delta (w,r)=\delta ((c,d),r)-\delta (c,w)=16.5-14=2.5}



The dendrogram is now complete. It is ultrametric because all tips (



a


{\displaystyle a}

 to 



e


{\displaystyle e}

) are equidistant from 



r


{\displaystyle r}

 :




δ
(
a
,
r
)
=
δ
(
b
,
r
)
=
δ
(
e
,
r
)
=
δ
(
c
,
r
)
=
δ
(
d
,
r
)
=
16.5


{\displaystyle \delta (a,r)=\delta (b,r)=\delta (e,r)=\delta (c,r)=\delta (d,r)=16.5}


The dendrogram is therefore rooted by 



r


{\displaystyle r}

, its deepest node.
A trivial implementation of the algorithm to construct the UPGMA tree has 



O
(

n

3


)


{\displaystyle O(n^{3})}

 time complexity, and using a heap for each cluster to keep its distances from other cluster reduces its time to 



O
(

n

2


log
⁡
n
)


{\displaystyle O(n^{2}\log n)}

. Fionn Murtagh presented some other approaches for special cases, a 



O
(
k

3

k



n

2


)


{\displaystyle O(k3^{k}n^{2})}

 time algorithm by Day and Edelsbrunner[4] for k-dimensional data that is optimal 



O
(

n

2


)


{\displaystyle O(n^{2})}

 for constant k, and another 



O
(

n

2


)


{\displaystyle O(n^{2})}

 algorithm for restricted inputs, when "the anglomerative strategy satisfies the reducibility property."[5]